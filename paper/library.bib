Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.
@inproceedings{Kostrikov2022-implicitQlearning,
title={Offline Reinforcement Learning with Implicit Q-Learning},
author={Ilya Kostrikov and Ashvin Nair and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=68n2s9ZJWF8}
}

@misc{Wu2020-BehaviorRegularizedAC,
title={Behavior Regularized Offline Reinforcement Learning},
author={Yifan Wu and George Tucker and Ofir Nachum},
year={2020},
url={https://openreview.net/forum?id=BJg9hTNKPH}
}



@inbook{Kumar2019-BootstrapErrorQlearningMMD,
author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
title = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
year = {2019},
booktitle = {33rd Conference on Neural Information Processing Systems},
}

@misc{Jaques2020-OfflineDialog,
title={Way Off-Policy Batch Deep Reinforcement Learning of Human Preferences in Dialog},
author={Natasha Jaques and Asma Ghandeharioun and Judy Hanwen Shen and Craig Ferguson and Agata Lapedriza and Noah Jones and Shixiang Gu and Rosalind Picard},
year={2020},
url={https://openreview.net/forum?id=rJl5rRVFvH}
}

@inproceedings{Fujimoto2021-minimalist,
	title={A Minimalist Approach to Offline Reinforcement Learning},
	author={Scott Fujimoto and Shixiang Shane Gu},
	booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
	year={2021},
}


@InProceedings{Kostrikov2021-fisherCriticReg,
  title = 	 {Offline Reinforcement Learning with Fisher Divergence Critic Regularization},
  author =       {Kostrikov, Ilya and Fergus, Rob and Tompson, Jonathan and Nachum, Ofir},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5774--5783},
  year = 	 {2021},
}


@InProceedings{Dadashi2021-pseudoMetricOffline,
  title = 	 {Offline Reinforcement Learning with Pseudometric Learning},
  author =       {Dadashi, Robert and Rezaeifar, Shideh and Vieillard, Nino and Hussenot, L{\'e}onard and Pietquin, Olivier and Geist, Matthieu},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2307--2318},
  year = 	 {2021},
}

@misc{Nair2021-awac,
title={{\{}AWAC{\}}: Accelerating Online Reinforcement Learning with Offline Datasets},
author={Ashvin Nair and Murtaza Dalal and Abhishek Gupta and Sergey Levine},
year={2021},
url={https://openreview.net/forum?id=OJiM1R3jAtZ}
}

@misc{Gulcehre2021-regularizedBehavior,
      title={Regularized Behavior Value Estimation}, 
      author={Caglar Gulcehre and Sergio Gómez Colmenarejo and Ziyu Wang and Jakub Sygnowski and Thomas Paine and Konrad Zolna and Yutian Chen and Matthew Hoffman and Razvan Pascanu and Nando de Freitas},
      year={2021},
      eprint={2103.09575},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Tsai2021-selfsupervisedRelativePredictiveCoding,
title={Self-supervised Representation Learning with Relative Predictive Coding},
author={Yao-Hung Hubert Tsai and Martin Q. Ma and Muqiao Yang and Han Zhao and Louis-Philippe Morency and Ruslan Salakhutdinov},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=068E_JSq9O}
}

@inproceedings{Xiao2023-InSampleSoftmax,
title={The In-Sample Softmax for Offline Reinforcement Learning},
author={Chenjun Xiao and Han Wang and Yangchen Pan and Adam White and Martha White},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=u-RuvyDYqCM}
}

@InProceedings{Fujimoto2019-InSampleMax,
  title = 	 {Off-Policy Deep Reinforcement Learning without Exploration},
  author =       {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2052--2062},
  year = 	 {2019},
}


@article{Blondel-2020LearningFenchelYoundLoss,
  author  = {Mathieu Blondel and AndrÃ© F.T. Martins and Vlad Niculae},
  title   = {Learning with Fenchel-Young losses},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {35},
  pages   = {1--69},
}

@inproceedings{goodfellow2014-GAN,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Generative Adversarial Nets},
 year = {2014}
}

@misc{zhu2023generalized,
      title={Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence}, 
      author={Lingwei Zhu and Zheng Chen and Takamitsu Matsubara and Martha White},
      year={2023},
      eprint={2301.11476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Yu2016-orthogonalRandomFeatures,
 author = {Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Orthogonal Random Features},
 volume = {29},
 year = {2016}
}



@inproceedings{li2021-prototypicalContrastive,
title={Prototypical Contrastive Learning of Unsupervised Representations},
author={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KmykpuSrjcq}
}

@InProceedings{Belghazi2018-MINE,
  title = 	 {Mutual Information Neural Estimation},
  author =       {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {531--540},
  year = 	 {2018},
  abstract = 	 {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.}
}

@article{Nielsen2013-chiApproxFdiv,
author = {Nielsen, Frank and Nock, Richard},
year = {2013},
month = {09},
title = {On the Chi Square and Higher-Order Chi Distances for Approximating f-Divergences},
volume = {21},
journal = {Signal Processing Letters, IEEE},
}

@article{Cen2022-GlobalConvergNPG-Entropy,
author = {Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
title = {Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization},
journal = {Operations Research},
volume = {70},
number = {4},
pages = {2563-2578},
year = {2022},
}



@inproceedings{Kakade2001-naturalGradient,
author = {Kakade, Sham},
title = {A Natural Policy Gradient},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1531–1538},
series = {NIPS'01}
}

@article{Agarwal2021-TheoryPG,
  author  = {Alekh Agarwal and Sham M. Kakade and Jason D. Lee and Gaurav Mahajan},
  title   = {On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {98},
  pages   = {1--76},
}

@article{Nachum2019-Algaedice,
  title={AlgaeDICE: Policy Gradient from Arbitrary Experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and
      Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}

@inproceedings{Zhang2020-GenDICE,
title={GenDICE: Generalized Offline Estimation of Stationary Values},
author={Ruiyi Zhang and Bo Dai and Lihong Li and Dale Schuurmans},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HkxlcnVFwB}
}

@inproceedings{Nachum2019-DualDICE,
author = {Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
title = {DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections},
year = {2019},
abstract = {In many real-world reinforcement learning applications, access to the environment is limited to a fixed dataset, instead of direct (online) interaction with the environment. When using this data for either evaluation or training of a new policy, accurate estimates of discounted stationary distribution ratios — correction terms which quantify the likelihood that the new policy will experience a certain state-action pair normalized by the probability with which the state-action pair appears in the dataset — can improve accuracy and performance. In this work, we propose an algorithm, DualDICE, for estimating these quantities. In contrast to previous approaches, our algorithm is agnostic to knowledge of the behavior policy (or policies) used to generate the dataset. Furthermore, it eschews any direct use of importance weights, thus avoiding potential optimization instabilities endemic of previous methods. In addition to providing theoretical guarantees, we present an empirical study of our algorithm applied to off-policy policy evaluation and find that our algorithm significantly improves accuracy compared to existing techniques.},
booktitle = {Advances in Neural Information Processing Systems},
numpages = {11}
}

@article{Nachum2020-RLFenchelRockafellar,
  author    = {Ofir Nachum and
               Bo Dai},
  title     = {Reinforcement Learning via Fenchel-Rockafellar Duality},
  year      = {2020},
  url       = {http://arxiv.org/abs/2001.01866},
  eprinttype = {arXiv},
  eprint    = {2001.01866},
}
@inproceedings{Yu2020-EBMwithFDiv,
author = {Yu, Lantao and Song, Yang and Song, Jiaming and Ermon, Stefano},
title = {Training Deep Energy-Based Models with f-Divergence Minimization},
year = {2020},
abstract = {Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
series = {ICML'20},
pages={1-11},
}

@inproceedings{Wan2020-fDivVI,
 author = {Wan, Neng and Li, Dapeng and Hovakimyan, Naira},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {17370--17379},
 title = {f-Divergence Variational Inference},
 volume = {33},
 year = {2020}
}


@inproceedings{Ghasemipour2019-divergenceMinPerspImitation,
  title={A Divergence Minimization Perspective on Imitation Learning Methods},
  author={Seyed Kamyar Seyed Ghasemipour and Richard S. Zemel and Shixiang Shane Gu},
  booktitle={Conference on Robot Learning},
  year={2019},
  pages = {1-19},
}

@articile{Ke2019-imitationLearningAsfDiv,
  doi = {10.48550/ARXIV.1905.12888},
  url = {https://arxiv.org/abs/1905.12888},
  author = {Ke, Liyiming and Choudhury, Sanjiban and Barnes, Matt and Sun, Wen and Lee, Gilwoo and Srinivasa, Siddhartha},
  title = {Imitation Learning as $f$-Divergence Minimization},
  publisher = {arXiv},
  year = {2019},
}

@Article{Belousov2019-AlphaDivergence,
AUTHOR = {Belousov, Boris and Peters, Jan},
TITLE = {Entropic Regularization of Markov Decision Processes},
JOURNAL = {Entropy},
VOLUME = {21},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {674},
}


@inproceedings{Wang2018-tailAdaptivefDiv,
author = {Wang, Dilin and Liu, Hao and Liu, Qiang},
title = {Variational Inference with Tail-Adaptive f-Divergence},
year = {2018},
abstract = {Variational inference with α-divergences has been widely used in modern probabilistic machine learning. Compared to Kullback-Leibler (KL) divergence, a major advantage of using α-divergences (with positive α values) is their mass-covering property. However, estimating and optimizing α-divergences require to use importance sampling, which may have large or infinite variance due to heavy tails of importance weights. In this paper, we propose a new class of tail-adaptive f-divergences that adaptively change the convex function f with the tail distribution of the importance weights, in a way that theoretically guarantees finite moments, while simultaneously achieving mass-covering properties. We test our methods on Bayesian neural networks, as well as deep reinforcement learning in which our method is applied to improve a recent soft actor-critic (SAC) algorithm (Haarnoja et al., 2018). Our results show that our approach yields significant advantages compared with existing methods based on classical KL and α-divergences.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5742–5752},
series = {NIPS'18}
}

@inproceedings{Nowozin2016-fGAN,
 author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1-9},
 title = {f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization},
 volume = {29},
 year = {2016}
}

@inproceedings{Huang2022-bregmanPO,
title={Bregman Gradient Policy Optimization},
author={Feihu Huang and Shangqian Gao and Heng Huang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ZU-zFnTum1N}
}

@article{suyari2020-advantageQlog,
	title = {Advantages of q-logarithm representation over q-exponential representation from the sense of scale and shift on nonlinear systems},
	volume = {229},
	abstract = {Addition and subtraction of observed values can be computed under the obvious and implicit assumption that the scale unit of measurement should be the same for all arguments, which is valid even for any nonlinear systems. This paper starts with the distinction between exponential and non-exponential family in the sense of the scale unit of measurement. In the simplest nonlinear model dy∕dx = yq, it is shown how typical effects such as rescaling and shift emerge in the nonlinear systems and affect observed data. Based on the present results, the two representations, namely the q-exponential and the q-logarithm ones, are proposed. The former is for rescaling, the latter for unified understanding with a fixed scale unit. As applications of these representations, the corresponding entropy and the general probability expression for unified understanding with a fixed scale unit are presented. For the theoretical study of nonlinear systems, q-logarithm representation is shown to have significant advantages over q-exponential representation.},
	number = {5},
	journal = {The European Physical Journal Special Topics},
	author = {Suyari, Hiroki and Matsuzoe, Hiroshi and Scarfone, Antonio M.},
	year = {2020},
	pages = {773--785},
}

@book{hiriart2004-convexanalysis,
  title={Fundamentals of Convex Analysis},
  author={Hiriart-Urruty, J.B. and Lemar{\'e}chal, C.},
  series={Grundlehren Text Editions},
  year={2004},
  publisher={Springer Berlin Heidelberg}
}

@Article{TsallisRelativeEntropy,
AUTHOR = {Prehl, Janett and Essex, Christopher and Hoffmann, Karl Heinz},
TITLE = {Tsallis Relative Entropy and Anomalous Diffusion},
JOURNAL = {Entropy},
VOLUME = {14},
YEAR = {2012},
NUMBER = {4},
PAGES = {701--716},
}

@InProceedings{Futami2018-robustDivergence,
  title = 	 {Variational Inference based on Robust Divergences},
  author = 	 {Futami, Futoshi and Sato, Issei and Sugiyama, Masashi},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {813--822},
  year = 	 {2018},
  volume = 	 {84},
  abstract = 	 {Robustness to outliers is a central issue in real-world machine learning applications. While replacing a model to a heavy-tailed one (e.g., from Gaussian to Student-t)  is a standard approach for robustification, it can only be applied to simple models. In this paper, based on Zellner’s optimization and variational formulation of Bayesian inference, we propose an outlier-robust pseudo-Bayesian variational method by replacing the Kullback-Leibler divergence used for data fitting to a robust divergence such as the beta- and gamma-divergences. An advantage of our approach is that superior but complex models such as deep networks can also be handled. We theoretically prove that, for deep networks with ReLU activation functions, the influence function in our proposed method is bounded, while it is unbounded in the ordinary variational inference. This implies that our proposed method is robust to both of input and output outliers, while the ordinary variational method is not. We experimentally demonstrate that our robust variational method outperforms ordinary variational inference in regression and classification with deep networks.}
}

@InProceedings{Gutmann2010-NCE,
title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
booktitle = 	 {Thirteenth International Conference on Artificial Intelligence and Statistics},
pages = 	 {297--304},
year = 	 {2010},
}
@inproceedings{Henaff2020-cpcv2,
author = {H\'{e}naff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and Van Den Oord, Aaron},
title = {Data-Efficient Image Recognition with Contrastive Predictive Coding},
year = {2020},
booktitle = {37th International Conference on Machine Learning},
}

@inproceedings{Lyle2022-UnderstandingCapacityLossRL,
title={Understanding and Preventing Capacity Loss in Reinforcement Learning},
author={Clare Lyle and Mark Rowland and Will Dabney},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ZkC8wKoLbQ7},
pages = {1-13},
}

@article{PNAS2017-CatastrophicForgetting,
author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz and et al. },
title = {Overcoming catastrophic forgetting in neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {114},
number = {13},
pages = {3521-3526},
year = {2017},
}


@InProceedings{Kitamura2021-GVI,
  title = 	 {Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning},
  author =       {Kitamura, Toshinori and Zhu, Lingwei and Matsubara, Takamitsu},
  booktitle = 	 {Proceedings of The 13th Asian Conference on Machine Learning},
  pages = 	 {918--931},
  year = 	 {2021},
  volume = 	 {157},
}

@inproceedings{Vahdat2020-NVAE,
 author = {Vahdat, Arash and Kautz, Jan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {19667--19679},
 title = {NVAE: A Deep Hierarchical Variational Autoencoder},
 volume = {33},
 year = {2020}
}

@InProceedings{Tomczak2018-VampPrior,
  title = 	 {VAE with a VampPrior},
  author = 	 {Tomczak, Jakub and Welling, Max},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1214--1223},
  year = 	 {2018},
  volume = 	 {84},
  abstract = 	 {Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.}
}

@misc{Yang2022-EMBC-CancerSubtyping,
  url = {https://arxiv.org/abs/2204.02278},
  author = {Yang, Ziwei and Zhu, Lingwei and Chen, Zheng and Huang, Ming and Ono, Naoaki and Altaf-Ul-Amin, MD and Kanaya, Shigehiko},
  title = {Cancer Subtyping via Embedded Unsupervised Learning on Transcriptomics Data},
  publisher = {arXiv},
  year = {2022},
}

@inproceedings{Bachman2019-MutualAcrossViews,
author = {Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
title = {Learning Representations by Maximizing Mutual Information across Views},
year = {2019},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
pages = {1-11}
}

@inproceedings{Hjelm2019-deepInfomax,
author = {Hjelm, Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Philip and Trischler, Adam and Bengio, Yoshua},
title = {Learning deep representations by mutual information estimation and maximization},
booktitle = {7th International Conference on Learning Representations {(ICLR)} },
year = {2019},
pages={1-14},
}

@inproceedings{Kulshyn2019-HierarchicalPriorsVAE,
 author = {Klushyn, Alexej and Chen, Nutan and Kurle, Richard and Cseke, Botond and van der Smagt, Patrick},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1-10},
 title = {Learning Hierarchical Priors in VAEs},
 volume = {32},
 year = {2019}
}
@inproceedings{Razavi2019-VQVAE2,
 author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1-11},
 title = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
 volume = {32},
 year = {2019}
}


@inproceedings{Gao2022-SmoothAL, 
title={Smoothing Advantage Learning}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Yaozhong Gan and Zhe Zhang and Xiaoyang Tan}, 
year={2022}, 
pages={1-8},
}

@inproceedings{Smirnova-2020-ConvergenceSmoothRegularizedVI,
 author = {Smirnova, Elena and Dohmatob, Elvis},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6540--6550},
 title = {On the Convergence of Smooth Regularized Approximate Value Iteration Schemes},
 volume = {33},
 year = {2020}
}


@misc{Kozuno-2022-KLGenerativeMinMaxOptimal,
  url = {https://arxiv.org/abs/2205.14211},
  author = {Kozuno, Tadashi and Yang, Wenhao and Vieillard, Nino and Kitamura, Toshinori and Tang, Yunhao and Mei, Jincheng and Ménard, Pierre and Azar, Mohammad Gheshlaghi and Valko, Michal and Munos, Rémi and Pietquin, Olivier and Geist, Matthieu and Szepesvári, Csaba},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal},
  publisher = {arXiv},
  year = {2022},
}

@book{Spall-stochasticSearchOptimization,
author = {Spall, James C.},
title = {Introduction to Stochastic Search and Optimization},
year = {2003},
isbn = {0471330523},
publisher = {John Wiley \& Sons, Inc.},
address = {USA},
edition = {1},
}

@article{Mo2017-iclusterBayes,
	title = {A fully {Bayesian} latent variable model for integrative clustering analysis of multi-type omics data},
	volume = {19},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/kxx017},
	number = {1},
	journal = {Biostatistics},
	author = {Mo, Qianxing and Shen, Ronglai and Guo, Cui and Vannucci, Marina and Chan, Keith S and Hilsenbeck, Susan G},
	month = may,
	year = {2017},
	pages = {71--86},
}


@inproceedings{Aravind2017-simpleContinuousControl,
 author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel V. and Kakade, Sham M},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--12},
 title = {Towards Generalization and Simplicity in Continuous Control},
 volume = {30},
 year = {2017}
}



@inproceedings{gu2017interpolated,
  title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
  author={Gu, Shixiang Shane and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3846--3855},
  year={2017}
}

@inproceedings{precup2001-offpolicy,
author = {Precup, Doina and Sutton, Richard S. and Dasgupta, Sanjoy},
title = {Off-Policy Temporal Difference Learning with Function Approximation},
year = {2001},
booktitle = {International Conference on Machine Learning},
pages = {417–424},
numpages = {8},
}

@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1054--1062},
  year={2016}
}

@article{hafner2020-dream,
      title={Dream to Control: Learning Behaviors by Latent Imagination}, 
      author={Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
      year={2020},
      journal={arXiv preprint, arXiv:1912.01603},
}


@inproceedings{fakoor2020p3o,
  title={P3o: Policy-on policy-off policy optimization},
  author={Fakoor, Rasool and Chaudhari, Pratik and Smola, Alexander J},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={1017--1027},
  year={2020},
}

@inproceedings{wang2017sample,
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
booktitle = {International Conference on Learning Representations},
title = {{Sample efficient actor-critic with experience replay}},
year = {2017},
pages={1--13}
}

@book{Kullback,
  address = {New York},
  author = {Kullback, Solomon},
  booktitle = {Information Theory and Statistics},
  publisher = {Wiley},
  title = {Information Theory and Statistics},
  year = 1959
}



@book{tsallis2009introduction,
  title={Introduction to Nonextensive Statistical Mechanics: Approaching a Complex World},
  author={Tsallis, C.},
  isbn={9780387853581},
  year={2009},
  publisher={Springer New York}
}

BibTeX export options  can be customized via Options -> BibTeX in Mendeley Desktop⁠
@inproceedings{chen2017-variationalLossyEncoder,
  author    = {Xi Chen and
               Diederik P. Kingma and
               Tim Salimans and
               Yan Duan and
               Prafulla Dhariwal and
               John Schulman and
               Ilya Sutskever and
               Pieter Abbeel},
  title     = {Variational Lossy Autoencoder},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,},
  year      = {2017},
}

@InProceedings{McAllester2020-limitationsBoundsMutualInfo,
  title = 	 {Formal Limitations on the Measurement of Mutual Information},
  author =       {McAllester, David and Stratos, Karl},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {875--884},
  year = 	 {2020},
  volume = 	 {108},
}


@InProceedings{Poole2019-variationalBoundsMutualInfo,
  title = 	 {On Variational Bounds of Mutual Information},
  author =       {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5171--5180},
  year = 	 {2019},
  volume = 	 {97},
}


@article{PNAS2005-informationBasedClustering,
author = {Noam Slonim  and Gurinder Singh Atwal  and Gašper Tkačik  and William Bialek },
title = {Information-based clustering},
journal = {Proceedings of the National Academy of Sciences},
volume = {102},
number = {51},
pages = {18297-18302},
year = {2005},
}


@InProceedings{Zhu2020-robustRepresentationWorstMutualInfo,
  title = 	 {Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization},
  author =       {Zhu, Sicheng and Zhang, Xiao and Evans, David},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11609--11618},
  year = 	 {2020},
  volume = 	 {119},
}

@inproceedings{Ozair2019-WassersteinDependencyRepresentation,
author = {Ozair, Sherjil and Lynch, Corey and Bengio, Yoshua and van den Oord, A\"{a}ron and Levine, Sergey and Sermanet, Pierre},
title = {Wasserstein Dependency Measure for Representation Learning},
year = {2019},
abstract = {Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound on mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.},
booktitle = {Advances in Neural Information Processing Systems},
numpages = {1-11}
}

@inproceedings{Vieillard2019-ConnectOptRL,
title	= {On Connections between Constrained Optimization  and Reinforcement Learning},
author	= {Nino Vieillard and Olivier Pietquin and Matthieu Geist},
year	= {2019},
URL	= {https://arxiv.org/abs/1910.08476},
booktitle	= {Optimization Foundation for Reinforcement Learning Workshop at NeurIPS}
}

@article{HUBBS2020-drl-chemicalScheduling,
title = "A deep reinforcement learning approach for chemical production scheduling",
journal = "Computers and Chemical Engineering",
volume = "141",
pages = "106982",
year = "2020",
author = "Christian D. Hubbs and Can Li and Nikolaos V. Sahinidis and Ignacio E. Grossmann and John M. Wassick",
keywords = "Machine learning, Reinforcement learning, Optimization, Scheduling, Stochastic programming",
abstract = "This work examines applying deep reinforcement learning to a chemical production scheduling process to account for uncertainty and achieve online, dynamic scheduling, and benchmarks the results with a mixed-integer linear programming (MILP) model that schedules each time interval on a receding horizon basis. An industrial example is used as a case study for comparing the differing approaches. Results show that the reinforcement learning method outperforms the naive MILP approaches and is competitive with a shrinking horizon MILP approach in terms of profitability, inventory levels, and customer service. The speed and flexibility of the reinforcement learning system is promising for achieving real-time optimization of a scheduling system, but there is reason to pursue integration of data-driven deep reinforcement learning methods and model-based mathematical optimization approaches."
}


@article{NIAN2020-reviewRL-process,
title = "A review On reinforcement learning: Introduction and applications in industrial process control",
journal = "Computers and Chemical Engineering",
volume = "139",
pages = "106886",
year = "2020",
author = "Rui Nian and Jinfeng Liu and Biao Huang",
keywords = "Reinforcement learning, Model predictive control, Optimal control, Machine learning, Process industry, Process control",
abstract = "In recent years, reinforcement learning (RL) has attracted significant attention from both industry and academia due to its success in solving some complex problems. This paper provides an overview of RL along with tutorials for practitioners who are interested in implementing RL solutions into process control applications. The paper starts by providing an introduction to different reinforcement learning algorithms. Then, recent successes of RL applications across different industries will be explored, with more emphasis on process control applications. A detailed RL implementation example will also be shown. Afterwards, RL will be compared with traditional optimal control methods, in terms of stability and computational complexity among other factors, and the current shortcomings of RL will be introduced. This paper is concluded with a summary of RL’s potential advantages and disadvantages."
}

@incollection{BADGWELL2018,
title = "Reinforcement Learning – Overview of Recent Progress and Implications for Process Control",
series = "Computer and Chemical Engineering",
pages = "71 - 85",
year = "2018",
booktitle = "13th International Symposium on Process Systems Engineering (PSE)",
author = "Thomas A. Badgwell and Jay H. Lee and Kuang-Hung Liu",
keywords = "Reinforcement Learning, Model Predictive Control, Process Control",
abstract = "This paper provides a brief introduction to Reinforcement Learning (RL) technology, summarizes recent developments in this area, and discusses their potential implications for the field of process control. The paper begins with a brief introduction to RL, a machine learning technology that allows an agent to learn, through trial and error, the best way to accomplish a task. We then highlight two new developments in RL that have led to the recent wave of applications and media interest. A comparison of the key features of RL and Model Predictive Control (MPC) is then presented in order to clarify their similarities and differences. This is followed by an assessment of five ways that RL technology can potentially be used in process control applications. A final section summarizes our conclusions and lists directions for future RL research that may improve its relevance for process control applications."
}

@article{YOO2021-RL-batchProcess-DDPG,
title = "Reinforcement learning based optimal control of batch processes using Monte-Carlo deep deterministic policy gradient with phase segmentation",
journal = "Computers and Chemical Engineering",
volume = "144",
pages = "107133",
year = "2021",
author = "Haeun Yoo and Boeun Kim and Jong Woo Kim and Jay H. Lee",
keywords = "Batch process, Reinforcement learning, Optimal control, Actor-Critic",
abstract = "Batch process control represents a challenge given its dynamic operation over a large operating envelope. Nonlinear model predictive control (NMPC) is the current standard for optimal control of batch processes. The performance of conventional NMPC can be unsatisfactory in the presence of uncertainties. Reinforcement learning (RL) which can utilize simulation or real operation data is a viable alternative for such problems. To apply RL to batch process control effectively, however, choices such as the reward function design and value update method must be made carefully. This study proposes a phase segmentation approach for the reward function design and value/policy function representation. In addition, the deep deterministic policy gradient algorithm (DDPG) is modified with Monte-Carlo learning to ensure more stable and efficient learning behavior. A case study of a batch polymerization process producing polyols is used to demonstrate the improvement brought by the proposed approach and to highlight further issues."
}

@book{Winder2020-RL,
  title={Reinforcement Learning: Industrial Applications of Intelligent Agents},
  author={Winder, Philip},
  isbn={9781098114831},
  year={2020},
  publisher={O'Reilly Media, Incorporated}
}
@article{Spielberg2019-AIChE,
author = {Spielberg, Steven and Tulsyan, Aditya and Lawrence, Nathan P. and Loewen, Philip D. and Bhushan Gopaluni, R.},
title = {Toward self-driving processes: A deep reinforcement learning approach to control},
journal = {AIChE Journal},
volume = {65},
number = {10},
pages = {e16689},
keywords = {actor–critic networks, deep learning, model-free learning, process control, reinforcement learning},
abstract = {Abstract Advanced model-based controllers are well established in process industries. However, such controllers require regular maintenance to maintain acceptable performance. It is a common practice to monitor controller performance continuously and to initiate a remedial model re-identification procedure in the event of performance degradation. Such procedures are typically complicated and resource intensive, and they often cause costly interruptions to normal operations. In this article, we exploit recent developments in reinforcement learning and deep learning to develop a novel adaptive, model-free controller for general discrete-time processes. The deep reinforcement learning (DRL) controller we propose is a data-based controller that learns the control policy in real time by merely interacting with the process. The effectiveness and benefits of the DRL controller are demonstrated through many simulations.},
year = {2019}
}

@book{Kreyszig-functional,
author = {Kreyszig, Erwin},
title = {Introductory Functional Analysis with Applications},
year = {1991},
publisher = {Wiley},
isbn = {9780471504597},
}

@book{Rockafellar-cvx,
  address = {Princeton, N. J.},
  author = {Rockafellar, R. Tyrrell},
  publisher = {Princeton University Press},
  series = {Princeton Mathematical Series},
  title = {Convex analysis},
  year = {1970},
}



@book{Nesterov-cvxLectures,
author = {Nesterov, Yurii},
title = {Lectures on Convex Optimization},
year = {2018},
isbn = {3319915770},
publisher = {Springer Publishing Company, Incorporated},
edition = {2nd},
}

@book{Szepesvari-AlgRL,
author = {Szepesvari, Csaba},
title = {Algorithms for Reinforcement Learning},
year = {2010},
isbn = {1608454924},
publisher = {Morgan and Claypool Publishers},
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.}
}

@book{Powell-ADP,
author = {Powell, Warren B.},
title = {Approximate Dynamic Programming: Solving the Curses of Dimensionality (Wiley Series in Probability and Statistics)},
year = {2007},
isbn = {0470171553},
publisher = {Wiley-Interscience},
address = {USA}
}

@article{Kobers2013-RLRobotics-Survey,
author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
title = {Reinforcement Learning in Robotics: A Survey},
year = {2013},
issue_date = {September 2013},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {32},
number = {11},
issn = {0278-3649},
journal = {Int. J. Rob. Res.},
month = {sep},
pages = {1238–1274},
numpages = {37},
keywords = {Reinforcement learning, learning control, survey, robot}
}

@inproceedings{Caron-2018deepClustering,
  author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  description = {ECCV 2018 Open Access Repository},
  title = {Deep Clustering for Unsupervised Learning of Visual Features},
  year = {2018},
  pages={1-18},
}



@inproceedings{Serdega2020-variationalMutualInfoMaxDiscreteContinuous,
  author    = {Andriy Serdega and
               Dae{-}Shik Kim},
  title     = {{VMI-VAE:} Variational Mutual Information Maximization Framework for
               {VAE} With Discrete and Continuous Priors},
  booktitle   = {arXiv},
  volume    = {abs/2005.13953},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.13953},
}

@InProceedings{Alemi2018-fixBrokenELBO,
  title = 	 {Fixing a Broken {ELBO}},
  author =       {Alemi, Alexander and Poole, Ben and Fischer, Ian and Dillon, Joshua and Saurous, Rif A. and Murphy, Kevin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {159--168},
  year = 	 {2018},
  volume = 	 {80},
  abstract = 	 {Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.}
}

@inproceedings{Oord2019-contrastivePredictiveCoding,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  booktitle   = {arXiv},
  volume    = {abs/1807.03748},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03748},
}

@inproceedings{Boudiaf2020-transductiveInfoMaxFewShot,
 author = {Boudiaf, Malik and Ziko, Imtiaz and Rony, J\'{e}r\^{o}me and Dolz, Jose and Piantanida, Pablo and Ben Ayed, Ismail},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2445--2457},
 title = {Information Maximization for Few-Shot Learning},
 volume = {33},
 year = {2020}
}


@InProceedings{Liang2020-reallyAccessSourceData,
  title = 	 {Do We Really Need to Access the Source Data? {S}ource Hypothesis Transfer for Unsupervised Domain Adaptation},
  author =       {Liang, Jian and Hu, Dapeng and Feng, Jiashi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6028--6039},
  year = 	 {2020},
  volume = 	 {119},
  abstract = 	 {Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.}
}

@inproceedings{Lowe2019-putEndtoEndtoEnd,
 author = {L\"{o}we, Sindy and O\textquotesingle Connor, Peter and Veeling, Bastiaan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1-13},
 title = {Putting An End to End-to-End: Gradient-Isolated Learning of Representations},
 volume = {32},
 year = {2019}
}


@inproceedings{Tschannen2020-OnInfoMaxRepresentation,
title={On Mutual Information Maximization for Representation Learning},
author={Michael Tschannen and Josip Djolonga and Paul K. Rubenstein and Sylvain Gelly and Mario Lucic},
booktitle={International Conference on Learning Representations},
year={2020},
pages = {1-12}
}


@inproceedings{Bachman2019-MaxMutualViews,
 author = {Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1-11},
 title = {Learning Representations by Maximizing Mutual Information Across Views},
 volume = {32},
 year = {2019}
}

@inproceedings{Dosovitskiy2014-discriminativeUnsupervisedFeatureCNN,
 author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {766–774},
 title = {Discriminative Unsupervised Feature Learning with Convolutional Neural Networks},
 volume = {27},
 year = {2014}
}


@inproceedings{Oord2017-VQVAE-neuralDiscreteRepresentation,
author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
title = {Neural Discrete Representation Learning},
year = {2017},
booktitle = {Advances in Neural Information Processing Systems},
pages = {6309–6318},
}

@inproceedings{Gomes2010-RIM,
 author = {Krause, Andreas and Perona, Pietro and Gomes, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {775–783},
 title = {Discriminative Clustering by Regularized Information Maximization},
 volume = {23},
 year = {2010}
}


@inproceedings{Springenberg2015-un-supervisedCategoricalGAN,
  author    = {Jost Tobias Springenberg},
  title     = {Unsupervised and Semi-supervised Learning with Categorical Generative
               Adversarial Networks},
  booktitle = {4th International Conference on Learning Representations},
  year      = {2016},
  pages     = {1-12},
}

@inproceedings{Grandvalet2004-semisupervisedByEntropyMinimization,
 author = {Grandvalet, Yves and Bengio, Yoshua},
 title = {Semi-supervised Learning by Entropy Minimization},
 volume = {17},
 year = {2004},
 booktitle = {Advances in Neural Information Processing Systems},
pages = {529–536},
}


@inproceedings{Bridle1991-UnsupervisedClassifierMutualInfo,
 author = {Bridle, John and Heading, Anthony and MacKay, David},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1096-1101},
 title = {Unsupervised Classifiers, Mutual Information and \textquotesingle Phantom Targets},
 volume = {4},
 year = {1991}
}


@inproceedings{Hu2017-discreteSelfAugmentingRIM,
author = {Hu, Weihua and Miyato, Takeru and Tokui, Seiya and Matsumoto, Eiichi and Sugiyama, Masashi},
title = {Learning Discrete Representations via Information Maximizing Self-Augmented Training},
year = {2017},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
volumne = {70},
pages = {1558–1567},
}

@inproceedings{Shi2012-DiscriminativeUnsupervisedDomainAdaptation,
author = {Shi, Yuan and Sha, Fei},
title = {Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation},
year = {2012},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1275–1282},
}

@inproceedings{Kingma2014-VaritionalBayesAutoEncoder,
  author = {Kingma, Diederik P. and Welling, Max},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  title = {{Auto-Encoding Variational Bayes}},
  year = {2014},
  pages = {1-9},
}


@inbook{Leibfried2019-BellmanEmpowerment,
author = {Leibfried, Felix and Pascual-D\'{\i}az, Sergio and Grau-Moya, Jordi},
title = {A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment},
year = {2019},
booktitle = {Advances in Neural Information Processing Systems},
numpages = {12},
pages = {1-12},
}

@inproceedings{Zhang2021-RenyiEntropyRewardFree, 
title={Exploration by Maximizing Renyi Entropy for Reward-Free RL Framework}, 
volume={35}, 
number={12}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Zhang, Chuheng and Cai, Yuanying and Huang, Longbo and Li, Jian}, 
year={2021}, 
pages={10859-10867} 
}

@inproceedings{agarwal2021-DRL-edge-precipice,
title={Deep Reinforcement Learning at the Edge of the Statistical Precipice},
author={Rishabh Agarwal and Max Schwarzer and Pablo Samuel Castro and Aaron Courville and Marc G Bellemare},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
pages={1-10}
}

@inproceedings{Leibfried2019-CORL-mutualInfoActorCritic,
  author={Felix Leibfried and Jordi Grau-Moya},
  title={Mutual-Information Regularization in Markov Decision Processes and Actor-Critic Learning},
  year={2019},
  pages={360-373},
  booktitle={Conference on Robot Learning},
}

@inproceedings{tavakoli2018action,
  title={Action Branching Architectures for Deep Reinforcement Learning},
  author={Tavakoli, Arash and Pardo, Fabio and Kormushev, Petar},
  booktitle={AAAI Conference on Artificial Intelligence},
  pages={4131--4138},
  year={2018}
}

@inproceedings{kurutach2018-modelensemble,
title={Model-Ensemble Trust-Region Policy Optimization},
author={Thanard Kurutach and Ignasi Clavera and Yan Duan and Aviv Tamar and Pieter Abbeel},
booktitle={International Conference on Learning Representations},
year={2018},
pages={1-11},
}
@Book{Bertseka2005-dp,
  Title                    = {Dynamic Programming and Optimal Control},
  Author                   = {Dimitri P. Bertsekas},
  Publisher                = {Athena Scientific},
  Year                     = {2005},
  Address                  = {Belmont, MA, USA},
  Edition                  = {3rd},
  Volume                   = {I}
}

@incollection{Pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, others},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
}

@inproceedings{Wu2017-scalableTRPO,
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using Kronecker-Factored Approximation},
year = {2017},
abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also the method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the Mu-JoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5285–5294},
numpages = {10},
}

@inproceedings{fakoor2020-p3o,
  author    = {Rasool Fakoor and
               Pratik Chaudhari and
               Alexander J. Smola},
  title     = {{P3O:} Policy-on Policy-off Policy Optimization},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019},
  pages     = {371},
  year      = {2019},
}

@article{hafner2019dreamer,
  title={Dream to Control: Learning Behaviors by Latent Imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@article{SonyNature2022-QRSAC,
author = {Wurman, Peter and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and Gilpin, Leilani and Khandelwal, Piyush and Kompella, Varun and Lin, HaoChih and MacAlpine, Patrick and Oller, Declan and Seno, Takuma and Sherstan, Craig and Thomure, Michael and Kitano, Hiroaki},
year = {2022},
pages = {223-228},
title = {Outracing champion Gran Turismo drivers with deep reinforcement learning},
volume = {602},
journal = {Nature},
}

@InProceedings{Han2020-diverseAC,
  title = 	 {Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration},
  author =       {Han, Seungyul and Sung, Youngchul},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4018--4029},
  year = 	 {2021},
  volume = 	 {139},
}


@inproceedings{Han2021-maxmin-entropy,
 author = {Han, Seungyul and Sung, Youngchul},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--13},
 title = {A Max-Min Entropy Framework for Reinforcement Learning},
 year = {2021}
}

@inproceedings{rainbow,
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad Gheshlaghi and Silver, David},
  booktitle = {AAAI},
  pages = {3215-3222},
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning.},
  year = 2018
}



@INPROCEEDINGS{mujoco,  
author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},  
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},   
title={MuJoCo: A physics engine for model-based control},   
year={2012},  
volume={},  
number={},  
pages={5026-5033},  
}

@article{schulman18-equivalence,
  author    = {John Schulman and
               Pieter Abbeel and
               Xi Chen},
  title     = {Equivalence Between Policy Gradients and Soft Q-Learning},
  journal   = {CoRR},
  volume    = {abs/1704.06440},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.06440},
}

@article{Naudts2002DeformedLogarithm,
  title={Deformed exponentials and logarithms in generalized thermostatistics},
  author={Jan Naudts},
  journal={Physica A-statistical Mechanics and Its Applications},
  year={2002},
  volume={316},
  pages={323-334}
}

@article{Ohara2007-geometryDisrtibutionsTsallis,
title = {Geometry of distributions associated with Tsallis statistics and properties of relative entropy minimization},
journal = {Physics Letters A},
volume = {370},
number = {3},
pages = {184-193},
year = {2007},
author = {Atsumi Ohara},
keywords = {Information geometry, Nonextensive statistics, Constant curvature, Relative entropy minimization},
abstract = {Geometric aspects of Tsallis' nonextensive statistics are discussed by means of geometry with dual α-connections. Consequently, a close relation with the nonextensivity and curvature is elucidated. As main results we investigate several attractive properties of the minimization of Tsallis relative entropy with normalized q-expectation constraints.}
}

@ARTICLE{Suyari2005-LawErrorTsallis,  
author={Suyari, H. and Tsukada, M.},  
journal={IEEE Transactions on Information Theory},   
title={Law of error in Tsallis statistics},   
year={2005},  
volume={51},  
number={2},  
pages={753-757},  
}

@article{Furuichi2004-fundamentals-qKL,
author = {Furuichi,S.  and Yanagi,K.  and Kuriyama,K. },
title = {Fundamental properties of Tsallis relative entropy},
journal = {Journal of Mathematical Physics},
volume = {45},
number = {12},
pages = {4868-4877},
year = {2004},
}

@article{Yamano2004-properties-qlogexp,
title = {Some properties of q-logarithm and q-exponential functions in Tsallis statistics},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {305},
number = {3},
pages = {486-496},
year = {2002},
author = {Takuya Yamano},
keywords = {Tsallis statistics},
abstract = {We present some formulae which q-logarithmic and q-exponential functions introduced in Tsallis statistics satisfy. Exact expression of the Mellin transform of q-exponential function is given. Moreover, the q-generalization of the digamma function is presented.}
}


@inproceedings{Lee2020-generalTsallisRSS,
  author    = {Kyungjae Lee and
               Sungyub Kim and
               Sungbin Lim and
               Sungjoon Choi and
               Mineui Hong and
               Jae In Kim and
               Yong{-}Lae Park and
               Songhwai Oh},
  title     = {Generalized Tsallis Entropy Reinforcement Learning and Its Application
               to Soft Mobile Robots},
  booktitle = {Robotics: Science and Systems XVI},
  year      = {2020},
  pages     = {1-10},
}


@inproceedings{Liang2016-NeuralSymbolicMachines,
title	= {Neural Symbolic Machines:  Learning Semantic Parsers on Freebase  with Weak Supervision},
author	= {Chen Liang and Jonathan Berant and Quoc V. Le and Ken Forbus and Ni Lao},
year	= {2017},
booktitle	= {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
pages	= {23--33},
}



@inproceedings{Tang2020-selfImitation,
 author = {Tang, Yunhao},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {13964--13975},
 title = {Self-Imitation Learning via Generalized Lower Bound Q-learning},
 volume = {33},
 year = {2020}
}


@InProceedings{Oh2018-selfImitationLearning,
  title = 	 {Self-Imitation Learning},
  author =       {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3878--3887},
  year = 	 {2018},
  volume = 	 {80},
}


@inproceedings{zhu2021-CAC,
title={Cautious Actor-Critic},
author={Lingwei Zhu and Toshinori Kitamura and Takamitsu Matsubara},
booktitle={Asian Conference on Machine Learning},
year={2021},
pages = {220-235},
}


@inproceedings{grau-moya2018soft,
title={Soft Q-Learning with Mutual-Information Regularization},
author={Jordi Grau-Moya and Felix Leibfried and Peter Vrancx},
booktitle={International Conference on Learning Representations},
year={2019},
pages = {1-13},
}

@article{OpenAI2020,
author = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Others},
title ={Learning dexterous in-hand manipulation},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {1},
pages = {3-20},
year = {2020},
}



@book{Cover2006-information,
author = {Cover, Thomas M. and Thomas, Joy A.},
title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
year = {2006},
publisher = {Wiley-Interscience},
address = {USA}
}

@article{ZHU2022CCE,
title = {Alleviating parameter-tuning burden in reinforcement learning for large-scale process control},
journal = {Computers \& Chemical Engineering},
volume = {158},
pages = {107658},
year = {2022},
author = {Lingwei Zhu and Go Takami and Mizuo Kawahara and Hiroaki Kanokogi and Takamitsu Matsubara},
}

@inproceedings{Wang2019-divergenceAugmented,
 author = {Wang, Qing and Li, Yingru and Xiong, Jiechao and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1-12},
 title = {Divergence-Augmented Policy Optimization},
 volume = {32},
 year = {2019}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@article{Shin1978-AMPI,
 author = {Martin L. Puterman and Moon Chirl Shin},
 journal = {Management Science},
 number = {11},
 pages = {1127--1137},
 publisher = {INFORMS},
 title = {Modified Policy Iteration Algorithms for Discounted Markov Decision Problems},
 volume = {24},
 year = {1978}
}

@inproceedings{Lan2020-maxminQ,
  author    = {Qingfeng Lan and
               Yangchen Pan and
               Alona Fyshe and
               Martha White},
  title     = {Maxmin Q-learning: Controlling the Estimation Bias of Q-learning},
  booktitle = {8th International Conference on Learning Representations {(ICLR)} },
  year      = {2020},
  pages     = {1-10},
}
@inproceedings{Song2019-softmaxDQN,
    title={Revisiting the Softmax Bellman Operator: New Benefits and New Perspective},
    author={Song, Zhao and Parr, Ronald E. and Carin, Lawrence},
    booktitle={Proceedings of the 36th International Conference on Machine Learning},
    year={2019},
    pages = {1-10},
}

@InProceedings{fu2019-diagnosis, 
title = {Diagnosing Bottlenecks in Deep Q-learning Algorithms}, 
author = {Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey}, 
pages = {2021--2030}, 
year = {2019},  
volume = {97}, 
series = {Proceedings of 36th International Conference on Machine Learning}, 
}

@article{chan2021-greedification,
  author  = {Alan Chan and Hugo Silva and Sungsu Lim and Tadashi Kozuno and A. Rupam Mahmood and Martha White},
  title   = {Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {253},
  pages   = {1--79},
}

@inproceedings{Dabney2018-QRDQN, 
title={Distributional Reinforcement Learning With Quantile Regression}, 
volume={32},  
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, Rémi}
, 
pages = {2892-2899},
year={2018}, 
 }

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
}


@Article{young19minatar,
author = {{Young}, Kenny and {Tian}, Tian},
title = {MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments},
journal = {arXiv preprint arXiv:1903.03176},
year = "2019"
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}
@article{chen2018-TsallisApproximate,
  author    = {Gang Chen and
               Yiming Peng and
               Mengjie Zhang},
  title     = {Effective Exploration for Deep Reinforcement Learning via Bootstrapped
               Q-Ensembles under Tsallis Entropy Regularization},
  journal   = {arXiv:abs/1809.00403},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.00403},
  eprinttype = {arXiv},
  eprint    = {1809.00403},
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout


@article{TsallisEntropy,
author = {Tsallis, Constantino},
year = {1988},
pages = {479-487},
title = {Possible generalization of Boltzmann-Gibbs statistics},
volume = {52},
journal = {Journal of Statistical Physics},
}

@Article{Amari2011-qExpLog,
AUTHOR = {Amari, Shun-ichi and Ohara, Atsumi},
TITLE = {Geometry of q-Exponential Family of Probability Distributions},
JOURNAL = {Entropy},
VOLUME = {13},
YEAR = {2011},
NUMBER = {6},
PAGES = {1170--1185},
}


@inproceedings{Lu2019-generalStochasticAL,
 author = {Lu, Yingdong and Squillante, Mark and Wu, Chai Wah},
 booktitle = {Advances in Neural Information Processing Systems 32},
 pages = {1--11},
 title = {A Family of Robust Stochastic Operators for Reinforcement Learning},
 year = {2019}
}

@inproceedings{Farahmand2011-actionGap,
 author = {Farahmand, Amir-massoud},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 title = {Action-Gap Phenomenon in Reinforcement Learning},
 year = {2011}
}

@incollection{Baird1995-residual,
title = {Residual Algorithms: Reinforcement Learning with Function Approximation},
booktitle = {Machine Learning Proceedings 1995},
pages = {30-37},
year = {1995},
author = {Leemon Baird},
}

@ARTICLE{Lee2018-TsallisRAL,  
author={Lee, Kyungjae and Choi, Sungjoon and Oh, Songhwai},  
journal={IEEE Robotics and Automation Letters},   
title={Sparse Markov Decision Processes With Causal Sparse Tsallis Entropy Regularization for Reinforcement Learning},   
year={2018},  
volume={3},  
pages={1466-1473},  
}

@article{lee2019tsallis,
      title={Tsallis Reinforcement Learning: A Unified Framework for Maximum Entropy Reinforcement Learning}, 
      author={Kyungjae Lee and Sungyub Kim and Sungbin Lim and Sungjoon Choi and Songhwai Oh},
      year={2019},
      journal={arXiv cs.LG 1902.00137}
}

@inproceedings{Ferret2021-selfImitationAdvantage,
author = {Ferret, Johan and Pietquin, Olivier and Geist, Matthieu},
title = {Self-Imitation Advantage Learning},
year = {2021},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {501–509},
numpages = {9},
series = {AAMAS '21}
}

@inproceedings{Li2019-regularizedSparse,
author = {Li, Xiang and Yang, Wenhao and Zhang, Zhihua},
title = {A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning},
year = {2019},
booktitle = {Advances in Neural Information Processing Systems 32},
numpages = {11},
pages = {1-11},
}

@phdthesis{Farahmand2011-phd,
author = {Farahmand, Amir-massoud},
title = {Regularization in Reinforcement Learning},
year = {2011},
publisher = {University of Alberta},
address = {University of Alberta},
}

@inproceedings{Farahmand2008-regularizedPI,
author = {Farahmand, Amir-massoud and Ghavamzadeh, Mohammad and Szepesv\'{a}ri, Csaba and Mannor, Shie},
title = {Regularized Policy Iteration},
year = {2008},
booktitle = {Proceedings of the 21st International Conference on Neural Information Processing Systems},
pages = {441–448},
numpages = {8},
}

@InProceedings{Anschel2017-AverageDQN,
  title = 	 {Averaged-{DQN}: Variance Reduction and Stabilization for Deep Reinforcement Learning},
  author =       {Oron Anschel and Nir Baram and Nahum Shimkin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {176--185},
  year = 	 {2017},
  volume = 	 {70},
}


@InProceedings{Vieillard2020Momentum,
  title = 	 {Momentum in Reinforcement Learning},
  author =       {Vieillard, Nino and Scherrer, Bruno and Pietquin, Olivier and Geist, Matthieu},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2529--2538},
  year = 	 {2020},
  volume = 	 {108},
}

@inproceedings{Nachum2017-TrustPCL,
  author    = {Ofir Nachum and
               Mohammad Norouzi and
               Kelvin Xu and
               Dale Schuurmans},
  title     = {Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
  booktitle = {International Conference on Learning Representations
               },
  year      = {2018},
  pages={1--11}
}


@InProceedings{Ghasemipour2021-EMaQ,
  title = 	 {EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL},
  author =       {Ghasemipour, Seyed Kamyar Seyed and Schuurmans, Dale and Gu, Shixiang Shane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3682--3691},
  year = 	 {2021},
  volume = 	 {139},
}


@inproceedings{Zhu2021-cautious,
 author = {Zhu, Lingwei and Kitamura Toshinori and Matsubara Takamitsu},
 booktitle = {Asian Conference on Machine Learning},
 pages = {1--16},
 title = {Cautious Actor-Critic},
 volume = {In Press},
 year = {2021}
}

@inproceedings{Azar2011-speedy,
 author = {Azar, Mohammad and Kappen, Hilbert and Ghavamzadeh, Mohammad and Munos, R\'{e}mi},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 title = {Speedy Q-Learning},
 volume = {24},
 year = {2011}
}



@InProceedings{Grill20-MCTSasRegulariedPolicyOpt,
  title = 	 {{M}onte-{C}arlo Tree Search as Regularized Policy Optimization},
  author =       {Grill, Jean-Bastien and Altch{\'e}, Florent and Tang, Yunhao and Hubert, Thomas and Valko, Michal and Antonoglou, Ioannis and Munos, Remi},
  booktitle = 	 {37th International Conference on Machine Learning},
  pages = 	 {3769--3778},
  year = 	 {2020},
  volume = 	 {119},
}


@InProceedings{haarnoja18b-SACapplication,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
}


@article{uchibe2021-forwardBackward,
title = {Forward and inverse reinforcement learning sharing network weights and hyperparameters},
journal = {Neural Networks},
volume = {144},
pages = {138-153},
year = {2021},
author = {Eiji Uchibe and Kenji Doya},
keywords = {Reinforcement learning, Inverse reinforcement learning, Imitation learning, Entropy regularization},
abstract = {This paper proposes model-free imitation learning named Entropy-Regularized Imitation Learning (ERIL) that minimizes the reverse Kullback–Leibler (KL) divergence. ERIL combines forward and inverse reinforcement learning (RL) under the framework of an entropy-regularized Markov decision process. An inverse RL step computes the log-ratio between two distributions by evaluating two binary discriminators. The first discriminator distinguishes the state generated by the forward RL step from the expert’s state. The second discriminator, which is structured by the theory of entropy regularization, distinguishes the state–action–next-state tuples generated by the learner from the expert ones. One notable feature is that the second discriminator shares hyperparameters with the forward RL, which can be used to control the discriminator’s ability. A forward RL step minimizes the reverse KL estimated by the inverse RL step. We show that minimizing the reverse KL divergence is equivalent to finding an optimal policy. Our experimental results on MuJoCo-simulated environments and vision-based reaching tasks with a robotic arm show that ERIL is more sample-efficient than the baseline methods. We apply the method to human behaviors that perform a pole-balancing task and describe how the estimated reward functions show how every subject achieves her goal.}
}

@inproceedings{Chen2019-rewardConstrainedPO,
title	= {Reward Constrained Policy Optimization},
author	= {Chen Tessler and Daniel Mankowitz and Shie Mannor},
year	= {2019},
booktitle	= {Proceedings of the International Conference on Representation Learning (ICLR 2019)},
pages = {1--11},
}

@article{DOGRU2021-JPC-onlineRL,
title = {Online reinforcement learning for a continuous space system with experimental validation},
journal = {Journal of Process Control},
volume = {104},
pages = {86-100},
year = {2021},
author = {Oguzhan Dogru and Nathan Wieczorek and Kirubakaran Velswamy and Fadi Ibrahim and Biao Huang},
}

@InProceedings{wen2020-batchStationryEstimation,
  title = 	 {Batch Stationary Distribution Estimation},
  author =       {Wen, Junfeng and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10203--10213},
  year = 	 {2020},
  volume = 	 {119},

}

@article{Metelli2021-SPI,
  author  = {Alberto Maria Metelli and Matteo Pirotta and Daniele Calandriello and Marcello Restelli},
  title   = {Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {97},
  pages   = {1-83},
}

@article{Sason2016-fDiverInequalities,
  title={f-Divergence Inequalities},
  author={I. Sason and S. Verd{\'u}},
  journal={IEEE Transactions on Information Theory},
  year={2016},
  volume={62},
  pages={5973-6006}
}

@inproceedings{optuna,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-Generation Hyperparameter Optimization Framework},
year = {2019},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery Data Mining},
pages = {2623–2631},
numpages = {9},
series = {KDD '19}
}




@inproceedings{Yannis2021-AGAC,
title	= {Adversarially Guided Actor-Critic},
author	= {Yannis Flet-Berliac and Johan Ferret and Olivier Pietquin and Philippe Preux and Matthieu Geist},
year	= {2021},
booktitle	= {Proceedings of the International Conference on Representation Learning (ICLR 2021)},
pages = {1--12},
}



@inproceedings{Feng2019-kernelLoss,
 author = {Feng, Yihao and Li, Lihong and Liu, Qiang},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--12},
 title = {A Kernel Loss for Solving the Bellman Equation},
 volume = {32},
 year = {2019}
}

@inproceedings{Chen2021-policyInertia, 
title={Addressing Action Oscillations through Learning Policy Inertia}, 
volume={35}, 
number={8}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Chen, Chen and Tang, Hongyao and Hao, Jianye and Liu, Wulong and Meng, Zhaopeng},
year={2021}, 
month={May}, 
pages={7020-7027} 
}

@book{Bishop2006-PRML,
author = {Bishop, Christopher M.},
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
year = {2006},
isbn = {0387310738},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@article{Munos08-finiteTimeAVI,
  author  = {R{{\'e}}mi Munos and Csaba Szepesv{{\'a}}ri},
  title   = {Finite-Time Bounds for Fitted Value Iteration},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {27},
  pages   = {815-857},
}

@article{Handerson2018-DRLmatters,
  author    = {Peter Henderson and
               Riashat Islam and
               Philip Bachman and
               Joelle Pineau and
               Doina Precup and
               David Meger},
  title     = {Deep Reinforcement Learning that Matters},
  journal   = {CoRR},
  volume    = {abs/1709.06560},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.06560},
  archivePrefix = {arXiv},
  eprint    = {1709.06560},
}

@inproceedings{Tang2020-discretizeActionOnPolicy, 
title={Discretizing Continuous Action Space for On-Policy Optimization},
volume = {34}, 
number = {04}, 
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
author = {Tang, Yunhao and Agrawal, Shipra}, 
year = {2020}, 
pages = {5981-5988} 
}

@inproceedings{Mania2018-SimpleRandomSearchRL,
 author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--10},
 publisher = {Curran Associates, Inc.},
 title = {Simple random search of static linear policies is competitive for reinforcement learning},
 volume = {31},
 year = {2018}
}

@article{Shalev2012-onlineOptimization,
year = {2012},
volume = {4},
journal = {Foundations and Trends® in Machine Learning},
title = {Online Learning and Online Convex Optimization},
number = {2},
pages = {107-194},
author = {Shai Shalev-Shwartz}
}

@article{McMahan2017-surveyOnlineLearning,
  author  = {H. Brendan McMahan},
  title   = {A survey of Algorithms and Analysis for Adaptive Online Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {90},
  pages   = {1-50},
}

@book{krishnamurthy2016-POMDP, 
place={Cambridge}, 
title={Partially Observed Markov Decision Processes: From Filtering to Controlled Sensing}, 
publisher={Cambridge University Press}, 
author={Krishnamurthy, Vikram}, 
year={2016}
}

@inproceedings{Adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{Lazaric2012-LSPIsampleAnalysis,
  author  = {Alessandro Lazaric and Mohammad Ghavamzadeh and R{{\'e}}mi Munos},
  title   = {Finite-Sample Analysis of Least-Squares Policy Iteration},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {98},
  pages   = {3041-3074},
}

@article{Lazaric2016-classificationPolicyIterationAnalysis,
  author  = {Alessandro Lazaric and Mohammad Ghavamzadeh and R{\'e}mi Munos},
  title   = {Analysis of Classification-based Policy Iteration Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {19},
  pages   = {1-30},
}

@article{Antos2008-fitPolicyIteration,
author = {Antos, Andr\'{a}s and Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi},
title = {Learning Near-Optimal Policies with Bellman-Residual Minimization Based Fitted Policy Iteration and a Single Sample Path},
year = {2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {71},
number = {1},
journal = {Mach. Learn.},
month = apr,
pages = {89–129},
numpages = {41},
keywords = {Nonparametric regression, Least-squares temporal difference learning, Least-squares regression, Finite-sample bounds, Bellman-residual minimization, Reinforcement learning, Policy iteration, Off-policy learning}
}

@inproceedings{Rudi2018-fastLeverageScore,
 author = {Rudi, Alessandro and Calandriello, Daniele and Carratino, Luigi and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--11},
 title = {On Fast Leverage Score Sampling and Optimal Learning},
 volume = {31},
 year = {2018}
}

@book{Neal1996-BayesianNetwork,
author = {Neal, Radford M.},
title = {Bayesian Learning for Neural Networks},
year = {1996},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
}

@inproceedings{Munkhoeva2018-QuadratureRF,
 author = {Munkhoeva, Marina and Kapushev, Yermek and Burnaev, Evgeny and Oseledets, Ivan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--10},
 title = {Quadrature-based features for kernel approximation},
 volume = {31},
 year = {2018}
}

@InProceedings{Choromanski2018-geometryRF,
 title = {The Geometry of Random Features}, 
 author = {Krzysztof Choromanski and Mark Rowland and Tamas Sarlos and Vikas Sindhwani and Richard Turner and Adrian Weller}, 
 booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = {1--9}, 
  year = {2018},
  volume = {84}, 
  series = {Proceedings of Machine Learning Research}, 
}

@InProceedings{Farahat2011-greedyNystrom, 
title = {A novel greedy algorithm for {N}ystr\"om approximation}, 
author = {Ahmed Farahat and Ali Ghodsi and Mohamed Kamel}, 
booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, 
pages = {269--277},
 year = {2011}, 
 volume = {15}, 
 series = {Proceedings of Machine Learning Research}, 
 }

@article{Talwalkar2013-largeSVDManifold,
  author  = {Ameet Talwalkar and Sanjiv Kumar and Mehryar Mohri and Henry Rowley},
  title   = {Large-scale SVD and Manifold Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2013},
  volume  = {14},
  number  = {60},
  pages   = {3129-3152},
}

@InProceedings{Zhang2019-lowPrecisionRF, 
title = {Low-Precision Random Fourier Features for Memory-constrained Kernel Approximation}, 
author = {Zhang, Jian and May, Avner and Dao, Tri and Re, Christopher}, 
booktitle = {Proceedings of Machine Learning Research}, 
pages = {1264--1274}, 
year = {2019}, 
volume = {89}, 
series = {Proceedings of Machine Learning Research},
}

@inproceedings{Rawat2019-sampledSoftmaxRF,
 author = {Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix Xinnan X and Suresh, Ananda Theertha and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--11},
 title = {Sampled Softmax with Random Fourier Features},
 volume = {32},
 year = {2019}
}

@InProceedings{Choromanski19-unifyOrthogonalMC, 
title = {Unifying Orthogonal {M}onte {C}arlo Methods}, 
author = {Choromanski, Krzysztof and Rowland, Mark and Chen, Wenyu and Weller, Adrian}, 
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
pages = {1203--1212}, 
year = {2019},
volume = {97}, 
series = {Proceedings of Machine Learning Research},
}

@InProceedings{Li2019-unifiedAnalysisRF, 
title = {Towards a Unified Analysis of Random {F}ourier Features}, 
author = {Li, Zhu and Ton, Jean-Francois and Oglic, Dino and Sejdinovic, Dino}, 
booktitle = {Proceedings of the 36th International Conference on Machine Learning}, 
pages = {3905--3914}, 
year = {2019}, 
volume = {97}, 
series = {Proceedings of Machine Learning Research},
}

@InProceedings{cutajar2017-deepGP, 
title = {Random Feature Expansions for Deep {G}aussian Processes}, 
author = {Kurt Cutajar and Edwin V. Bonilla and Pietro Michiardi and Maurizio Filippone}, 
booktitle = {Proceedings of the 34th International Conference on Machine Learning}, 
pages = {884--893}, 
year = {2017}, 
volume = {70}, 
}

@inproceedings{Mukuta2018-semigroupRF,
	author = {Yusuke Mukuta and Yoshitaka Ushiku and Tatsuya Harada},
	title = {Alternating Circulant Random Features for Semigroup Kernels},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2018},
  pages = {3836--3843}
}

@article{Apsemidis2020-reviewKernelBasic,
title = {A review of machine learning kernel methods in statistical process monitoring},
journal = {Computers \& Industrial Engineering},
volume = {142},
pages = {106376},
year = {2020},
author = {Anastasios Apsemidis and Stelios Psarakis and Javier M. Moguerza},
}

@Article{Pilario2020-reviewKernelProcessComprehensive,
AUTHOR = {Pilario, Karl Ezra and Shafiee, Mahmood and Cao, Yi and Lao, Liyun and Yang, Shuang-Hua},
TITLE = {A Review of Kernel Methods for Feature Extraction in Nonlinear Process Monitoring},
JOURNAL = {Processes},
VOLUME = {8},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {24},
}


@article{Bach2017-equivalence,
author = {Bach, Francis},
title = {On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions},
year = {2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
pages = {714–751},
numpages = {38},
journal = {J. Mach. Learn. Res.},
}

@inproceedings{chen2015-shiftInvariant,
	author = {Xixian Chen and Haiqin Yang and Irwin King and Michael R. Lyu},
	title = {Training-Efficient Feature Map for Shift-Invariant Kernels},
	booktitle = {International Joint Conference on Artificial Intelligence},
	year = {2015},
  pages = {3395--3410},
}

@inproceedings{Yen2014-sparseRandomFeature,
 author = {Yen, Ian En-Hsu and Lin, Ting-Wei and Lin, Shou-De and Ravikumar, Pradeep K and Dhillon, Inderjit S},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 title = {Sparse Random Feature Algorithm as Coordinate Descent in Hilbert Space},
 volume = {27},
 year = {2014}
}

@inproceedings{Feng2015-RFCirculant,
author = {Feng, Chang and Hu, Qinghua and Liao, Shizhong},
title = {Random Feature Mapping with Signed Circulant Matrix Projection},
year = {2015},
abstract = {Random feature mappings have been successfully used for approximating non-linear kernels to scale up kernel methods. Some work aims at speeding up the feature mappings, but brings increasing variance of the approximation. In this paper, we propose a novel random feature mapping method that uses a signed Circulant Random Matrix (CRM) instead of an unstructured random matrix to project input data. The signed CRM has linear space complexity as the whole signed CRM can be recovered from one column of the CRM, and ensures loglinear time complexity to compute the feature mapping using the Fast Fourier Transform (FFT). Theoretically, we prove that approximating Gaussian kernel using our mapping method is unbiased and does not increase the variance. Experimentally, we demonstrate that our proposed mapping method is time and space efficient while retaining similar accuracies with state-of-the-art random feature mapping methods. Our proposed random feature mapping method can be implemented easily and make kernel methods scalable and practical for large scale training and predicting problems.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3490–3496},
numpages = {7},
series = {IJCAI'15}
}

@inproceedings{sun2018-linearSVMRF,
 author = {Sun, Yitong and Gilbert, Anna and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--10},
 title = {But How Does It Work in Theory? Linear SVM with Random Features},
 volume = {31},
 year = {2018}
}

@inproceedings{Chang2017-dataRFStein,
  author    = {Wei-Cheng Chang and Chun-Liang Li and Yiming Yang and Barnabás Póczos},
  title     = {Data-driven Random Fourier Features using Stein Effect},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {1497--1503},
  year      = {2017},
}

@inproceedings{kawaguchi2018-deepSemiRF,
title={Deep Semi-Random Features for Nonlinear Function Approximation},
author={Kawaguchi, Kenji and Xie, Bo and Verma, Vikas and Song, Le},
booktitle={Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)},
year={2018},
pages={3392-3389},
}

@inproceedings{Cho2009-deepKernelMethods,
 author = {Cho, Youngmin and Saul, Lawrence},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 title = {Kernel Methods for Deep Learning},
 volume = {22},
 year = {2009}
}

@InProceedings{agrawal2019-dataDependentCompressKernelApprox, 
title = {Data-dependent compression of random features for large-scale kernel approximation}, 
author = {Agrawal, Raj and Campbell, Trevor and Huggins, Jonathan and Broderick, Tamara}, 
booktitle = {Proceedings of Machine Learning Research}, 
pages = {1822--1831}, 
year = {2019}, 
volume = {89}, 
series = {Proceedings of Machine Learning Research}, 
 }

@inproceedings{Shahin2018-dataDependentRandomFeature,
	author = {Shahin Shahrampour and Ahmad Beirami and Vahid Tarokh},
	title = {On Data-Dependent Random Features for Improved Generalization in Supervised Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
  pages = {4026--4033}
}

@inproceedings{Carratino2018-SGDRandomFeature,
 author = {Carratino, Luigi and Rudi, Alessandro and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--12},
 title = {Learning with SGD and Random Features},
 volume = {31},
 year = {2018}
}

@inproceedings{Shen2017-randomFeatureMomentMatching,
	author = {Weiwei Shen and Zhihui Yang and Jun Wang},
	title = {Random Features for Shift-Invariant Kernels with Moment Matching},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2017},
  pages = {2520--2526}
}

@inproceedings{Li2019-AdaptiveRandomFeature, 
title={Learning Adaptive Random Features}, 
volume={33}, 
 number={01}, 
 booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
 author={Li, Yanjun and Zhang, Kai and Wang, Jun and Kumar, Sanjiv}, 
 year={2019}, 
 pages={4229-4236}
  }

@inproceedings{Sinha2016-learningKernel,
 author = {Sinha, Aman and Duchi, John C},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 title = {Learning Kernels with Random Features},
 volume = {29},
 year = {2016}
}

@inproceedings{Talwalkar2010-matrixCoeherence,
author = {Talwalkar, Ameet and Rostamizadeh, Afshin},
title = {Matrix Coherence and the Nystr\"{o}m Method},
year = {2010},
publisher = {AUAI Press},
booktitle = {Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
pages = {572–579},
numpages = {8},
series = {UAI'10}
}

@inproceedings{Rudi2017-generalizationRandom,
 author = {Rudi, Alessandro and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--10},
 title = {Generalization Properties of Learning with Random Features},
 volume = {30},
 year = {2017}
}

@inproceedings{Rudi2017-Falkon,
 author = {Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--10},
 publisher = {Curran Associates, Inc.},
 title = {FALKON: An Optimal Large Scale Kernel Method},
 volume = {30},
 year = {2017}
}


@inproceedings{David2007-Kmeans++,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {K-Means++: The Advantages of Careful Seeding},
year = {2007},
publisher = {Society for Industrial and Applied Mathematics},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
series = {SODA '07}
}

@inproceedings{Greg2003-learningKmeans,
author = {Hamerly, Greg and Elkan, Charles},
title = {Learning the k in k-Means},
year = {2003},
publisher = {MIT Press},
pages = {281–288},
numpages = {8},
location = {Whistler, British Columbia, Canada},
series = {NIPS'03}
}

@Book{blum2017foundations,
author = {Blum, Avrim and Hopcroft, John and Kannan, Ravi},
title = {Foundations of Data Science},
year = {2020},
pages = {1-465},
publisher = {Cambridge University Press},
}

@article{Wang2013-ImproveCUR-Nystrom,
author = {Wang, Shusen and Zhang, Zhihua},
title = {Improving CUR Matrix Decomposition and the Nystr\"{o}m Approximation via Adaptive Sampling},
year = {2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2729–2769},
numpages = {41},
}

@article{Zhang2010-ClusteredNystrom,  
author={K. Zhang and J. T. Kwok},  
journal={IEEE Transactions on Neural Networks},   
title={Clustered Nyström Method for Large Scale Manifold Learning and Dimension Reduction},   
year={2010},  
volume={21},  
number={10},  
pages={1576-1587}
}

@InProceedings{Ouimet2005-greedy, 
title = {Greedy Spectral Embedding}, 
author = {Marie Ouimet and Yoshua Bengio}, 
booktitle = {Proceedings of the Eighth International Conference on Artificial Intelligence and Statistics},
pages = {1--8}, 
year = {2005}, 
series = {Proceedings of Machine Learning Research},
}

@InProceedings{Lim2018-MultiscaleNystrom, 
title = {Multi-scale Nystrom Method}, 
author = {Woosang Lim and Rundong Du and Bo Dai and Kyomin Jung and Le Song and Haesun Park}, 
booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
pages = {68--76}, 
year = {2018}, 
volume = {84}, 
series = {Proceedings of Machine Learning Research},
}

@InProceedings{Lim2015-doubleNystrom, 
title = {Double Nystrom Method: An Efficient and Accurate Nystr\"om Scheme for Large-Scale Data Sets}, 
author = {Woosang Lim and Minhwan Kim and Haesun Park and Kyomin Jung}, 
booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, 
pages = {1367--1375}, 
year = {2015},
volume = {37}, 
series = {Proceedings of Machine Learning Research}
 }

@inproceedings{Cho2014-divideConquer-SVM,
author = {Hsieh, Cho-Jui and Si, Si and Dhillon, Inderjit S.},
title = {A Divide-and-Conquer Solver for Kernel Support Vector Machines},
year = {2014},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {I–566–I–574},
location = {Beijing, China},
series = {ICML'14}
}


@inproceedings{Cho2014-FastPrediction,
 author = {Hsieh, Cho-Jui and Si, Si and Dhillon, Inderjit S},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 publisher = {Curran Associates, Inc.},
 title = {Fast Prediction for Large-Scale Kernel Machines},
 volume = {27},
 year = {2014}
}



@article {Mahoney2009-CUR,
	author = {Mahoney, Michael W. and Drineas, Petros},
	title = {CUR matrix decompositions for improved data analysis},
	volume = {106},
	number = {3},
	pages = {697--702},
	year = {2009},
	publisher = {National Academy of Sciences},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{Halko2011-sturctureRandomness,
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
year = {2011},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {53},
number = {2},
journal = {SIAM Rev.},
pages = {217–288},
numpages = {72},
keywords = {rank-revealing QR factorization, principal component analysis, matrix approximation, parallel algorithm, eigenvalue decomposition, singular value decomposition, random matrix, pass-efficient algorithm, randomized algorithm, interpolative decomposition, Johnson-Lindenstrauss lemma, streaming algorithm, dimension reduction}
}


@inproceedings{Bach2005-lowRankIncompleteCholesky,
author = {Bach, Francis R. and Jordan, Michael I.},
title = {Predictive Low-Rank Decomposition for Kernel Methods},
year = {2005},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {33–40},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}


@inproceedings{Achlioptas2001-samplingKernel,
 author = {Achlioptas, Dimitris and Mcsherry, Frank and Sch\"{o}lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {335--342},
 publisher = {MIT Press},
 title = {Sampling Techniques for Kernel Methods},
 volume = {14},
 year = {2002}
}



@article{Fine2002-incompleteCholesky,
author = {Fine, Shai and Scheinberg, Katya},
title = {Efficient Svm Training Using Low-Rank Kernel Representations},
year = {2002},
publisher = {JMLR.org},
volume = {2},
journal = {J. Mach. Learn. Res.},
month = mar,
numpages = {22},
pages = {243-264},
}

@inproceedings{Smola2000-greedyBasis,
author = {Smola, Alex J. and Sch\"{o}kopf, Bernhard},
title = {Sparse Greedy Matrix Approximation for Machine Learning},
year = {2000},
booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
pages = {911–918},
numpages = {8},
series = {ICML '00}
}

@InProceedings{Weinberger2009-featureHashing,
author = {Weinberger, Kilian and Dasgupta, Anirban and Langford, John and Smola, Alex and Attenberg, Josh},
title = {Feature Hashing for Large Scale Multitask Learning},
year = {2009},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1113–1120},
numpages = {8}
}

@article{Woodruff2014-sketching,
author = {Woodruff, David P.},
title = {Sketching as a Tool for Numerical Linear Algebra},
year = {2014},
volume = {10},
number = {1–2},
pages = {1–157},
numpages = {157},
journal = {Foundations and Trends for Theoretical Computer Science},
publisher = {Now Publishers Inc.},
}

@inproceedings{Avron2014-subspacePolynomial,
 author = {Avron, Haim and Nguyen, Huy and Woodruff, David},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2258--2266},
 title = {Subspace Embeddings for the Polynomial Kernel},
 volume = {27},
 year = {2014}
}

@book{Steinwart2008-svm,
author = {Steinwart, Ingo and Christmann, Andreas},
title = {Support Vector Machines},
year = {2008},
isbn = {0387772413},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
}

@inproceedings{Musco2017-recursiveSamplingNystrom,
author = {Musco, Cameron and Musco, Christopher},
title = {Recursive Sampling for the Nystr\"{o}m Method},
year = {2017},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {3836–3848},
numpages = {13},
series = {NIPS'17}
}

@inproceedings{Rudi2015-NystromRegularization,
 author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1657--1665},
 title = {Less is More: Nystr\"{o}m Computational Regularization},
 volume = {28},
 year = {2015}
}



@inproceedings{Lu2013-fasterSubsampledHadamard,
 author = {Lu, Yichao and Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {369--377},
 title = {Faster Ridge Regression via the Subsampled Randomized Hadamard Transform},
 volume = {26},
 year = {2013}
}

@article{kimeldorf1970,
author = "Kimeldorf, George S. and Wahba, Grace",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "04",
number = "2",
pages = "495--502",
publisher = "The Institute of Mathematical Statistics",
title = "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines",
volume = "41",
year = "1970"
}

@article{Yu2018-circulantJMLR,
  author  = {Felix X. Yu and Aditya Bhaskara and Sanjiv Kumar and Yunchao Gong and Shih-Fu Chang},
  title   = {On Binary Embedding using Circulant Matrices},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {150},
  pages   = {1-30},
}

@inproceedings{Yu2014-circulantBinary,
author = {Yu, Felix X. and Kumar, Sanjiv and Gong, Yunchao and Chang, Shih-Fu},
title = {Circulant Binary Embedding},
year = {2014},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–946–II–954},
location = {Beijing, China},
series = {ICML'14}
}

@InProceedings{Zhang2012-scaleKernelSVM, 
title = {Scaling up Kernel SVM on Limited Resources: A Low-rank Linearization Approach}, 
author = {Kai Zhang and Liang Lan and Zhuang Wang and Fabian Moerchen}, 
booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
 pages = {1425--1434}, 
 year = {2012}, 
 volume = {22}, 
 series = {Proceedings of Machine Learning Research}, 
 publisher = {PMLR}, 
}

@inproceedings{Zhang2008-improvedNystrom,
author = {Zhang, Kai and Tsang, Ivor W. and Kwok, James T.},
title = {Improved Nystr\"{o}m Low-Rank Approximation and Error Analysis},
year = {2008},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1232–1239},
numpages = {8},
series = {ICML '08}
}

@InProceedings{Si2016-fastNystromTransform, 
title = {Computationally Efficient Nystr\"{o}m Approximation using Fast Transforms}, 
author = {Si Si and Cho-Jui Hsieh and Inderjit Dhillon}, 
booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, 
pages = {2655--2663}, 
year = {2016}, 
volume = {48}, 
series = {Proceedings of Machine Learning Research}, 
publisher = {PMLR}
}

@inproceedings{Li2010-largeNystrom,
author = {Li, Mu and Kwok, James T. and Lu, Bao-Liang},
title = {Making Large-Scale Nystr\"{o}m Approximation Possible},
year = {2010},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {631–638},
numpages = {8},
series = {ICML'10}
}

@InProceedings{Hamid2014-compactRF, 
title = {Compact Random Feature Maps},
 author = {Raffay Hamid and Ying Xiao and Alex Gittens and Dennis Decoste},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
   pages = {19--27}, 
   year = {2014},
   volume = {32}, 
   number = {2}, 
   series = {Proceedings of Machine Learning Research}, 
   publisher = {PMLR},
}

@inproceedings{Sindhwani2015-smallFootprint,
 author = {Sindhwani, Vikas and Sainath, Tara and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3088--3096},
 title = {Structured Transforms for Small-Footprint Deep Learning},
 volume = {28},
 year = {2015}
}

@article{Drineas2011-FasterLeastSquares,
author = {Drineas, Petros and Mahoney, Michael W. and Muthukrishnan, S. and Sarl\'{o}s, Tam\'{a}s},
title = {Faster Least Squares Approximation},
year = {2011},
issue_date = {February 2011},
publisher = {Springer-Verlag},
volume = {117},
number = {2},
journal = {Numer. Math.},
month = feb,
pages = {219–249},
numpages = {31}
}

@article{Drineas2012-ApproxmationMatrixCoherence,
author = {Drineas, Petros and Magdon-Ismail, Malik and Mahoney, Michael W. and Woodruff, David P.},
title = {Fast Approximation of Matrix Coherence and Statistical Leverage},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {3475–3506},
numpages = {32},
}

@INPROCEEDINGS{Boutsidis2011-colMatrixReconstruct,  
author={C. {Boutsidis} and P. {Drineas} and M. {Magdon-Ismail}},  
booktitle={2011 IEEE 52nd Annual Symposium on Foundations of Computer Science},   
title={Near Optimal Column-Based Matrix Reconstruction},   
year={2011},  
pages={305-314},  
}

@article{Gittens2016-revisitNystrom,
author = {Gittens, Alex and Mahoney, Michael W.},
title = {Revisiting the Nystr\"{o}m Method for Improved Large-Scale Machine Learning},
year = {2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3977–4041},
numpages = {65},
}

@InProceedings{Li2010-skewedHistogram,
author="Li, Fuxin
and Ionescu, Catalin
and Sminchisescu, Cristian",
title="Random Fourier Approximations for Skewed Multiplicative Histogram Kernels",
booktitle="Pattern Recognition",
year="2010",
pages="262--271",
}


@book{Owen2013-MCBook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples},
   publisher = {Stanford University},
}

@article{dick2013-qmc, 
title={High-dimensional integration: The quasi-Monte Carlo way}, 
volume={22}, 
journal={Acta Numerica}, 
publisher={Cambridge University Press}, 
author={Dick, Josef and Kuo, Frances Y. and Sloan, Ian H.}, 
year={2013}, 
pages={133–288}}

@inproceedings{Ailon2006-ANN-FJLT,
author = {Ailon, Nir and Chazelle, Bernard},
title = {Approximate Nearest Neighbors and the Fast Johnson-Lindenstrauss Transform},
year = {2006},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the Thirty-Eighth Annual ACM Symposium on Theory of Computing},
pages = {557–563},
numpages = {7},
series = {STOC '06}
}


@article{Huang2014-random-insight,
author = {Huang, Guang-Bin},
year = {2014},
month = {09},
pages = {376-390},
title = {An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels},
volume = {6},
journal = {Cognitive Computation},
}

@book{golub13,
  author = {Golub, Gene H. and van Loan, Charles F.},
  edition = {Fourth},
  publisher = {Johns Hopkins University Press},
  title = {Matrix Computations},
  year = 2013
}




@InProceedings{choromanska16-binaryEmbeddings, 
title = {Binary embeddings with structured hashed projections}, 
author = {Anna Choromanska and Krzysztof Choromanski and Mariusz Bojarski and Tony Jebara and Sanjiv Kumar and Yann LeCun}, 
booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, 
pages = {344--353}, 
year = {2016}, 
volume = {48}, 
series = {Proceedings of Machine Learning Research}, 
month = {20--22 Jun}, 
publisher = {PMLR}
}


@inproceedings{Yu2015-SphericalPolynomial,
 author = {Pennington, Jeffrey and Yu, Felix Xinnan X and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1846--1854},
 publisher = {Curran Associates, Inc.},
 title = {Spherical Random Features for Polynomial Kernels},
 volume = {28},
 year = {2015}
}



@inproceedings{Rahimi2009-RKS,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1313--1320},
 publisher = {Curran Associates, Inc.},
 title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
 volume = {21},
 year = {2009}
}


@article{Bochner1933,
title = {Monotone Funktionen, Stieltjessche Integrale und harmonische Analyse},
journal = {Mathematische Annalen},
volume = {108},
number = {33},
pages = {378-410},
year = {1933},
author = {S. Bochner},
}

@book{wendland2004, 
place={Cambridge}, 
series={Cambridge Monographs on Applied and Computational Mathematics},
title={Scattered Data Approximation}, 
publisher={Cambridge University Press}, 
author={Wendland, Holger}, 
year={2004}, 
collection={Cambridge Monographs on Applied and Computational Mathematics}}



@article{LEE2004-ICA-process,
title = {Statistical process monitoring with independent component analysis},
journal = {Journal of Process Control},
volume = {14},
number = {5},
pages = {467-485},
year = {2004},
issn = {0959-1524},
author = {Jong-Min Lee and ChangKyoo Yoo and In-Beum Lee},
}

@article{CHENG2004-JITL,
title = {A new data-based methodology for nonlinear process modeling},
journal = {Chemical Engineering Science},
volume = {59},
number = {13},
pages = {2801-2810},
year = {2004},
issn = {0009-2509},
author = {Cheng Cheng and Min-Sen Chiu},
}

@inproceedings{cybenko1996-JITL,
author = {G. Cybenko},
title = {Just-in-time learning and estimation},
year = {1996},
volume = {153},
booktitle = {NATO ASI Series. F. Computer and System Sciences  (NATO ASI Ser F)},
pages = {423-434},
numpages = {12},
}

@article{LIN2007-systematic-softSensor,
title = {A systematic approach for soft sensor development},
journal = {Computers \& Chemical Engineering},
volume = {31},
number = {5},
pages = {419-425},
year = {2007},
note = {ESCAPE-15},
issn = {0098-1354},
author = {Bao Lin and Bodil Recke and Jørgen K.H. Knudsen and Sten Bay Jørgensen},
}

@article{MOES2008-linear-tabletProcess,
title = {Application of process analytical technology in tablet process development using NIR spectroscopy: Blend uniformity, content uniformity and coating thickness measurements},
journal = {International Journal of Pharmaceutics},
volume = {357},
number = {1},
pages = {108-118},
year = {2008},
issn = {0378-5173},
author = {Johannes J. Moes and Marco M. Ruijken and Erik Gout and Henderik W. Frijlink and Michael I. Ugwoke},
}

@article{BERTHIAUX2006-PCA-homogeneity,
title = {Principal component analysis for characterising homogeneity in powder mixing using image processing techniques},
journal = {Chemical Engineering and Processing: Process Intensification},
volume = {45},
number = {5},
pages = {397-403},
year = {2006},
issn = {0255-2701},
author = {H. Berthiaux and V. Mosorov and L. Tomczak and C. Gatumel and J.F. Demeyre},
keywords = {Powder mixing, Homogeneity criterion, Image analysis, Principal component analysis},
}

@article{JOEQIN1998-recursivePLS-processMonitoring,
title = {Recursive PLS algorithms for adaptive data modeling},
journal = {Computers \& Chemical Engineering},
volume = {22},
number = {4},
pages = {503-514},
year = {1998},
issn = {0098-1354},
author = {S. {Joe Qin}},
}

@article{LI2000-recursivePCA-processMonitoring,
title = {Recursive PCA for adaptive process monitoring},
journal = {Journal of Process Control},
volume = {10},
number = {5},
pages = {471-486},
year = {2000},
issn = {0959-1524},
author = {Weihua Li and H.Henry Yue and Sergio Valle-Cervantes and S.Joe Qin},
}

@inproceedings{williams2000-Nystrom,
 author = {Williams, Christopher and Seeger, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {682--688},
 publisher = {MIT Press},
 title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
 volume = {13},
 year = {2001}
}


@inproceedings{Saunders1998-krr,
author = {Saunders, Craig and Gammerman, Alexander and Vovk, Volodya},
title = {Ridge Regression Learning Algorithm in Dual Variables},
year = {1998},
isbn = {1558605568},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
pages = {515–521},
numpages = {7},
series = {ICML '98}
}

@book{Scholkopf-learningWithKernels,
author = {Scholkopf, Bernhard and Smola, Alexander J.},
title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
year = {2001},
isbn = {0262194759},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {From the Publisher:In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.   Learning with Kernels  provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}
}

@inproceedings{Xie2015-doublyRF-NCA,
 author = {Xie, Bo and Liang, Yingyu and Song, Le},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2341--2349},
 publisher = {Curran Associates, Inc.},
 title = {Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients},
 volume = {28},
 year = {2015}
}


@article{Fan2014-GMM-JIL-softsensing,
author = {Fan, Miao and Ge, Zhiqiang and Song, Zhihuan},
title = {Adaptive Gaussian Mixture Model-Based Relevant Sample Selection for JITL Soft Sensor Development},
journal = {Industrial \& Engineering Chemistry Research},
volume = {53},
number = {51},
pages = {19979-19986},
year = {2014},
}



@article{KIM2011-LWPLS-pharmaceutical,
title = {Estimation of active pharmaceutical ingredients content using locally weighted partial least squares and statistical wavelength selection},
journal = {International Journal of Pharmaceutics},
volume = {421},
number = {2},
pages = {269-274},
year = {2011},
issn = {0378-5173},
author = {Sanghong Kim and Manabu Kano and Hiroshi Nakagawa and Shinji Hasebe},
}

@article{KIM2013-LWPLS-similarity,
title = {Development of soft-sensor using locally weighted PLS with adaptive similarity measure},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {124},
pages = {43-49},
year = {2013},
author = {Sanghong Kim and Ryota Okajima and Manabu Kano and Shinji Hasebe},
}

@article{ZHANG2017-LWKPRS-virtualsensing,
title = {Locally weighted kernel partial least squares regression based on sparse nonlinear features for virtual sensing of nonlinear time-varying processes},
journal = {Computers \& Chemical Engineering},
volume = {104},
pages = {164-171},
year = {2017},
author = {Xinmin Zhang and Manabu Kano and Yuan Li},
}

@article{Yuan2015-PPCA,
title = {Nonlinear feature extraction for soft sensor modeling based on weighted probabilistic PCA},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {147},
pages = {167-175},
year = {2015},
issn = {0169-7439},
author = {Xiaofeng Yuan and Lingjian Ye and Liang Bao and Zhiqiang Ge and Zhihuan Song},
}

@article{Yuan2014-LWKPCR-process,
author = {Yuan, Xiaofeng and Ge, Zhiqiang and Song, Zhihuan},
title = {Locally Weighted Kernel Principal Component Regression Model for Soft Sensing of Nonlinear Time-Variant Processes},
journal = {Industrial \& Engineering Chemistry Research},
volume = {53},
number = {35},
pages = {13736-13749},
year = {2014},
}


@InProceedings{Lyu2017-sphericalRF, 
title = {Spherical Structured Feature Maps for Kernel Approximation},
 author = {Yueming Lyu}, 
 booktitle = {Proceedings of the 34th International Conference on Machine Learning}, 
 pages = {2256--2264}, 
 year = {2017}, 
 }

@InProceedings{Wilson2016-DKL, 
title = {Deep Kernel Learning}, 
author = {Andrew Gordon Wilson and Zhiting Hu and Ruslan Salakhutdinov and Eric P. Xing}, 
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics}, 
pages = {370--378}, 
year = {2016}, 
}

@InProceedings{Avron2017-RF-KRR, 
title = {Random {F}ourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees}, 
author = {Haim Avron and Michael Kapralov and Cameron Musco and Christopher Musco and Ameya Velingker and Amir Zandieh}, 
booktitle = {Proceedings of the 34th International Conference on Machine Learning}, 
pages = {253--262}, 
year = {2017}, 
}

@article{Bojarski2016-structuredRF,
  author    = {Mariusz Bojarski and
               Anna Choromanska and
               Krzysztof Choromanski and
               Francois Fagan and
               C{\'{e}}dric Gouy{-}Pailler and
               Anne Morvan and
               Nourhan Sakr and
               Tam{\'{a}}s Sarl{\'{o}}s and
               Jamal Atif},
  title     = {Structured adaptive and random spinners for fast machine learning
               computations},
  journal   = {CoRR},
  volume    = {abs/1610.06209},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1610.06209},
}

@inproceedings{Alaoui2015-randomizedKRR-Nystrom,
 author = {Alaoui, Ahmed and Mahoney, Michael W},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {775--783},
 publisher = {Curran Associates, Inc.},
 title = {Fast Randomized Kernel Ridge Regression with Statistical Guarantees},
 volume = {28},
 year = {2015}
}


@inproceedings{Sutherland2015-errorFourier,
author = {Sutherland, Dougal J. and Schneider, Jeff},
title = {On the Error of Random Fourier Features},
year = {2015},
isbn = {9780996643108},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
pages = {862–871},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {UAI'15}
}

@inproceedings{Sriperumbudur2015-optimalRates,
author = {Sriperumbudur, Bharath K. and Szab\'{o}, Zolt\'{a}n},
title = {Optimal Rates for Random Fourier Features},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1144–1152},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{Dai2014-doublyKernel,
 author = {Dai, Bo and Xie, Bo and He, Niao and Liang, Yingyu and Raj, Anant and Balcan, Maria-Florina F and Song, Le},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3041--3049},
 publisher = {Curran Associates, Inc.},
 title = {Scalable Kernel Methods via Doubly Stochastic Gradients},
 volume = {27},
 year = {2014}
}

@InProceedings{choromanski2016-recycle, 
title = {Recycling Randomness with Structure for Sublinear time Kernel Expansions}, 
author = {Krzysztof Choromanski and Vikas Sindhwani}, 
booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, 
pages = {2502--2510}, 
year = {2016}, 
}

@InProceedings{yang2015-AlaCarte, 
title = {{A la Carte -- Learning Fast Kernels}}, 
author = {Zichao Yang and Andrew Wilson and Alex Smola and Le Song}, 
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics}, 
pages = {1098--1106}, 
year = {2015}, 
series = {Proceedings of Machine Learning Research},
}

@article{Si2017-memoryEfficient,
  author  = {Si Si and Cho-Jui Hsieh and Inderjit S. Dhillon},
  title   = {Memory Efficient Kernel Approximation},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {20},
  pages   = {1-32},
}

@article{Avron2016-quasiMC,
  author  = {Haim Avron and Vikas Sindhwani and Jiyan Yang and Michael W. Mahoney},
  title   = {Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {120},
  pages   = {1-38},
}

@inproceedings{Pham2013-polynomial,
author = {Pham, Ninh and Pagh, Rasmus},
title = {Fast and Scalable Polynomial Kernels via Explicit Feature Maps},
year = {2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {239–247},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@article{Kumar2012-samplingNystrom,
author = {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
title = {Sampling Methods for the Nystr\"{o}m Method},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
journal = {J. Mach. Learn. Res.},
month = apr,
pages = {981–1006},
numpages = {26},
}

@inproceedings{Yang2012-NystromVsRandom,
 author = {Yang, Tianbao and Li, Yu-feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {476--484},
 publisher = {Curran Associates, Inc.},
 title = {Nystr\"{o}m Method vs Random Fourier Features: A Theoretical and Empirical Comparison},
 volume = {25},
 year = {2012}
}


@inproceedings{Alexandr2015-LSH,
 author = {Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1225--1233},
 publisher = {Curran Associates, Inc.},
 title = {Practical and Optimal LSH for Angular Distance},
 volume = {28},
 year = {2015}
}

@InProceedings{Kar2012-dotProduct, 
title = {Random Feature Maps for Dot Product Kernels}, 
author = {Purushottam Kar and Harish Karnick}, 
booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics}, 
pages = {583--591}, 
year = {2012},
series = {Proceedings of Machine Learning Research}, 
}

@InProceedings{Cortes2010-kernelAccuracy, 
title = {On the Impact of Kernel Approximation on Learning Accuracy}, 
author = {Corinna Cortes and Mehryar Mohri and Ameet Talwalkar}, 
booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, 
pages = {113--120}, 
year = {2010},
volume = {9}, 
series = {Proceedings of Machine Learning Research}, 
address = {Chia Laguna Resort, Sardinia, Italy}, 
month = {13--15 May}, 
publisher = {JMLR Workshop and Conference Proceedings}
}

@ARTICLE{Vedaldi2012-additive,  
author={A. {Vedaldi} and A. {Zisserman}},  
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Efficient Additive Kernels via Explicit Feature Maps},   
year={2012},  
volume={34},  
number={3},  
pages={480-492}
}

@article{Drineas2005-Nystrom,
author = {Drineas, Petros and Mahoney, Michael W.},
title = {On the Nystr\"{o}m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning},
year = {2005},
issue_date = {12/1/2005},
publisher = {JMLR.org},
volume = {6},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {2153–2175},
numpages = {23}
}



@inproceedings{Gabillon2013-ADPTetris,
 author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Scherrer, Bruno},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 publisher = {Curran Associates, Inc.},
 title = {Approximate Dynamic Programming Finally Performs Well in the Game of Tetris},
 volume = {26},
 year = {2013}
}



@inproceedings{Papini2017-adaptiveBatchSizePG,
 author = {Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--10},
 title = {Adaptive Batch Size for Safe Policy Gradients},
 volume = {30},
 year = {2017}
}

@article{papini2019-smoothSafePG,
  author    = {Matteo Papini and
               Matteo Pirotta and
               Marcello Restelli},
  title     = {Smoothing Policies and Safe Policy Gradients},
  journal   = {CoRR arXiv:abs/1905.03231},
  volume    = {abs/1905.03231},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.03231},
}


@InProceedings{papini20-balanceSpeedStabilityPG, 
title = {Balancing Learning Speed and Stability in Policy Gradient via Adaptive Exploration}, 
author = {Papini, Matteo and Battistello, Andrea and Restelli, Marcello}, 
booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, 
pages = {1188--1199}, 
year = {2020}, 
volume = {108}, 
series = {Proceedings of Machine Learning Research},
}

@inproceedings{Pirotta13-adaptiveStepSizePG,
 author = {Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--9},
 title = {Adaptive Step-Size for Policy Gradient Methods},
 volume = {26},
 year = {2013}
}




@inproceedings{Liu2019-NeuralPpoTrpoGlobalConvergence,
 author = {Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--12},
 title = {Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy},
 volume = {32},
 year = {2019}
}



@InProceedings{Hazan2019-maxEntExploration, 
title = {Provably Efficient Maximum Entropy Exploration}, 
author = {Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby}, 
booktitle = {Proceedings of the 36th International Conference on Machine Learning}, 
pages = {2681--2691}, 
year = {2019}, 
volume = {97},
series = {Proceedings of Machine Learning Research}, 
}

@InProceedings{Fujimoto18-addressingApproximationError,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author =       {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  volume = 	 {80},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}
@article{chen2021addressing,
      title={Addressing Action Oscillations through Learning Policy Inertia}, 
      author={Chen Chen and Hongyao Tang and Jianye Hao and Wulong Liu and Zhaopeng Meng},
      year={2021},
      eprint={arXiv preprint, arXiv:2103.02287},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url = {https://arxiv.org/pdf/2103.02287.pdf},
}


@inproceedings{engstrom2020implementation,
      title={Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO}, 
      author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
      year={2019},
      booktitle={International Conference on Learning Representations (ICLR)},
      pages = {1--12},
      }

@book{Beck2017-firstOrder,
author = {Beck, Amir},
title = {First-Order Methods in Optimization},
publisher = {Society for Industrial and Applied Mathematics},
year = {2017},
address = {Philadelphia, PA},
}


@article{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      journal={arXiv:1707.06347},
      primaryClass={cs.LG},
      url={https://arxiv.org/pdf/1707.06347.pdf},
}

@InProceedings{Scherrer14-approximatePIcompare, 
title = {Approximate Policy Iteration Schemes: A Comparison}, 
author = {Bruno Scherrer}, 
booktitle = {Proceedings of the 31st International Conference on Machine Learning}, 
pages = {1314--1322}, 
year = {2014}, 
volume = {32}, 
number = {2}, 
series = {Proceedings of Machine Learning Research},  
}


@InProceedings{Scherrer2014-localPolicySearch,
author="Scherrer, Bruno
and Geist, Matthieu",
title="Local Policy Search in a Convex Space and Conservative Policy Iteration as Boosted Policy Search",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2014",
pages="35--50",
}


@article{tomar2021mirror,
      title={Mirror Descent Policy Optimization}, 
      author={Manan Tomar and Lior Shani and Yonathan Efroni and Mohammad Ghavamzadeh},
      year={2021},
      journal={arXiv preprint, arXiv:2005.09814},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/pdf/2005.09814.pdf},
}

@InProceedings{Agarwal20-theoryPolicyGradient, 
title = {Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes}, 
author = {Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
booktitle = {Proceedings of Thirty Third Conference on Learning Theory}, 
pages = {64--66}, 
year = {2020}, 
volume = {125}, 
series = {Proceedings of Machine Learning Research},
}

@inproceedings{Mei2019-principledEntropySearch,
  title     = {On Principled Entropy Exploration in Policy Optimization},
  author    = {Mei, Jincheng and Xiao, Chenjun and Huang, Ruitong and Schuurmans, Dale and Müller, Martin},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  pages     = {3130--3136},
  year      = {2019},
}


@article{zhu2020ensuring,
    title={Ensuring Monotonic Policy Improvement in Entropy-regularized Value-based Reinforcement Learning},
    author={Lingwei Zhu and Takamitsu Matsubara},
    year={2020},
    journal={arXiv preprint, arXiv:2008.10806},
    url             = {https://arxiv.org/pdf/2008.10806.pdf},
}

@InProceedings{Metelli18-configurable,
  title = 	 {Configurable {M}arkov Decision Processes},
  author =       {Metelli, Alberto Maria and Mutti, Mirco and Restelli, Marcello},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3491--3500},
  year = 	 {2018},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
}


@article{Lior19-adaptiveTRPO,
  author    = {Lior Shani and
               Yonathan Efroni and
               Shie Mannor},
  title     = {Adaptive Trust Region Policy Optimization: Global Convergence and
               Faster Rates for Regularized MDPs},
  journal   = {CoRR},
  volume    = {abs/1909.02769},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.02769},
}

@InProceedings{mei20b-globalConvergence, 
title = {On the Global Convergence Rates of Softmax Policy Gradient Methods}, 
author = {Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale}, 
booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
pages = {6820--6829}, 
year = {2020}, 
volume = {119}, 
}

@article{Bretagnolle-betterTVKL,
     author = {Bretagnolle, Jean and Huber, Catherine},
     title = {Estimation des densit\'es : risque minimax},
     journal = {S\'eminaire de probabilit\'es de Strasbourg},
     pages = {342--363},
     publisher = {Springer - Lecture Notes in Mathematics},
     volume = {12},
     year = {1978},
     mrnumber = {520011},
     language = {fr},
}

@misc{clement-betterKLTV, 
author = {Clement Canonne},
title={Better bounds relating Total Variation and KL divergence}, 
howpublished = {\url{https://twitter.com/ccanonne_/status/1339337913532784640}}, 
journal={Twitter}, 
year={2020},
publisher={Twitter}
}

@book{tsybakov-nonparametric,
author = {Tsybakov, Alexandre B.},
title = {Introduction to Nonparametric Estimation},
year = {2008},
isbn = {0387790519},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This is a concise text developed from lecture notes and ready to be used for a course on the graduate level. The main idea is to introduce the fundamental concepts of the theory while maintaining the exposition suitable for a first approach in the field. Therefore, the results are not always given in the most general form but rather under assumptions that lead to shorter or more elegant proofs. The book has three chapters. Chapter 1 presents basic nonparametric regression and density estimators and analyzes their properties. Chapter 2 is devoted to a detailed treatment of minimax lower bounds. Chapter 3 develops more advanced topics: Pinskers theorem, oracle inequalities, Stein shrinkage, and sharp minimax adaptivity. This book will be useful for researchers and grad students interested in theoretical aspects of smoothing techniques. Many important and useful results on optimal and adaptive estimation are provided. As one of the leading mathematical statisticians working in nonparametrics, the author is an authority on the subject.}
}


@inproceedings{ azar18-noisynet,
title	= {Noisy Networks for Exploration},
author	= {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alexander Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
year	= {2018},
booktitle	= {Proceedings of the International Conference on Representation Learning (ICLR 2018)},
address	= {Vancouver (Canada)}
}



@article{bellemare13-arcade-jair,
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
year = {2013},
volume = {47},
number = {1},
issn = {1076-9757},
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
journal = {Journal of Artificial Intelligence Research},
pages = {253–279},
numpages = {27}
}


@article{xiao10a-dualAveraging,
  author  = {Lin Xiao},
  title   = {Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {88},
  pages   = {2543-2596},
}


@InProceedings{dabney2018-IQN, 
title = {Implicit Quantile Networks for Distributional Reinforcement Learning}, 
author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Remi}, 
pages = {1096--1105}, 
year = {2018}, 
volume = {80}, 
series = {Proceedings of Machine Learning Research},  
publisher = {PMLR},
abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm’s implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.} 
}

@inproceedings{Bellemare2017-distributionRL,
author = {Bellemare, Marc G. and Dabney, Will and Munos, R\'{e}mi},
title = {A Distributional Perspective on Reinforcement Learning},
year = {2017},
abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {449–458},
numpages = {10},
}

@book{Feinberg02-handbookMDP,
author = {Feinberg, Eugene and Shwartz, Adam},
year = {2002},
month = {01},
pages = {},
title = {Handbook of Markov Decision Processes: Methods and Applications},
volume = {40},
isbn = {0792374592},
doi = {10.1007/978-1-4615-0805-2}
}

@phdthesis{ziebart2010-phd,
author = {Ziebart, Brian D.},
advisor = {Bagnell, J. Andrew},
title = {Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
year = {2010},
isbn = {9781124414218},
publisher = {Carnegie Mellon University},
address = {Carnegie Mellon University},
abstract = {Predicting human behavior from a small amount of training examples is a challenging machine learning problem. In this thesis, we introduce the principle of maximum causal entropy, a general technique for applying information theory to decision-theoretic, game-theoretic, and control settings where relevant information is sequentially revealed over time. This approach guarantees decision-theoretic performance by matching purposeful measures of behavior (Abbeel &amp; Ng, 2004), and/or enforces game-theoretic rationality constraints (Aumann, 1974), while otherwise being as uncertain as possible, which minimizes worst-case predictive log-loss (Gr\"{u}nwald &amp; Dawid, 2003). We derive probabilistic models for decision, control, and multi-player game settings using this approach. We then develop corresponding algorithms for efficient inference that include relaxations of the Bellman equation (Bellman, 1957), and simple learning algorithms based on convex optimization. We apply the models and algorithms to a number of behavior prediction tasks. Specifically, we present empirical evaluations of the approach in the domains of vehicle route preference modeling using over 100,000 miles of collected taxi driving data, pedestrian motion modeling from weeks of indoor movement data, and robust prediction of game play in stochastic multi-player games.}
}

@article{Neu17-unified,
  author    = {Gergely Neu and
               Anders Jonsson and
               Vicen{\c{c}} G{\'{o}}mez},
  title     = {A unified view of entropy-regularized Markov decision processes},
  journal   = {arXiv:1705.07798},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07798},
}

@inproceedings{Baird1999-gradient-actionGap,
author = {Baird, Leemon and Moore, Andrew},
title = {Gradient Descent for General Reinforcement Learning},
year = {1999},
booktitle = {Proceedings of the 1998 Conference on Advances in Neural Information Processing Systems II},
pages = {968–974},
numpages = {7}
}


@inproceedings{Martins16-sparsemax,
author = {Martins, Andr\'{e} F. T. and Astudillo, Ram\'{o}n F.},
title = {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
year = {2016},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
pages = {1614–1623},
}


@InProceedings{Nachum18a-tsallis, 
title = {Path Consistency Learning in {T}sallis Entropy Regularized {MDP}s}, 
author = {Chow, Yinlam and Nachum, Ofir and Ghavamzadeh, Mohammad}, 
pages = {979--988}, 
year = {2018}, 
booktitle = {International Conference on Machine Learning}
}


@incollection{vieillard2020munchausen,
      title={Munchausen Reinforcement Learning}, 
      author={Nino Vieillard and Olivier Pietquin and Matthieu Geist},
      booktitle = {Advances in Neural Information Processing Systems 33},
      year={2020},
      pages = {1--11},
}


@incollection{Nachum2017-bridgeGap,
title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {2775--2785},
year = {2017},
}



@InProceedings{ahmed19-entropy-policyOptimization, 
title = {Understanding the Impact of Entropy on Policy Optimization}, 
author = {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale}, 
pages = {151--160}, 
year = {2019}, 
volume = {97}, 
booktitle = {Proceedings of 36th International Conference on Machine Learning}, 
abstract = {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.} 
}

@article{busoniu08-multiagent-survey,
author = {Busoniu, L. and Babuska, R. and De Schutter, B.},
title = {A Comprehensive Survey of Multiagent Reinforcement Learning},
year = {2008},
issue_date = {March 2008},
publisher = {IEEE Press},
volume = {38},
number = {2},
issn = {1094-6977},
abstract = {Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.},
journal = {Transactions on Systems, Man and Cybernetics, Part C},
month = mar,
pages = {156–172},
numpages = {17}
}

@inproceedings{stone2012-humanMDPreward,
author = {Knox, W. Bradley and Stone, Peter},
title = {Reinforcement Learning from Simultaneous Human and MDP Reward},
year = {2012},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
abstract = {As computational agents are increasingly used beyond research labs, their success will depend on their ability to learn new skills and adapt to their dynamic, complex environments. If human users---without programming skills---can transfer their task knowledge to agents, learning can accelerate dramatically, reducing costly trials. The tamer framework guides the design of agents whose behavior can be shaped through signals of approval and disapproval, a natural form of human feedback. More recently, tamer+rl was introduced to enable human feedback to augment a traditional reinforcement learning (RL) agent that learns from a Markov decision process's (MDP) reward signal. We address limitations of prior work on tamer and tamer+rl, contributing in two critical directions. First, the four successful techniques for combining human reward with RL from prior tamer+rl work are tested on a second task, and these techniques' sensitivities to parameter changes are analyzed. Together, these examinations yield more general and prescriptive conclusions to guide others who wish to incorporate human knowledge into an RL algorithm. Second, tamer+rl has thus far been limited to a sequential setting, in which training occurs before learning from MDP reward. In this paper, we introduce a novel algorithm that shares the same spirit as tamer+rl but learns simultaneously from both reward sources, enabling the human feedback to come at any time during the reinforcement learning process. We call this algorithm simultaneous tamer+rl. To enable simultaneous learning, we introduce a new technique that appropriately determines the magnitude of the human model's influence on the RL algorithm throughout time and state-action space.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {475–482},
numpages = {8},
location = {Valencia, Spain},
series = {AAMAS '12}
}



@InProceedings{vieillard2020leverage,
    title={Leverage the Average: an Analysis of Regularization in RL},
    author={Nino Vieillard and Tadashi Kozuno and Bruno Scherrer and Olivier Pietquin and Rémi Munos and Matthieu Geist},
    year={2020},
    booktitle = {Advances in Neural Information Processing Systems 33},
    pages = {1--12},
}

@article{scherrer15-AMPI,
author = {Scherrer, Bruno and Ghavamzadeh, Mohammad and Gabillon, Victor and Lesner, Boris and Geist, Matthieu},
title = {Approximate Modified Policy Iteration and Its Application to the Game of Tetris},
year = {2015},
volume = {16},
number = {1},
abstract = {Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unify those for approximate policy and value iteration. We develop the finite-sample analysis of these algorithms, which highlights the influence of their parameters. In the classification-based version of the algorithm (CBMPI), the analysis shows that MPI's main parameter controls the balance between the estimation error of the classifier and the overall value function approximation. We illustrate and evaluate the behavior of these new algorithms in the Mountain Car and Tetris problems. Remarkably, in Tetris, CBMPI outperforms the existing DP approaches by a large margin, and competes with the current state-of-the-art methods while using fewer samples.},
journal = {Journal of Machine Learning Research},
pages = {1629–1676},
}


@InProceedings{geist19-regularized, 
title = {A Theory of Regularized {M}arkov Decission Processes}, 
author = {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier}, 
pages = {2160--2169}, 
year = {2019}, 
volume = {97}, 
booktitle = 	 {36th International Conference on Machine Learning},
}

@book{cvx-opt-boyd,
author = {Boyd, Stephen and Vandenberghe, Lieven},
title = {Convex Optimization},
year = {2004},
publisher = {Cambridge University Press},
address = {USA}
}

@InProceedings{abbasi-yadkori19b-politex, 
title = {{POLITEX}: Regret Bounds for Policy Iteration using Expert Prediction}, 
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gellert}, 
pages = {3692--3702}, 
year = {2019}, 
volume = {97},
series = {Proceedings of Machine Learning Research}, 
abstract = {We present POLITEX (POLicy ITeration with EXpert advice), a variant of policy iteration where each policy is a Boltzmann distribution over the sum of action-value function estimates of the previous policies, and analyze its regret in continuing RL problems. We assume that the value function error after running a policy for $\tau$ time steps scales as $\epsilon(\tau) = \epsilon_0 + O(\sqrt{d/\tau})$, where $\epsilon_0$ is the worst-case approximation error and $d$ is the number of features in a compressed representation of the state-action space. We establish that this condition is satisfied by the LSPE algorithm under certain assumptions on the MDP and policies. Under the error assumption, we show that the regret of POLITEX in uniformly mixing MDPs scales as $O(d^{1/2}T^{3/4} + \epsilon_0T)$, where $O(\cdot)$ hides logarithmic terms and problem-dependent constants. Thus, we provide the first regret bound for a fully practical model-free method which only scales in the number of features, and not in the size of the underlying MDP. Experiments on a queuing problem confirm that POLITEX is competitive with some of its alternatives, while preliminary results on Ms Pacman (one of the standard Atari benchmark problems) confirm the viability of POLITEX beyond linear function approximation.} }


@InProceedings{abbasi-yadkori19a-linearQuadraticPrediction, 
title = {Model-Free Linear Quadratic Control via Reduction to Expert Prediction}, 
author = {Abbasi-Yadkori, Yasin and Lazic, Nevena and Szepesvari, Csaba}, 
pages = {3108--3117},
year = {2019}, 
volume = {89}, 
series = {Proceedings of Machine Learning Research}, 
abstract = {Model-free approaches for reinforcement learning (RL) and continuous control find policies based only on past states and rewards, without fitting a model of the system dynamics. They are appealing as they are general purpose and easy to implement; however, they also come with fewer theoretical guarantees than model-based RL. In this work, we present a new model-free algorithm for controlling linear quadratic (LQ) systems, and show that its regret scales as $O(T^{\xi+2/3})$ for any small $\xi>0$ if time horizon satisfies $T>C^{1/\xi}$ for a constant $C$. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem. In practice, it corresponds to a variant of policy iteration with forced exploration, where the policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that provably achieves sublinear regret and has a polynomial computation cost. Empirically, our algorithm dramatically outperforms standard policy iteration, but performs worse than a model-based approach.} 
}

@article{sarsa1994,
author = {Rummery, G. and Niranjan, Mahesan},
year = {1994},
month = {11},
pages = {},
title = {On-Line Q-Learning Using Connectionist Systems},
journal = {Technical Report CUED/F-INFENG/TR 166}
}

@inproceedings{Brys-shapingDemo2015,
author = {Brys, Tim and Harutyunyan, Anna and Suay, Halit Bener and Chernova, Sonia and Taylor, Matthew E. and Now\'{e}, Ann},
title = {Reinforcement Learning from Demonstration through Shaping},
year = {2015},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3352–3358},
numpages = {7},
series = {IJCAI’15}
}
  


@InProceedings{Vieillard-2020DCPI,
author = {Vieillard, Nino and Pietquin, Olivier and Geist, Matthieu},
year = {2020},
pages = {6070-6077},
title = {Deep Conservative Policy Iteration},
booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}'20},
}

@InProceedings{abbasi-improvement16,
  title = 	 {A Fast and Reliable Policy Improvement Algorithm},
  author = 	 {Yasin Abbasi-Yadkori and Peter L. Bartlett and Stephen J. Wright},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1338--1346},
  year = 	 {2016},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
}

@phdthesis{kakade03thesis,
author = {Kakade, Machandranath Sham},
school = {University College London},
title = {{On the Sample Complexity of Reinforcement Learning}},
year = {2003}
}


@article{akrour-monotonic-2016,
  author  = {Riad Akrour and Abbas Abdolmaleki and Hany Abdulsamad and Jan Peters and Gerhard Neumann},
  title   = {Model-Free Trajectory-based Policy Optimization with Monotonic Improvement},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {14},
  pages   = {1-25}
}

@inproceedings{Rawlik2013-SOCRL,
author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
title = {On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)},
year = {2013},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {3052–3056},
location = {Beijing, China},
series = {IJCAI ’13}
}
  

@article{Ye2011,
author = {Ye, Yinyu},
title = {The Simplex and Policy-Iteration Methods Are Strongly Polynomial for the Markov Decision Problem with a Fixed Discount Rate},
year = {2011},
publisher = {INFORMS},
volume = {36},
journal = {Mathematics of Operations Research},
pages = {593–603},
numpages = {11},
keywords = {policy-iteration method, linear programming, dynamic programming, Markov decision problem, strongly polynomial time, simplex method}
}
  


@inproceedings{Gao-hierarchical-shaping-2015,
author = {Gao, Yang and Toni, Francesca},
title = {Potential Based Reward Shaping for Hierarchical Reinforcement Learning},
year = {2015},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3504–3510},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI’15}
}
  

@inproceedings{haarnoja-SAC2018,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
}



@book{cesa-bianchi2006,
author = {Cesa-Bianchi, Nicolo and Lugosi, Gabor},
title = {Prediction, Learning, and Games},
year = {2006},
publisher = {Cambridge University Press},
address = {USA}
}
  


@book{Borkar-stoApproximation2009,
author = {Vivek S. Borkar},
title = {Stochastic Approximation: A Dynamical Systems Viewpoint},
publisher = {Cambridge University Press},
year = {2009}
}

@misc{openai-five, 
author = {OpenAI},
title={OpenAI Five Blog}, 
howpublished = {\url{https://openai.com/blog/openai-five/}}, 
journal={OpenAI}, 
year={2018},
publisher={OpenAI}
}


@misc{zou-2019MetaShaping,
    title={Reward Shaping via Meta-Learning},
    author={Zou, Haosheng and Ren, Tongzheng and Dong, Yan and Hang, Su and Jun, Zhu},
    year={2019},
    eprint={1901.09330},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Zhu2020DAAP,
author = {Zhu, L and Cui, Yunduan and Matsubara, Takamitsu},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {10681--10687},
title = {{Dynamic Actor-Advisor Programming for Scalable Safe Reinforcement Learning}},
year = {2020}
}

@inproceedings{Ng-shaping1999,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML ’99}
}
  


@inproceedings{Randlov1998,
author = {Randl\o{}v, Jette and Alstr\o{}m, Preben},
title = {Learning to Drive a Bicycle Using Reinforcement Learning and Shaping},
year = {1998},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
pages = {463–471},
numpages = {9},
series = {ICML ’98}
}
  

@book{Sutton-RL2018,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA}
}

@inproceedings{Asmuth-PBRS-model-2008,
author = {Asmuth, John and Littman, Michael L. and Zinkov, Robert},
title = {Potential-Based Shaping in Model-Based Reinforcement Learning},
year = {2008},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2},
pages = {604–609},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI’08}
}
  
@inproceedings{Marthi-autoShaping-2007,
author = {Marthi, Bhaskara},
title = {Automatic Shaping and Decomposition of Reward Functions},
year = {2007},
address = {New York, NY, USA},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {601–608},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML ’07}
}
  
@inproceedings{Marom-beliefShaping-2018,
	author = {Ofir Marom and Benjamin Rosman},
	title = {Belief Reward Shaping in Reinforcement Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {reward shaping; Bayesian statistics},
	abstract = {A key challenge in many reinforcement learning problems is delayed rewards, which can significantly slow down learning. Although reward shaping has previously been introduced to accelerate learning by bootstrapping an agent with additional information, this can lead to problems with convergence. We present a novel Bayesian reward shaping framework that augments the reward distribution with prior beliefs that decay with experience. Formally, we prove that under suitable conditions a Markov decision process augmented with our framework is consistent with the optimal policy of the original MDP when using the Q-learning algorithm. However, in general our method integrates seamlessly with any reinforcement learning algorithm that learns a value or action-value function through experience. Experiments are run on a gridworld and a more complex backgammon domain that show that we can learn tasks significantly faster when we specify intuitive priors on the reward distribution.},
}
@inproceedings{wiewiora-principled2003,
author = {Wiewiora, Eric and Cottrell, Garrison and Elkan, Charles},
title = {Principled Methods for Advising Reinforcement Learning Agents},
year = {2003},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {792–799},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML’03}
}

@inproceedings{devlin-PBRSMA-2011,
author = {Devlin, Sam and Kudenko, Daniel},
title = {Theoretical Considerations of Potential-Based Reward Shaping for Multi-Agent Systems},
year = {2011},
address = {Richland, SC},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {225–232},
numpages = {8},
keywords = {multiagent learning, reinforcement learning, reward shaping, reward structures for learning},
location = {Taipei, Taiwan},
series = {AAMAS ’11}
}

@inproceedings{devlin-dynamicShaping-2012,
author = {Devlin, Sam and Kudenko, Daniel},
title = {Dynamic Potential-Based Reward Shaping},
year = {2012},
address = {Richland, SC},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {433–440},
numpages = {8},
keywords = {reinforcement learning, reward shaping},
location = {Valencia, Spain},
series = {AAMAS ’12}
}

@article{GRZES-onlineShaping-NNs2010,
title = "Online learning of shaping rewards in reinforcement learning",
journal = "Neural Networks",
volume = "23",
number = "4",
pages = "541 - 550",
year = "2010",
author = "Marek Grześ and Daniel Kudenko",
keywords = "Potential-based reward shaping, Reinforcement learning, Learning heuristic",
abstract = "Potential-based reward shaping has been shown to be a powerful method to improve the convergence rate of reinforcement learning agents. It is a flexible technique to incorporate background knowledge into temporal-difference learning in a principled way. However, the question remains of how to compute the potential function which is used to shape the reward that is given to the learning agent. In this paper, we show how, in the absence of knowledge to define the potential function manually, this function can be learned online in parallel with the actual reinforcement learning process. Two cases are considered. The first solution which is based on the multi-grid discretisation is designed for model-free reinforcement learning. In the second case, the approach for the prototypical model-based R-max algorithm is proposed. It learns the potential function using the free space assumption about the transitions in the environment. Two novel algorithms are presented and evaluated."
}

@inproceedings{Harutyunyan-arbitrary-2015,
author = {Harutyunyan, Anna and Devlin, Sam and Vrancx, Peter and Nowe, Ann},
title = {Expressing Arbitrary Reward Functions as Potential-Based Advice},
year = {2015},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2652–2658},
numpages = {7},
location = {Austin, Texas},
series = {AAAI’15}
}
  


@inproceedings{Grzes-episodic-shaping-2017,
author = {Grzes, Marek},
title = {Reward Shaping in Episodic Reinforcement Learning},
year = {2017},
address = {Richland, SC},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {565–573},
numpages = {9},
keywords = {reward shaping, reinforcement learning, reward structures for learning, multiagent learning, potential-based reward shaping},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS ’17}
}
  

@InProceedings{wagner2011,
title = {A reinterpretation of the policy oscillation phenomenon in approximate policy iteration},
author = {Wagner, Paul},
booktitle = {Advances in Neural Information Processing Systems 24},
pages = {2573--2581},
year = {2011},
}


@book{concentrationInequalities2013,
 author = {Boucheron, Stephane, Lugosi, Gabor and Massart, Pascal},
 title = {Concentration Inequalities: A Nonasymptotic Theory of Independence},
 year = {2013},
 edition = {1st},
 publisher = {Oxford University Press}
}

@InProceedings{trpo-schulman15,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  volume = 	 {37},
}

@inproceedings{cheng2019,
  author    = {Richard Cheng and
               G{\'{a}}bor Orosz and
               Richard M. Murray and
               Joel W. Burdick},
  title     = {End-to-End Safe Reinforcement Learning through Barrier Functions for
               Safety-Critical Continuous Control Tasks},
  booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}'19},
  pages     = {3387--3395},
  year      = {2019},
}


@inproceedings{Achiam2017,
author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
title = {Constrained Policy Optimization},
year = {2017},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML) - Volume 70},
pages = {22–31},
numpages = {10},
series = {ICML’17}
}



@article{ZHU2020CEP,
title = "Scalable reinforcement learning for plant-wide control of vinyl acetate monomer process",
journal = "Control Engineering Practice",
volume = "97",
pages = "104331-104340",
year = "2020",
author = "Lingwei Zhu and Yunduan Cui and Go Takami and Hiroaki Kanokogi and Takamitsu Matsubara",
keywords = "Chemical process control, Reinforcement learning, Vinyl acetate monomer",
abstract = "This paper explores a reinforcement learning (RL) approach that designs automatic control strategies in a large-scale chemical process control scenario as the first step for leveraging an RL method to intelligently control real-world chemical plants. The huge number of units for chemical reactions as well as feeding and recycling the materials of a typical chemical process induces a vast amount of samples and subsequent prohibitive computation complexity in RL for deriving a suitable control policy due to high-dimensional state and action spaces. To tackle this problem, a novel RL algorithm: Factorial Fast-food Dynamic Policy Programming (FFDPP) is proposed. By introducing a factorial framework that efficiently factorizes the action space, Fast-food kernel approximation that alleviates the curse of dimensionality caused by the high dimensionality of state space, into Dynamic Policy Programming (DPP) that achieves stable learning even with insufficient samples. FFDPP is evaluated in a commercial chemical plant simulator for a Vinyl Acetate Monomer (VAM) process. Experimental results demonstrate that without any knowledge of the model, the proposed method successfully learned a stable policy with reasonable computation resources to produce a larger amount of VAM product with comparative performance to a state-of-the-art model-based control."
}


@InProceedings{GriffithPolicyShaping2013,
title = {Policy Shaping: Integrating Human Feedback with Reinforcement Learning},
author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 26},
pages = {2625--2633},
year = {2013},
}

@article{plisnier2019,
  author    = {H{\'{e}}l{\`{e}}ne Plisnier and
               Denis Steckelmacher and
               Diederik M. Roijers and
               Ann Now{\'{e}}},
  title     = {The Actor-Advisor: Policy Gradient With Off-Policy Advice},
  journal   = {CoRR},
  volume    = {abs/1902.02556},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02556},
  archivePrefix = {arXiv},
  eprint    = {1902.02556},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
}

@InProceedings{leike2017,
  author    = {Jan Leike and
               Miljan Martic and
               Victoria Krakovna and
               Pedro A. Ortega and
               Tom Everitt and
               Andrew Lefrancq and
               Laurent Orseau and
               Shane Legg},
  title     = {{AI} Safety Gridworlds},
  year      = {2017},
  booktitle      = {http://arxiv.org/abs/1711.09883},
  archivePrefix = {arXiv},
  eprint    = {1711.09883},
  timestamp = {Mon, 13 Aug 2018},
}

@InProceedings{asadi17a,
  title = 	 {An Alternative Softmax Operator for Reinforcement Learning},
  author = 	 {Kavosh Asadi and Michael L. Littman},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  pages = 	 {243--252},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  abstract = 	 {A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.}
}

@inproceedings{Bellemare2016-increaseGap,
author = {Bellemare, Marc G. and Ostrovski, Georg and Guez, Arthur and Thomas, Philip S. and Munos, R\'{e}mi},
title = {Increasing the Action Gap: New Operators for Reinforcement Learning},
year = {2016},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {1476–1483},
numpages = {8},
}


@inproceedings{Munos2005,
 author = {Munos, R{\'e}mi},
 title = {Error Bounds for Approximate Value Iteration},
 booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence - Volume 2},
 series = {AAAI'05},
 year = {2005},
 location = {Pittsburgh, Pennsylvania},
 pages = {1006--1011},
} 

@article{Bertsekas2011,
author = {Bertsekas, Dimitri},
year = {2011},
month = {08},
pages = {310-335},
title = {Approximate policy iteration: A survey and some new methods},
volume = {9},
journal = {Journal of Control Theory and Applications}
}

@InProceedings{pirotta13,
  title = 	 {Safe Policy Iteration},
  author = 	 {Matteo Pirotta and Marcello Restelli and Alessio Pecorino and Daniele Calandriello},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {307--315},
  year = 	 {2013},
  volume = 	 {28},
  abstract = 	 {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.  When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.  To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.  We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.}
}

@InProceedings{haarnoja-SAC2017a,
  title = 	 {Reinforcement Learning with Deep Energy-Based Policies},
  author = 	 {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {34th International Conference on Machine Learning},
  pages = 	 {1352--1361},
  year = 	 {2017},
}

@inproceedings{Fox2016,
 author = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
 title = {Taming the Noise in Reinforcement Learning via Soft Updates},
 booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
 year = {2016},
 pages = {202--211},
 numpages = {10},
}





@InProceedings{kozunoCVI,
   title = 	 {Theoretical Analysis of Efficiency and Robustness of Softmax and Gap-Increasing Operators in Reinforcement Learning},
  author =       {Kozuno, Tadashi and Uchibe, Eiji and Doya, Kenji},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2995--3003},
  year = 	 {2019},
  volume = 	 {89},
}

@article{Xu2007,
author={X. {Xu} and D. {Hu} and X. {Lu}},
journal={IEEE Transactions on Neural Networks},
title={Kernel-Based Least Squares Policy Iteration for Reinforcement Learning},
year={2007},
volume={18},
number={4},
pages={973-992},
keywords={adaptive control;feedback;iterative methods;learning (artificial intelligence);least squares approximations;Markov processes;nonlinear control systems;optimal control;state-space methods;uncertain systems;kernel-based least squares policy iteration;reinforcement learning;continuous state spaces;adaptive feedback control;uncertain dynamic systems;near-optimal control policy;control plants;Mercer kernels;policy evaluation;kernel-based least squares temporal-difference algorithm;generalization ability;kernel sparsification procedure;approximate linear dependency;large-scale Markov decision problems;stochastic chain problem;least squares policy iteration algorithm;nonlinear feedback control;ship heading control problem;swing up control;double-link underactuated pendulum;acrobot;online learning control;Least squares methods;Learning;Kernel;Feedback control;Least squares approximation;State-space methods;Programmable control;Adaptive control;Linear approximation;Convergence;Approximate dynamic programming;kernel methods;least squares;Markov decision problems (MDPs);reinforcement learning (RL);Algorithms;Artificial Intelligence;Biomimetics;Computer Simulation;Decision Support Techniques;Feedback;Least-Squares Analysis;Markov Chains;Models, Theoretical;Reinforcement (Psychology)},
ISSN={1045-9227},
}






@InProceedings{busic18a,
  title = 	 {Action-Constrained {Markov Decision Processes} With {Kullback-Leibler} Cost},
  author = 	 {Bu\v{s}i\'{c}, Ana and Meyn, Sean},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {1431--1444},
  year = 	 {2018},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  abstract = 	 {This paper concerns computation of optimal policies in which the one-step reward function contains a cost term that models Kullback-Leibler divergence with respect to nominal dynamics. This technique was introduced by Todorov in 2007, where it was shown under general conditions that the solution to the average-reward optimality equations reduce to a simple eigenvector problem. Since then many authors have sought to apply this technique to control problems and models of bounded rationality in economics.  A crucial assumption is that the input process is essentially unconstrained. For example, if the nominal dynamics include randomness from nature (e.g., the impact of wind on a moving vehicle), then the optimal control solution does not respect the exogenous nature of this disturbance.  This paper introduces a technique to solve a more general class of action-constrained MDPs. The main idea is to solve an entire parameterized family of MDPs, in which the parameter is a scalar weighting the one-step  reward function. The approach is new and practical even in the original unconstrained formulation.}
}



@book{Khalil2002,
      author        = "Khalil, Hassan K",
      title         = "{Nonlinear systems; 3rd ed.}",
      publisher     = "Prentice-Hall",
      address       = "Upper Saddle River, NJ",
      year          = "2002",

}


@incollection{Hasselt2010,
title = {Double Q-learning},
author = {Hado V. Hasselt},
booktitle = {Advances in Neural Information Processing Systems 23},
pages = {2613--2621},
year = {2010},
}

@incollection{Moldovan2012risk,
author = {Moldovan, T.M. and Abbeel, P},
year = {2012},
month = {01},
pages = {3131-3139},
title = {Risk aversion in Markov decision processes via near-optimal Chernoff bounds},
volume = {4},
booktitle = {Advances in Neural Information Processing Systems (NIPS)}
}
@incollection{Osogami2012,
title = {Robustness and risk-sensitivity in Markov decision processes},
author = {Osogami, Takayuki},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 25},
pages = {233--241},
year = {2012},}

@article{Howard1972,
 abstract = {This paper considers the maximization of certain equivalent reward generated by a Markov decision process with constant risk sensitivity. First, value iteration is used to optimize possibly time-varying processes of finite duration. Then a policy iteration procedure is developed to find the stationary policy with highest certain equivalent gain for the infinite duration case. A simple example demonstrates both procedures.},
 author = {Ronald A. Howard and James E. Matheson},
 journal = {Management Science},
 number = {7},
 pages = {356--369},
 publisher = {INFORMS},
 title = {Risk-Sensitive Markov Decision Processes},
 volume = {18},
 year = {1972}
}

@book{Puterman1994,
 author = {Puterman, Martin L.},
 title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
 year = {1994},
 edition = {1st},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}
@incollection{Lim2013,
title = {Reinforcement Learning in Robust Markov Decision Processes},
author = {Lim, Shiau Hong and Xu, Huan and Mannor, Shie},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 26},
pages = {701--709},
year = {2013},
}
@incollection{Chow2015,
title = {Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach},
author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
booktitle = {Advances in Neural Information Processing Systems (NIPS) 28},
pages = {1522--1530},
year = {2015},
publisher = {Curran Associates, Inc.},
}

@article{lipton2016combating,
  author    = {Zachary C. Lipton and
               Jianfeng Gao and
               Lihong Li and
               Jianshu Chen and
               Li Deng},
  title     = {Combating Reinforcement Learning's Sisyphean Curse with Intrinsic
               Fear},
  journal   = {CoRR},
  volume    = {abs/1611.01211},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1611.01211},
}

@article{Garcia2015,
 author = {Garc\'{\i}a, Javier and Fern\'{a}ndez, Fernando},
 title = {A Comprehensive Survey on Safe Reinforcement Learning},
 journal = {Journal of Machine Learning Research (JMLR)},
 issue_date = {January 2015},
 volume = {16},
 number = {1},
 month = jan,
 year = {2015},
 issn = {1532-4435},
 pages = {1437--1480},
 numpages = {44},
 keywords = {reinforcement learning, risk sensitivity, safe exploration, teacher advice},
}


@book{Altman1999ConstrainedMD,
  title={Constrained Markov decision processes},
  author={E. Altman},
  year={1999}
}

@article{song09,
  author    = {Zhao Song and
               Ronald E. Parr and
               Lawrence Carin},
  title     = {Revisiting the Softmax Bellman Operator: Theoretical Properties and
               Practical Benefits},
  journal   = {CoRR},
  volume    = {abs/1812.00456},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1812.00456},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
}

@inproceedings{Kolter09,
 author = {Kolter, J. Zico and Ng, Andrew Y.},
 title = {Regularization and Feature Selection in Least-squares Temporal Difference Learning},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning (ICML)},
 year = {2009},
 isbn = {978-1-60558-516-1},
 location = {Montreal, Quebec, Canada},
 pages = {521--528},
 numpages = {8},

 address = {New York, NY, USA},
}

@InProceedings{banijamali19a,
  title = 	 {Optimizing over a Restricted Policy Class in MDPs},
  author = 	 {Banijamali, Ershad and Abbasi-Yadkori, Yasin and Ghavamzadeh, Mohammad and Vlassis, Nikos},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {3042--3050},
  year = 	 {2019},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  publisher = 	 {PMLR},
  abstract = 	 {We address the problem of finding an optimal policy in a Markov decision process (MDP) under a restricted policy class defined by the convex hull of a set of base policies. This problem is of great interest in applications in which a number of reasonably good (or safe) policies are already known and we are interested in optimizing in their convex hull. We first prove that solving this problem is NP-hard. We then propose an efficient algorithm that finds a policy whose performance is almost as good as that of the best convex combination of the base policies, under the assumption that the occupancy measures of the base policies have a large overlap. The running time of the proposed algorithm is linear in the number of states and polynomial in the number of base policies. A distinct advantage of the proposed algorithm is that, apart from the computation of the occupancy measures of the base policies, it does not need to interact with the environment during the optimization process. This is especially important (i) in problems that due to concerns such as safety, we are restricted in interacting with the environment only through the (safe) base policies, and (ii) in complex systems where estimating the value of a policy can be a time consuming process.}
}

@article{HUNT1992,
title = "Neural networks for control systems—A survey",
journal = "Automatica",
volume = "28",
number = "6",
pages = "1083 - 1112",
year = "1992",
author = "K.J. Hunt and D. Sbarbaro and R. Żbikowski and P.J. Gawthrop",
keywords = "Neural networks, nonlinear control systems, nonlinear systems modelling, systems identification",
abstract = "This paper focuses on the promise of artificial neural networks in the realm of modelling, identification and control of nonlinear systems. The basic ideas and techniques of artificial neural networks are presented in language and notation familiar to control engineers. Applications of a variety of neural network architectures in control are surveyed. We explore the links between the fields of control science and neural networks in a unified presentation and identify key areas for future research."
}

@book{Bellman2003,
 author = {Bellman, Richard Ernest},
 title = {Dynamic Programming},
 year = {2003},
 isbn = {0486428095},
 publisher = {Dover Publications, Inc.},
 address = {New York, NY, USA},
}

@book{Bertsekas2000,
 author = {Bertsekas, Dimitri P.},
 title = {Dynamic Programming and Optimal Control},
 year = {2000},
 isbn = {1886529094},
 edition = {2nd},
 publisher = {Athena Scientific},
}



@inproceedings{Sutton1999,
 author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 booktitle = {Advances in Neural Information Processing Systems (NIPS)},
 year = {1999},
 pages = {1057--1063},
}


@article{Fujiwara2009,
author = {Fujiwara, Koichi and Kano, Manabu and Hasebe, Shinji and Takinami, Akitoshi},
title = {Soft-sensor development using correlation-based just-in-time modeling},
journal = {AIChE Journal},
volume = {55},
number = {7},
pages = {1754-1765},
abstract = {Abstract Soft-sensors have been widely used for estimating product quality or other key variables, but their estimation performance deteriorate when the process characteristics change. To cope with such changes, recursive PLS and Just-In-Time (JIT) modeling have been developed. However, recursive PLS does not always function well when process characteristics change abruptly and JIT modeling does not always achieve the high-estimation performance. In the present work, a new method for constructing soft-sensors based on a JIT modeling technique is proposed. In the proposed method, referred to as correlation-based JIT modeling (CoJIT), the samples used for local modeling are selected on the basis of the correlation among measured variables and the model can adapt to changes in process characteristics. The usefulness of the proposed method is demonstrated through a case study of a CSTR process, in which catalyst deactivation and recovery are taken into account. In addition, its industrial application to a cracked gasoline fractionator is reported. © 2009 American Institute of Chemical Engineers AIChE J, 2009},
year = {2009}
}



@article{Fujiwara2015,
author = {Fujiwara, Koichi and Miyajima, Miho and Yamakawa, Toshitaka and Abe, Erika and Suzuki, Yoko and Sawada, Yuriko and Kano, Manabu and Maehara, Taketoshi and Ohta, Katsuya and Sasai, Taeko and Sasano, Tetsuo and Matsuura, Masato and Matsushima, Eisuke},
year = {2015},
month = {12},
pages = {1321-1332},
title = {Epileptic Seizure Prediction Based on Multivariate Statistical Process Control of Heart Rate Variability Features},
volume = {63},
journal = {IEEE Transactions on Biomedical Engineering},
}

@article{ZHANG2017,
title = "Locally weighted kernel partial least squares regression based on sparse nonlinear features for virtual sensing of nonlinear time-varying processes",
journal = "Computers and Chemical Engineering",
volume = "104",
pages = "164 - 171",
year = "2017",
author = "Xinmin Zhang and Manabu Kano and Yuan Li",
keywords = "Soft-sensor, Virtual sensing, Nonlinear time-varying processes, SKFCFs, Instance-wise kernelized elastic net learning, Locally weighted kernel partial least squares",
abstract = "Virtual sensing technology is crucial for monitoring product quality when real-time measurement is not available. To deal with both strong nonlinearity and time-varying dynamics of industrial processes, we propose a novel locally weighted kernel PLS (LW-KPLS) based on sparse nonlinear features in this research. Unlike the conventional locally weighted PLS (LW-PLS), the proposed method weights the training samples by using sparse kernel feature characterization factors (SKFCFs), which take account of the strength of nonlinear dependency between samples in the Hilbert feature space. By integrating the nonlinear features into the locally weighted regression framework, LW-KPLS not only can cope with the time-varying characteristics but also is more suitable for highly nonlinear processes. The proposed method was validated through a numerical example, a penicillin fermentation process, and a real industrial cleaning process for residual drug substances. The results have demonstrated that the proposed LW-KPLS outperforms the conventional PLS, KPLS, LW-PLS, and eLW-KPLS in the prediction performance."
}

@article{Kano2008,
title = "Data-based process monitoring, process control, and quality improvement: Recent developments and applications in steel industry",
journal = "Computers and Chemical Engineering",
volume = "32",
number = "1",
pages = "12 - 24",
year = "2008",
author = "Manabu Kano and Yoshiaki Nakagawa",
keywords = "Soft-sensor, Multivariate statistical process control, Multivariate analysis, Iron and steel process, Quality improvement, Quantification",
abstract = "The issue of how to improve product quality and product yield in a brief period of time becomes more critical in many industries. Even though industrial processes are totally different in appearance, the problems to solve are highly similar: how to build a reliable model from a limited data, how to analyze the model and relate it to first principles, how to optimize operating condition, and how to realize an on-line monitoring and control system and maintain it. In this paper, statistical process monitoring and control methodologies are briefly surveyed, and our application results in steel facilities are presented. The achievements of the present work are as follows: (1) the development of a new method that can cope with qualitative quality information and relate operating conditions to product quality or product yield, (2) the simultaneous analysis of multiple processing units including a converter, a continuous caster, a blooming process, and rolling processes, and (3) the successful application results in the steel industry."
}


@article{FLEMING2002,
title = "Evolutionary algorithms in control systems engineering: a survey",
journal = "Control Engineering Practice",
volume = "10",
number = "11",
pages = "1223 - 1241",
year = "2002",
author = "P.J Fleming and R.C Purshouse",
keywords = "Control system design, Genetic algorithms, Identification, Multiobjective optimisation, On-line control",
abstract = "Challenging optimisation problems, which elude acceptable solution via conventional methods, arise regularly in control systems engineering. Evolutionary algorithms (EAs) permit flexible representation of decision variables and performance evaluation and are robust to difficult search environments, leading to their widespread uptake in the control community. Significant applications are discussed in parameter and structure optimisation for controller design and model identification, in addition to fault diagnosis, reliable systems, robustness analysis, and robot control. Hybrid neural and fuzzy control schemes are also described. The important role of EAs in multiobjective optimisation is highlighted. Evolutionary advances in adaptive control and multidisciplinary design are predicted."
}

@article{PRECUP2011,
title = "A survey on industrial applications of fuzzy control",
journal = "Computers in Industry",
volume = "62",
number = "3",
pages = "213 - 226",
year = "2011",
author = "Radu-Emil Precup and Hans Hellendoorn",
keywords = "Adaptive fuzzy control, Mamdani fuzzy controllers, Predictive control, Stable design, Takagi-Sugeno fuzzy controllers",
abstract = "Fuzzy control has long been applied to industry with several important theoretical results and successful results. Originally introduced as model-free control design approach, model-based fuzzy control has gained widespread significance in the past decade. This paper presents a survey on recent developments of analysis and design of fuzzy control systems focused on industrial applications reported after 2000."
}

@article{DARBY2012,
title = "MPC: Current practice and challenges",
journal = "Control Engineering Practice",
volume = "20",
number = "4",
pages = "328 - 342",
year = "2012",
note = "Special Section: IFAC Symposium on Advanced Control of Chemical Processes - ADCHEM 2009",
author = "Mark L. Darby and Michael Nikolaou",
keywords = "Model predictive control, Model-based control, Constraints, Control system design, Modeling, Process identification.",
abstract = "Linear Model Predictive Control (MPC) continues to be the technology of choice for constrained, multivariable control applications in the process industry. Successful deployment of MPC requires “getting right” multiple aspects of the control problem. This includes the design of the underlying regulatory controls, design of the MPC(s), test design for model identification, model development, and dealing with nonlinearities. Approaches and techniques that are successfully applied in practice are described, including the challenges involved in ensuring a successful MPC application. Academic contributions are highlighted and suggestions provided for improving MPC."
}

@book{Camacho-MPC-1997,
 author = {Camacho, Eduardo F. and Bordons, Carlos A.},
 title = {Model Predictive Control in the Process Industry},
 year = {1997},
 isbn = {3540199241},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}


@article{ASTROM2014,
title = "Control: A perspective",
journal = "Automatica",
volume = "50",
number = "1",
pages = "3 - 43",
year = "2014",
author = "Karl J. Aström and P.R. Kumar",
keywords = "Feedback, Control, Computing, Communication, Theory, Applications",
abstract = "Feedback is an ancient idea, but feedback control is a young field. Nature long ago discovered feedback since it is essential for homeostasis and life. It was the key for harnessing power in the industrial revolution and is today found everywhere around us. Its development as a field involved contributions from engineers, mathematicians, economists and physicists. It is the first systems discipline; it represented a paradigm shift because it cut across the traditional engineering disciplines of aeronautical, chemical, civil, electrical and mechanical engineering, as well as economics and operations research. The scope of control makes it the quintessential multidisciplinary field. Its complex story of evolution is fascinating, and a perspective on its growth is presented in this paper. The interplay of industry, applications, technology, theory and research is discussed."
}

@article{SENGNG2010,
title = "Multi-agent based collaborative fault detection and identification in chemical processes",
journal = "Engineering Applications of Artificial Intelligence",
volume = "23",
number = "6",
pages = "934 - 949",
year = "2010",
author = "Yew Seng Ng and Rajagopalan Srinivasan",
keywords = "Multi-agent system, Decision fusion, Bayesian combination, Voting, Fault diagnosis",
abstract = "Fault detection and identification (FDI) has received significant attention in literature. Popular methods for FDI include principal component analysis, neural-networks, and signal processing methods. However, each of these methods inherit certain strengths and shortcomings. A method that works well under one circumstance might not work well under another when different features of the underlying process come to the fore. In this paper, we show that a collaborative FDI approach that combines the strengths of various heterogeneous FDI methods is able to maximize diagnostic performance. A multi-agent framework is proposed to realize such collaboration in practice where different FDI methods, i.e: principal component analysis, self-organizing maps, non-parametric approaches, or neural-networks are combined. Since the results produced by different FDI agents might be in conflict, we use decision fusion methods to combine FDI results. Two different methods – voting-based fusion and Bayesian probability fusion are studied here. Most monitoring and fault diagnosis algorithms are computationally complex, but their results are often needed in real-time. One advantage of the multi-agent framework is that it provides an efficient means for speeding up the execution time of the various FDI methods through seamless deployment in a large-scale grid. The proposed multi-agent approach is illustrated through fault diagnosis of the startup of a lab-scale distillation unit and the Tennessee Eastman Challenge problem. Extensive testing of the proposed method shows that combining diagnostic classifiers of different types can significantly improve diagnostic performance."
}

@INPROCEEDINGS{WANG2008,
author={H. {Wang} and Y. {Zhang}},
booktitle={2008 4th International Conference on Wireless Communications, Networking and Mobile Computing},
title={Multi-Agent Based Chemical Plant Process Monitoring and Management System},
year={2008},
volume={},
pages={1-4},
keywords={chemical industry;field buses;multi-agent systems;process control;process monitoring;production engineering computing;multiagent;chemical plant process monitoring;chemical plant management system;heterogeneous chemical plant machines;software systems;fieldbus;chemical plant process control systems;enterprise management;quick change manufacturing;Chemical processes;Monitoring;Chemical technology;Control systems;Chemical products;Production systems;Process control;Software systems;Field buses;Enterprise resource planning},
}

@article{YANG2008,
title = "A multi-agent system to facilitate component-based process modeling and design",
journal = "Computers and Chemical Engineering",
volume = "32",
number = "10",
pages = "2290 - 2305",
year = "2008",
author = "A. Yang and B. Braunschweig and E.S. Fraga and Z. Guessoum and W. Marquardt and O. Nadjemi and D. Paen and D. Piñol and P. Roux and S. Sama and M. Serra and I. Stalker",
keywords = "Multi-agent system, Process modeling, Matchmaking, Ontology, Component-based software",
abstract = "Component-based software technology and software interfaces standardization initiatives, such as CAPE–OPEN, have made it possible to model chemical processes and to perform model-based engineering tasks by combining components of process modeling software from different sources, hence providing the potential of exploiting the “best of breed” offered by the CAPE community. In this context, software component libraries, possibly located on a local computer, on the intranet of an organization, or on the Internet, have to be searched to find the most suitable components for a particular engineering task at hand to be integrated into the engineers’ computing environment. This paper proposes to address this issue through a multi-agent software system which facilitates the engineers to find and to integrate software components and aims at reducing the engineers’ effort to the minimum. Within this system, a directory facilitator serves as the “yellow pages” such that an undetermined set of software component libraries located anywhere may be registered with the system. A matchmaker is used to match the specification of a desired software component with the potential candidates in the relevant libraries. The integration of a matching component into the computing environment is handled by an integration manager. A prototype of such a system, called COGents, has been developed employing an existing multi-agent platform. The ontology OntoCAPE defines the chemical engineering and modeling concepts required for specifying desired software components and for characterizing existing ones. OntoCAPE also provides a shared semantic basis for communication between the software agents. Details of the implementation of COGents are presented and the re-usability of the parts of the COGents system is discussed. Three successful demonstrative applications of COGents are reported, each dealing with different types of tasks, specifically flowsheeting, detailed modeling and process design."
}

@article{QIAN2000,
title = "An object/agent based environment for the Computer Integrated Process Operation System",
journal = "Computers and Chemical Engineering",
volume = "24",
number = "2",
pages = "457 - 462",
year = "2000",
author = "Yu Qian and Qiming Huang and Weilu Lin and Xiuxi Li",
keywords = "Computer Integrated Process Operation System, Agent, Object-orientation",
abstract = "This paper outlines an object/agent based system model for the Computer Integrated Process Operation System (CIPOS). Most previous studies have focused on the Computer Aided Process Operation system (CAPO), while they are standalone and not good at communication with each other. The system model proposed in this work can solve some key integrated problems in CIPOS, which are problem integration, task integration, tools integration and personnel involvement. An instantiated system, TE_CIPOS, is implemented to give further explanation and verification of the proposed object/agent based system model and its integrating environment."
}

@article{SYAFIIE2007,
title = "Model-free learning control of neutralization processes using reinforcement learning",
journal = "Engineering Applications of Artificial Intelligence",
volume = "20",
number = "6",
pages = "767 - 782",
year = "2007",
author = "S. Syafiie and F. Tadeo and E. Martinez",
keywords = "Learning control, Goal-seeking control, Process control, Intelligent control, Online learning, Neutralization process, pH control",
abstract = "The pH process dynamic often exhibits severe nonlinear and time-varying behavior and therefore cannot be adequately controlled with a conventional PI control. This article discusses an alternative approach to pH process control using model-free learning control (MFLC), which is based on reinforcement learning algorithms. The MFLC control technique is proposed because this algorithm gives a general solution for acid–base systems, yet is simple enough to be implemented in existing control hardware without a model. Reinforcement learning is selected because it is a learning technique based on interaction with a dynamic system or process for which a goal-seeking control task must be performed. This “on-the-fly” learning is suitable for time varying or nonlinear processes for which the development of a model is too costly, time consuming or even not feasible. Results obtained in a laboratory plant show that MFLC gives good performance for pH process control. Also, control actions generated by MFLC are much smoother than conventional PID controller."
}

@ARTICLE{Harp2000,
author={S. A. {Harp} and S. {Brignone} and B. F. {Wollenberg} and T. {Samad}},
journal={IEEE Control Systems Magazine},
title={SEPIA. A simulator for electric power industry agents},
year={2000},
volume={20},
number={4},
pages={53-69},
keywords={power engineering computing;electricity supply industry;learning (artificial intelligence);multi-agent systems;discrete event simulation;decision support systems;SEPIA;electric power industry agents;technological convergence;processing power;memory capacities;component software infrastructure developments;adaptive agent architectures;nonexpert users;decision support;insight;problem understanding;electricity enterprise;deregulation;open competition;complex adaptive systems approach;Software tools;Power grids;Pricing;Power industry;Space technology;Power system simulation;Power generation;Power systems;Environmental management;Costs},
}

@inproceedings{Berkenkamp2017SafeRL,
  title = {Safe Model-based Reinforcement Learning with Stability Guarantees},
  booktitle = {Proc. of Neural Information Processing Systems (NIPS)},
  author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P. and Krause, Andreas},
  year = {2017},
  pages = {908--919},
  url = {https://arxiv.org/abs/1705.08551}
}


@article{garcia15a,
  author  = {Javier Garc{{\'i}}a and Fernando Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  pages   = {1437-1480},

}

@Article{Uchibe2018,
author="Uchibe, Eiji",
title="Model-Free Deep Inverse Reinforcement Learning by Logistic Regression",
journal="Neural Processing Letters",
year="2018",
volume="47",
number="3",
pages="891--905",
abstract="This paper proposes model-free deep inverse reinforcement learning to find nonlinear reward function structures. We formulate inverse reinforcement learning as a problem of density ratio estimation, and show that the log of the ratio between an optimal state transition and a baseline one is given by a part of reward and the difference of the value functions under the framework of linearly solvable Markov decision processes. The logarithm of density ratio is efficiently calculated by binomial logistic regression, of which the classifier is constructed by the reward and state value function. The classifier tries to discriminate between samples drawn from the optimal state transition probability and those from the baseline one. Then, the estimated state value function is used to initialize the part of the deep neural networks for forward reinforcement learning. The proposed deep forward and inverse reinforcement learning is applied into two benchmark games: Atari 2600 and Reversi. Simulation results show that our method reaches the best performance substantially faster than the standard combination of forward and inverse reinforcement learning as well as behavior cloning.",
}


@article{TSURUMINE2019,
title = "Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation",
journal = "Robotics and Autonomous Systems",
volume = "112",
pages = "72 - 83",
year = "2019",
author = "Yoshihisa Tsurumine and Yunduan Cui and Eiji Uchibe and Takamitsu Matsubara",
keywords = "Deep reinforcement learning, Robotic cloth manipulation, Dynamic policy programming",
abstract = "Deep Reinforcement Learning (DRL), which can learn complex policies with high-dimensional observations as inputs, e.g., images, has been successfully applied to various tasks. Therefore, it may be suitable to apply them for robots to learn and perform daily activities like washing and folding clothes, cooking, and cleaning since such tasks are difficult for non-DRL methods that often require either (1) direct access to state variables or (2) well-designed hand-engineered features extracted from sensory inputs. However, applying DRL to real robots remains very challenging because conventional DRL algorithms require a huge number of training samples for learning, which is arduous in real robots. To alleviate this dilemma, in this paper, we propose two sample efficient DRL algorithms: Deep P-Network (DPN) and Dueling Deep P-Network (DDPN). The core idea is to combine the nature of smooth policy update with the capability of automatic feature extraction in deep neural networks to enhance the sample efficiency and learning stability with fewer samples. The proposed methods were first investigated by a robot-arm reaching task in the simulation that compared previous DRL methods and applied to two real robotic cloth manipulation tasks: (1) flipping a handkerchief and (2) folding a t-shirt with a limited number of samples. All the results suggest that our method outperformed the previous DRL methods."
}
@article{JMLR:v15:vandermaaten14a,
  author  = {Laurens van der Maaten},
  title   = {Accelerating t-SNE using Tree-Based Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {3221-3245}
}
@article{AndrewBagnell2014,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
author = {{Andrew Bagnell}, J.},
isbn = {9783642276446},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement Learning in Robotics: A Survey}},
year = {2014}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{watkins1992q,
author = {Watkins, Christopher J C H and Dayan, Peter},
journal = {Machine learning},
number = {3-4},
pages = {279--292},
publisher = {Springer},
title = {{Q-learning}},
volume = {8},
year = {1992}
}
@article{Garcia2012a,
abstract = {In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.},
author = {Garcia, Javier and Fernandez, Fernando},
file = {:C$\backslash$:/Users/Lingwei Zhu/Downloads/1402.0560.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {515--564},
title = {{Safe exploration of state and action spaces in reinforcement learning}},
volume = {45},
year = {2012}
}


@article{klenske2016gaussian,
author = {Klenske, Edgar D and Zeilinger, Melanie N and Sch{\"{o}}lkopf, Bernhard and Hennig, Philipp},
journal = {IEEE Transactions on Control Systems Technology},
number = {1},
pages = {110--121},
publisher = {IEEE},
title = {{Gaussian process-based predictive control for periodic error correction}},
volume = {24},
year = {2016}
}
@article{zheng1999hierarchical,
author = {Zheng, Alex and Mahajanam, Rama V and Douglas, James M},
journal = {AIChE Journal},
number = {6},
pages = {1255--1265},
publisher = {Wiley Online Library},
title = {{Hierarchical procedure for plantwide control system synthesis}},
volume = {45},
year = {1999}
}
@article{Geibel2005,
abstract = {In this paper, we consider Markov Decision Processes (MDPs) with error states. Error$\backslash$nstates are those states entering which is undesirable or dangerous. We define the risk$\backslash$nwith respect to a policy as the probability of entering such a state when the policy is$\backslash$npursued. We consider the problem of finding good policies whose risk is smaller than$\backslash$nsome user-specified threshold, and formalize it as a constrained MDP with two criteria.$\backslash$nThe first criterion corresponds to the value function originally given. We will show that$\backslash$nthe risk can be formulated as a second criterion function based on a cumulative return,$\backslash$nwhose definition is independent of the original value function. We present a model free,$\backslash$nheuristic reinforcement learning algorithm that aims at finding good deterministic policies.$\backslash$nIt is based on weighting the original value function and the risk. The weight parameter is$\backslash$nadapted in order to find a feasible solution for the constrained problem that has a good$\backslash$nperformance with respect to the value function. The algorithm was successfully applied$\backslash$nto the control of a feed tank with stochastic inflows that lies upstream of a distillation$\backslash$ncolumn. This control task was originally formulated as an optimal control problem with$\backslash$nchance constraints, and it was solved under certain assumptions on the model to obtain an$\backslash$noptimal solution. The power of our learning algorithm is that it can be used even when$\backslash$nsome of these restrictive assumptions are relaxed.},
author = {Geibel, Peter and Wysotzki, Fritz},
journal = {Journal of Artificial Intelligence Research},
title = {{Risk-sensitive reinforcement learning applied to control under constraints}},
volume = {24},
pages = {81-108},
year = {2005}
}
@inproceedings{Finn2016deep,
author = {Finn, C and Tan, Xin Yu and Duan, Yan and Darrell, T and Levine, S and Abbeel, P},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {512--519},
title = {{Deep spatial autoencoders for visuomotor learning}},
year = {2016}
}
@inproceedings{PatelICRA2014,
author = {Patel, Mitesh and Miro, Jaime Valls and Dissanayake, Gamini},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {969--974},
title = {{A probabilistic approach to learn activities of daily living of a mobility aid device user}},
year = {2014}
}
@article{o2016pgq,
author = {O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
journal = {arXiv preprint arXiv:1611.01626},
title = {{{\{}PGQ{\}}: Combining policy gradient and {\{}Q{\}}-learning}},
year = {2016}
}
@inproceedings{lillicrap2015continuous,
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {International Conference on Learning Representations (ICLR)},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@inproceedings{shen2006fast,
author = {Shen, Yirong and Ng, Andrew and Seeger, Matthias},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1225--1232},
title = {{Fast {G}aussian process regression using {KD}-trees}},
year = {2006}
}

@article{azar2012dynamic,
author = {Azar, Mohammad Gheshlaghi and G{\'{o}}mez, Vicen{\c{c}} and Kappen, Hilbert J},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {3207--3245},
title = {{Dynamic policy programming}},
volume = {13},
year = {2012}
}

@book{sutton1998reinforcement,
author = {Sutton, Richard S and Barto, Andrew G},
publisher = {MIT press Cambridge},
title = {{Reinforcement learning: An introduction}},
year = {1998}
}

@inproceedings{spielberg2017,
author = {Spielberg, S P K and Gopaluni, R B and Loewen, P D},
booktitle = {International Symposium on Advanced Control of Industrial Processes (AdCONIP)},
pages = {201--206},
title = {Deep reinforcement learning approaches for process control},
year = {2017},
}

@book{Bertsekas:1996:NP:560669,
author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
edition = {1st},
isbn = {1886529108},
publisher = {Athena Scientific},
title = {{Neuro-Dynamic Programming}},
year = {1996}
}
@inproceedings{sutton1996generalization,
author = {Sutton, Richard S},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1038--1044},
title = {{Generalization in reinforcement learning: Successful examples using sparse coarse coding}},
year = {1996}
}
@inproceedings{TahaICRA2011,
author = {Taha, Tarek and Miro, Jaime Valls and Dissanayake, Gamini},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {544--549},
title = {{A {POMDP} framework for modelling human interaction with assistive robots}},
year = {2011}
}
@phdthesis{cui2017practical,
author = {Cui, Yunduan},
school = {Nara Institute of Science and Technology},
title = {{Practical Model-free Reinforcement Learning in Complex Robot Systems with \\ High Dimensional States}},
year = {2017}
}

@phdthesis{thomas2015safe,
author = {Thomas, Philip},
school = {University of Massachusetts, Amherst},
title = {{Safe Reinforcement Learning}},
year = {2015}
}


@article{lee2010approximate,
author = {Lee, Jay H and Wong, Weechin},
journal = {Journal of Process Control},
number = {9},
pages = {1038--1048},
publisher = {Elsevier},
title = {{Approximate dynamic programming approach for process control}},
volume = {20},
year = {2010}
}
@article{kamthe2017data,
author = {Kamthe, Sanket and Deisenroth, Marc Peter},
journal = {arXiv preprint arXiv:1706.06491},
title = {{Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control}},
year = {2017}
}
@article{FoxRAM1997,
author = {Fox, D and Burgard, W and Thrun, S},
journal = {IEEE Robotics Automation Magazine},
number = {1},
pages = {23--33},
title = {{The dynamic window approach to collision avoidance}},
volume = {4},
year = {1997}
}
@article{cheng2008dantzig,
author = {Cheng, Ruoyu and Forbes, J Fraser and {San Yip}, W},
journal = {Computers and Chemical Engineering},
number = {7},
pages = {1507--1522},
publisher = {Elsevier},
title = {{Dantzig--Wolfe decomposition and plant-wide MPC coordination}},
volume = {32},
year = {2008}
}
@inproceedings{thiery2010least,
author = {Thiery, Christophe and Scherrer, Bruno},
booktitle = {International Conference on Machine Learning},
title = {{Least-squares {\$}\lambda{\$} policy iteration: Bias-variance trade-off in control problems}},
year = {2010}
}
@phdthesis{mnih2013machine,
author = {Mnih, Volodymyr},
school = {University of Toronto},
title = {{Machine learning for aerial image labeling}},
year = {2013}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{\{}https://github.com/fchollet/keras{\}}},
publisher = {GitHub},
title = {{Keras}},
year = {2015}
}
@inproceedings{matsubara2014latent,
author = {Matsubara, Takamitsu and G{\'{o}}mez, Vicen{\c{c}} and Kappen, Hilbert J},
booktitle = {Conference on Uncertainty in Artificial Intelligence (UAI)},
pages = {583--592},
title = {{Latent Kullback-Leibler Control for Continuous-State Systems using Probabilistic Graphical Models}},
year = {2014}
}
@article{chen2003plantwide,
author = {Chen, Rong and McAvoy, Thomas},
journal = {Industrial and Engineering Chemistry Research},
number = {20},
pages = {4753--4771},
publisher = {ACS Publications},
title = {{Plantwide control system design: Methodology and application to a vinyl acetate process}},
volume = {42},
year = {2003}
}
@inproceedings{gu2017q,
author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
booktitle = {International Conference on Learning Representations (ICLR)},
title = {{Q-prop: Sample-efficient policy gradient with an off-policy critic}},
year = {2017}
}
@inproceedings{todorov2006linearly,
author = {Todorov, Emanuel},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1369--1376},
title = {{Linearly-solvable Markov decision problems}},
year = {2006}
}

@article{ernst2005approximate,
author={Damien Ernst and Mevludin Glavic and Pierre Geurts and Louis Wehenkel},
journal = {International Journal of Emerging Electric Power Systems},
number = {1},
pages = {1--35},
title = {{Approximate Value Iteration in the Reinforcement Learning Context. Application to Electrical Power System Control}},
volume = {3},
year = {2005}
}

@inproceedings{busoniu2010online,
author = {Busoniu, Lucian and Ernst, Damien and {De Schutter}, Bart and Babuska, Robert},
booktitle = {American Control Conference (ACC), 2010},
pages = {486--491},
title = {{Online least-squares policy iteration for reinforcement learning control}},
year = {2010}
}
@article{BorensteinRAM1991,
author = {Borenstein, J and Koren, Y},
journal = {IEEE Transactions on Robotics and Automation},
number = {3},
pages = {278--288},
title = {{The vector field histogram-fast obstacle avoidance for mobile robots}},
volume = {7},
year = {1991}
}
@article{Aylett1998,
abstract = {We performed a prospective, randomized, double-blind trial of topical aprotinin versus placebo in 100 patients undergoing cardiac operations with cardiopulmonary bypass. Fifty-five patients received aprotinin. Forty underwent coronary artery bypass grafting (CABG) and 15 valve replacement ± CABG. Of 45 patients in the control group 38 underwent CABG and 7 valve replacement ± CABG. Aprotinin (50 mL; 70 mg) or placebo was applied topically to the heart, pericardium, and mediastinum before sternal closure. There were five reentries for bleeding with a surgical site identified in four. Mean blood loss was significantly less in the aprotinin group (653 versus 903 mL; p = 0.002), and fewer aprotinin patients received blood as a volume expander (67.5{\%} versus 88{\%}; p = 0.03). In coronary patients alone when aspirin administration was continued until the day of operation there was no difference between treatment and placebo groups (768 versus 879 mL). When aspirin administration was discontinued 2 weeks before operation there was a significant difference (558 versus 884 mL; p = 0.016) as in the group overall. This provides the potential for intrapericardial instillation for patients with excessive postoperative bleeding.},
author = {Aylett, R S and Soutter, J and Petley, G J and Chung, P W H and Rushton, A},
isbn = {0003-4975; 0003-4975},
issn = {0003-4975},
journal = {European Conference on Artificial Intelligence (ECAI)},
pmid = {7524457},
title = {{AI Planning in a Chemical Plant Domain}},
year = {1998}
}

@inproceedings{aylett1998ai,
  title={AI Planning in a Chemical Plant Domain.},
  author={Aylett, Ruth and Soutter, James K and Petley, Gary J and Chung, Paul WH and Rushton, A},
  booktitle={European Conference on Artificial Intelligence (ECAI)},
  pages={622--626},
  year={1998}
}

@inproceedings{moldovan2012safe,
  title={Safe exploration in Markov decision processes},
  author={Moldovan, Teodor Mihai and Abbeel, Pieter},
  booktitle={International Coference on International Conference on Machine Learning (ICML)},
  pages={1451--1458},
  year={2012},
}
@inproceedings{van2016deep,
author = {{Van Hasselt}, Hado and Guez, Arthur and Silver, David},
booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
pages = {2094--2100},
title = {{Deep Reinforcement Learning with Double {Q}-Learning.}},
year = {2016}
}
@phdthesis{andoni2009nearest,
author = {Andoni, Alexandr},
school = {Massachusetts Institute of Technology},
title = {{Nearest neighbor search: the old, the new, and the impossible}},
year = {2009}
}
@inproceedings{humanoids2015cui,
author = {Cui, Yunduan and Matsubara, Takamitsu and Sugimoto, Kenji},
booktitle = {IEEE RAS International Conference on Humanoid Robots (Humanoids)},
pages = {1083--1089},
title = {{Local Update Dynamic Policy Programming in Reinforcement Learning of Pneumatic Artificial Muscle-Driven Humanoid Hand Control}},
year = {2015}
}
@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@inproceedings{li2009online,
author = {Li, Lihong and Littman, Michael L and Mansley, Christopher R},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems},
pages = {733--739},
title = {{Online exploration in least-squares policy iteration}},
year = {2009}
}
@inproceedings{BocsiSOGP2011,
author = {Bocsi, B and N.T, Duy and Csato, L and Scholkopf, B and Peters, J},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {698--703},
title = {{Learning inverse kinematics with structured prediction}},
year = {2011}
}
@article{psaltis2014plantwide,
author = {Psaltis, Andreas and Kookos, Ioannis K and Kravaris, Costas},
journal = {Computers and Chemical Engineering},
pages = {108--116},
publisher = {Elsevier},
title = {{Plantwide control structure selection methodology for the benchmark vinyl acetate monomer plant}},
volume = {62},
year = {2014}
}
@article{chou1996measurement,
author = {Chou, ChingPing and Hannaford, Blake},
journal = {IEEE Transactions on Robotics and Automation},
number = {1},
pages = {90--102},
publisher = {IEEE},
title = {{Measurement and modeling of McKibben pneumatic artificial muscles}},
volume = {12},
year = {1996}
}
@inproceedings{HuntemannICRA2013,
author = {Huntemann, A and Demeester, E and Poorten, E V and {Van Brussel}, H and {De Schutter}, J},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {4376--4383},
title = {{Probabilistic approach to recognize local navigation plans by fusing past driving information with a personalized user model}},
year = {2013}
}
@article{Silver-nature2017,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Others},
journal = {Nature},
volume = {550},
pages = {354--359},
title = {{Mastering the game of Go without human knowledge}},
year = {2017}
}
@inproceedings{MatsubaraROMAN2015,
author = {Matsubara, T and Miro, J V and Tanaka, D and Poon, J and Sugimoto, K},
booktitle = {IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
pages = {444--449},
title = {{Sequential intention estimation of a mobility aid user for intelligent navigational assistance}},
year = {2015}
}
@article{mcavoy1999synthesis,
author = {McAvoy, Thomas J},
journal = {Industrial and Engineering Chemistry Research},
number = {8},
pages = {2984--2994},
publisher = {ACS Publications},
title = {{Synthesis of plantwide control systems using optimization}},
volume = {38},
year = {1999}
}
@inproceedings{Urdiales07,
author = {Urdiales, C and Poncela, A and Sanchez-Tato, I and Galluppi, F and Olivetti, M and Sandoval, F},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {3586--3591},
title = {{Efficiency based reactive shared control for collaborative human/robot navigation}},
year = {2007}
}
@article{theodorou2010generalized,
author = {Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
journal = {The Journal of Machine Learning Research},
pages = {3137--3181},
publisher = {JMLR. org},
title = {{A generalized path integral control approach to reinforcement learning}},
volume = {11},
year = {2010}
}
@article{seki2010plantwide,
author = {Seki, Hiroya and Ogawa, Morimasa and Itoh, Toshiaki and Ootakara, Shigeki and Murata, Hisashi and Hashimoto, Yoshihiro and Kano, Manabu},
journal = {Computers and Chemical Engineering},
number = {8},
pages = {1282--1295},
publisher = {Elsevier},
title = {{Plantwide control system design of the benchmark vinyl acetate monomer production plant}},
volume = {34},
year = {2010}
}
@inproceedings{peters2010relative,
author = {Peters, Jan and M{\"{u}}lling, Katharina and Altun, Yasemin},
booktitle = {Association of the Advancement of Artificial Intelligence (AAAI)},
pages = {1607--1612},
title = {{Relative Entropy Policy Search.}},
year = {2010}
}
@article{baxter2001infinite,
author = {Baxter, Jonathan and Bartlett, Peter L},
journal = {Journal of Artificial Intelligence Research},
number = {10},
pages = {319--350},
title = {{Infinite-horizon policy-gradient estimation}},
volume = {12},
year = {2001}
}
@article{Doya2002,
abstract = {We propose a modular reinforcement learning architecture for nonlinear, nonstationary control tasks, which we call multiple model-based reinforcement learning (MMRL). The basic idea is to decompose a complex task into multiple domains in space and time based on the predictability of the environmental dynamics. The system is composed of multiple modules, each of which consists of a state prediction model and a reinforcement learning controller. The "responsibility signal," which is given by the softmax function of the prediction errors, is used to weight the outputs of multiple modules, as well as to gate the learning of the prediction models and the reinforcement learning controllers. We formulate MMRL for both discrete-time, finite-state case and continuous-time, continuous-state case. The performance of MMRL was demonstrated for discrete case in a nonstationary hunting task in a grid world and for continuous case in a nonlinear, nonstationary control task of swinging up a pendulum with variable physical parameters.},
author = {Doya, Kenji and Samejima, Kazuyuki and Katagiri, Ken Ichi and Kawato, Mitsuo},
isbn = {089976602753712972},
issn = {08997667},
journal = {Neural Computation},
pmid = {12020450},
title = {{Multiple model-based reinforcement learning}},
year = {2002}
}
@inproceedings{szegedy2015cnn,
author = {Szegedy, C and Liu, Wei and Jia, Yangqing and Sermanet, P and Reed, S and Anguelov, D and Erhan, D and Vanhoucke, V and Rabinovich, A},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1--9},
title = {{Going deeper with convolutions}},
year = {2015}
}
@article{Dotoli2015,
abstract = {The goal of this paper consists in providing a survey of the main advanced control techniques currently adopted in factory automation. In particular, attention is devoted to model based control, model predictive control, intelligent and adaptive control, discrete event and event- triggered control. Open issues and challenges are pointed out, and the needs for further research efforts are discussed in detail.},
author = {Dotoli, Mariagrazia and Fay, Alexander and Mi{\'{s}}kowicz, Marek and Seatzu, Carla},
file = {:C$\backslash$:/Users/Lingwei Zhu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dotoli et al. - 2015 - A Survey on Advanced Control Approaches in Factory Automation.pdf:pdf},
issn = {2405-8963},
journal = {IFAC-PapersOnLine},
number = {3},
pages = {394--399},
publisher = {Elsevier},
title = {{A Survey on Advanced Control Approaches in Factory Automation}},
volume = {48},
year = {2015}
}
@article{peters2008natural,
author = {Peters, Jan and Schaal, Stefan},
journal = {Neurocomputing},
number = {7},
pages = {1180--1190},
publisher = {Elsevier},
title = {{Natural actor-critic}},
volume = {71},
year = {2008}
}
@article{chen2003nonlinear,
author = {Chen, Rong and Dave, Kedar and McAvoy, Thomas J and Luyben, Michael},
journal = {Industrial and Engineering Chemistry Research},
number = {20},
pages = {4478--4487},
publisher = {ACS Publications},
title = {{A nonlinear dynamic model of a vinyl acetate process}},
volume = {42},
year = {2003}
}
@article{Mnih-humanlevel2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Others},
journal = {Nature},
number = {7540},
pages = {529--533},
title = {{Human-level control through deep reinforcement learning}},
year = {2015},
volume = {518},
}
@article{mnih2015human,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Others},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Research},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}

@inproceedings{cui2018factorial,
  title={Factorial Kernel Dynamic Policy Programming for Vinyl Acetate Monomer Plant Model Control},
  author={Cui, Yunduan and Zhu, Lingwei and Fujisaki, Morihiro and Kanokogi, Hiroaki and Matsubara, Takamitsu},
  booktitle={IEEE International Conference on Automation Science and Engineering (IEEE-CASE)},
  pages={304--309},
  year={2018},
  }

@book{Bertsekas2005,
abstract = {The leading and most up-to-date textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The treatment focuses on basic unifying themes, and conceptual foundations. It illustrates the versatility, power, and generality of the method with many examples and applications from engineering, operations research, and other fields. It also addresses extensively the practical application of the methodology, possibly through the use of approximations, and provides an introduction to the far-reaching methodology of Neuro-Dynamic Programming/Reinforcement Learning. The first volume is oriented towards modeling, conceptualization, and finite-horizon problems, but also includes a substantive introduction to infinite horizon problems that is suitable for classroom use. The second volume is oriented towards mathematical analysis and computation, and treats infinite horizon problems extensively. The text contains many illustrations, worked-out examples, and exercises.},
author = {Bertsekas, Dimitri P},
booktitle = {Analysis},
eprint = {arXiv:1011.1669v3},
isbn = {1886529264},
issn = {{\textless}null{\textgreater}},
pmid = {2563756},
title = {{Dynamic Programming and Optimal Control}},
year = {2005}
}

@book{bertsekas1995dynamic,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}
author = {Kohlbrecher, S and Meyer, J and Stryk, O Von and Klingauf, U},
booktitle = {Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)},
organization = {IEEE},
title = {{A Flexible and Scalable SLAM System with Full 3D Motion Estimation}},
year = {2011}
}

@INPROCEEDINGS{polani2007,
author={T. {Jung} and D. {Polani}},
booktitle={2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
title={Kernelizing LSPE(λ)},
year={2007},
volume={},
number={},
pages={338-345},
keywords={learning (artificial intelligence);least squares approximations;kernel-based methods;function approximator;least-squares-based policy evaluation;regressors approximation;relevant basis functions;online reinforcement learning;Octopus benchmark;Least squares approximation;Function approximation;Learning;Kernel;Dynamic programming;Electronic mail;Optimal control;Optimization methods;Least squares methods;Control systems},
month={April},}


@article{Schaal2010,
abstract = {Recent trends in robot learning are to use trajectory-based optimal control techniques and reinforcement learning to scale complex robotic systems. On the one hand, increased computational power and multiprocessing, and on the other hand, probabilistic reinforcement learning methods and function approximation, have contributed to a steadily increasing interest in robot learning. Imitation learning has helped significantly to start learning with reasonable initial behavior. However, many applications are still restricted to rather lowdimensional domains and toy applications. Future work will have to demonstrate the continual and autonomous learning abilities, which were alluded to in the introduction.},
author = {Schaal, Stefan and Atkeson, Christopher G.},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Learning control,Optimal control,Reinforcement learning,Robot learning},
title = {{Learning control in robotics}},
year = {2010}
}
@article{Downs1993,
abstract = {This paper describes a model of an industrial chemical process for the purpose of developing, studying and evaluating process control technology. This process is well suited for a wide variety of studies including both plant-wide control and multivariable control problems. It consists of a reactor/ separator/recycle arrangement involving two simultaneous gas-liquid exothermic reactions of the following form:. A(g) + C(g) + D(g) → G(liq), Product 1,. A(g) + C(g) + E(g) → H(liq), Product 2. Two additional byproduct reactions also occur. The process has 12 valves available for manipulation and 41 measurements available for monitoring or control. The process equipment, operating objectives, process control objectives and process disturbances are described. A set of FORTRAN subroutines which simulate the process are available upon request. The chemical process model presented here is a challenging problem for a wide variety of process control technology studies. Even though this process has only a few unit operations, it is much more complex than it appears on first examination. We hope that this problem will be useful in the development of the process control field. We are also interested in hearing about applications of the problem. {\textcopyright} 1993.},
author = {Downs, J. J. and Vogel, E. F.},
eprint = {1722},
file = {:C$\backslash$:/Users/Lingwei Zhu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Downs, Vogel - 1993 - A plant-wide industrial process control problem.pdf:pdf},
issn = {00981354},
journal = {Computers and Chemical Engineering},
number = {3},
pages = {245--255},
title = {{A plant-wide industrial process control problem}},
volume = {17},
year = {1993}
}
@techreport{ChosetBUG,
author = {Choset, H},
institution = {Carnegie Mellon University, Pittsburgh},
title = {{Robotic Motion Planning: Bug Algorithms}},
url = {http://www.cs.cmu.edu/{\%}5C{~}motionplanning/lecture/Chap2-Bug-Alg{\_}howie.pdf},
year = {2012}
}


@article{metzger2011survey,
  title={A survey on applications of agent technology in industrial process control},
  author={Metzger, Mieczyslaw and Polakow, Grzegorz},
  journal={IEEE Transactions on Industrial Informatics},
  volume={7},
  number={4},
  pages={570--581},
  year={2011},
  publisher={IEEE}
}

@article{shadowhand,
author = {Walker, Rich},
journal = {Shadow Robot Company},
publisher = {Shadow Robot Company},
title = {{Shadow dextrous hand technical specification}},
year = {2013}
}
@article{JollyRAS2009,
author = {Jolly, K G and Kumar, R Sreerama and Vijayakumar, R},
journal = {Robotics and Autonomous Systems},
number = {1},
pages = {23--33},
title = {{A Bezier curve based path planning in a multi-agent robot soccer system without violating the acceleration limits}},
volume = {57},
year = {2009}
}
@inproceedings{Yang2015,
abstract = {The fully connected layers of a deep convolutional neural network typically contain over 90{\%} of the network parameters, and consume the majority of the memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all fully connected layers in a deep convolutional neural network. This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named deep fried convolutional networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance.},
author = {Yang, Zichao and Moczulski, Marcin and Denil, Misha and Freitas, Nando De and Smola, Alex and Song, Le and Wang, Ziyu},
booktitle = {the IEEE International Conference on Computer Vision},
eprint = {1412.7149},
isbn = {9781467383912},
issn = {15505499},
pmid = {15003161},
title = {{Deep fried convnets}},
year = {2015}
}
@article{dahl2012context,
author = {Dahl, George E and Yu, Dong and Deng, Li and Acero, Alex},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {30--42},
publisher = {IEEE},
title = {{Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition}},
volume = {20},
year = {2012}
}
@incollection{atkeson1997locally,
author = {Atkeson, Christopher G and Moore, Andrew W and Schaal, Stefan},
booktitle = {Artificial Intelligence Review},
pages = {75--113},
title = {{Locally weighted learning for control}},
year = {1997}
}
@article{machida2016vinyl,
author = {Machida, Yuta and Ootakara, Shigeki and Seki, Hiroya and Hashimoto, Yoshihiro and Kano, Manabu and Miyake, Yasuhiro and Anzai, Naoto and Sawai, Masayoshi and Katsuno, Takashi and Omata, Toshiaki},
journal = {International Federation of Automatic Control (IFAC)},
number = {7},
pages = {533--538},
publisher = {Elsevier},
title = {{Vinyl Acetate Monomer (VAM) Plant Model: A New Benchmark Problem for Control and Operation Study}},
volume = {49},
year = {2016}
}
@article{Garcia2012,
abstract = {In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.},
author = {Garcia, Javier and Fernandez, Fernando},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
title = {{Safe exploration of state and action spaces in reinforcement learning}},
year = {2012}
}
@inproceedings{deisenroth2009analytic,
author = {Deisenroth, Marc Peter and Huber, Marco F and Hanebeck, Uwe D},
booktitle = {the 26th annual international conference on machine learning},
pages = {225--232},
title = {{Analytic moment-based Gaussian process filtering}},
year = {2009}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
journal = {Nature},
pmid = {26819042},
title = {{Mastering the game of Go with deep neural networks and tree search}},
year = {2016}
}
@inproceedings{DemeesterIROS2006,
author = {Demeester, E and Huntemann, A and Vanhooydonck, D and Vanacker, G and Degeest, A and {Van Brussel}, H and Nuttin, M},
booktitle = {Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference on},
pages = {5775--5780},
title = {{Bayesian Estimation of Wheelchair Driver Intents: Modeling Intents as Geometric Paths Tracked by the Driver}},
year = {2006}
}
@article{luyben1998industrial,
author = {Luyben, Michael L and Tyr{\'{e}}us, Bj{\"{o}}rn D},
journal = {Computers and Chemical Engineering},
number = {7-8},
pages = {867--877},
publisher = {Elsevier},
title = {{An industrial design/control study for the vinyl acetate monomer process}},
volume = {22},
year = {1998}
}
@article{bentley1975multidimensional,
author = {Bentley, Jon Louis},
journal = {Communications of the ACM},
number = {9},
pages = {509--517},
publisher = {ACM},
title = {{Multidimensional binary search trees used for associative searching}},
volume = {18},
year = {1975}
}
@article{Ijspeert2008,
abstract = {The problem of controlling locomotion is an area in which neuroscience and robotics can fruitfully interact. In this article, I will review research carried out on locomotor central pattern generators (CPGs), i.e. neural circuits capable of producing coordinated patterns of high-dimensional rhythmic output signals while receiving only simple, low-dimensional, input signals. The review will first cover neurobiological observations concerning locomotor CPGs and their numerical modelling, with a special focus on vertebrates. It will then cover how CPG models implemented as neural networks or systems of coupled oscillators can be used in robotics for controlling the locomotion of articulated robots. The review also presents how robots can be used as scientific tools to obtain a better understanding of the functioning of biological CPGs. Finally, various methods for designing CPGs to control specific modes of locomotion will be briefly reviewed. In this process, I will discuss different types of CPG models, the pros and cons of using CPGs with robots, and the pros and cons of using robots as scientific tools. Open research topics both in biology and in robotics will also be discussed. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Ijspeert, Auke Jan},
eprint = {NIHMS150003},
isbn = {08936080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Central pattern generators,Computational models,Dynamical systems,Locomotion,Neural networks,Robots,Systems of coupled oscillators},
pmid = {18555958},
title = {{Central pattern generators for locomotion control in animals and robots: A review}},
year = {2008}
}

@article{ijspeert2008central,
  title={Central pattern generators for locomotion control in animals and robots: a review},
  author={Ijspeert, Auke Jan},
  journal={Neural networks},
  volume={21},
  number={4},
  pages={642--653},
  year={2008},
  publisher={Elsevier}
}

@article{abadi2016tensorflow,
author = {Abadi, Mart$\backslash$'$\backslash$in and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Others},
journal = {arXiv preprint arXiv:1603.04467},
title = {{Tensorflow: Large-scale machine learning on heterogeneous distributed systems}},
year = {2016}
}
@article{Levine2018,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
isbn = {978-1-4503-3716-8},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
pmid = {21156984},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
year = {2018}
}
@inproceedings{boedecker2014approximate,
author = {Boedecker, Joschka and Springenberg, Jost Tobias and W{\"{u}}lfing, Jan and Riedmiller, Martin},
booktitle = {Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2014 IEEE Symposium on},
organization = {IEEE},
pages = {1--8},
title = {{Approximate real-time optimal control based on sparse gaussian process models}},
year = {2014}
}
@article{Perkins2002,
author = {Perkins, Theodore J and Barto, Andrew G},
file = {:C$\backslash$:/Users/Lingwei Zhu/Downloads/perkins02a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {lyapunov functions,reinforcement learning,safety,stability},
pages = {803--832},
title = {{Lyapunov Design for Safe Reinforcement Learning}},
volume = {3},
year = {2002}
}
@inproceedings{Wang-duel2016,
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {Van Hasselt}, Hado and Lanctot, Marc and {De Freitas}, Nando},
booktitle = {International Conference on Machine Learning ({ICML})},
pages = {1995--2003},
series = {ICML'16},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
year = {2016}
}
@inproceedings{azar2011dynamic,
author = {Azar, Mohammad G and G{\'{o}}mez, Vicen{\c{c}}c and Kappen, Hilbert J},
booktitle = {International Conference on Artificial Intelligence and Statistics ({AISTATS})},
pages = {119--127},
title = {{Dynamic policy programming with function approximation}},
year = {2011}
}
@article{schaal2010learning,
author = {Schaal, Stefan and Atkeson, Christopher G},
journal = {IEEE Robotics {\&} Automation Magazine},
number = {2},
pages = {20--29},
publisher = {IEEE},
title = {{Learning control in robotics}},
volume = {17},
year = {2010}
}
@techreport{CoulterPP1990,
author = {Coulter, R C},
institution = {Carnegie Mellon University, Pittsburgh},
title = {{Implementation of the Pure Pursuit Path Tracking Algorithm}},
year = {1990}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1177--1184},
  year={2008}
}

@article{cui2017pneumatic,
author = {Cui, Yunduan and Matsubara, Takamitsu and Sugimoto, Kenji},
journal = {Advanced Robotics},
pages = {1--16},
publisher = {Taylor {\&} Francis},
title = {{Pneumatic artificial muscle-driven robot control using local update reinforcement learning}},
year = {2017}
}
@article{Kano2009,
abstract = {ABSTRACT
In this age of globalization, the realization of production innovation and highly stable operation is the chief objective of the process industry in Japan. Obviously, modern advanced control plays an important role to achieve this target; but it is emphasized here that a key to success is the maximum utilization of PID control and conventional advanced control. This paper surveys how the three central pillars of process control – PID control, conventional advanced control, and linear/nonlinear model predictive control – have been used and how they have contributed toward increasing productivity. In addition to introducing eminently practical methods, emerging methods, and their applications, the authors point out challenging problems. In Japan, industry and academia are working in close cooperation to share their important problems and develop new technologies for solving them. Several methods introduced in this paper are results of such industry-academia collaboration among engineers and researchers in various companies and universities. Furthermore, soft-sensor or virtual sensor design is treated with emphasis on its maintenance, because soft-sensors must cope with changes in process characteristics for their continuous utilization. Maintenance is a key issue not only for soft-sensors but also for controllers. Finally, we will expand our scope and briefly introduce recent activities in tracking simulation and alarm management. A part of the results of our recent questionnaire survey of process control are also introduced; the results are extremely helpful in clarifying the state of the art in process control in Japan.},
author = {Kano, Manabu and Ogawa, Morimasa},
file = {:C$\backslash$:/Users/Lingwei Zhu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kano, Ogawa - 2009 - The State of the Art in Advanced Chemical Process Control in Japan.pdf:pdf},
isbn = {9783902661548},
issn = {1474-6670},
journal = {IFAC Proceedings Volumes},
number = {11},
pages = {10--25},
publisher = {Elsevier},
title = {{The State of the Art in Advanced Chemical Process Control in Japan}},
volume = {42},
year = {2009}
}

@inproceedings{Chow2018,
author = {Chow, Yinlam and Ofir, Nachum and Duenez-guzman, Edgar and Ghavamzadeh, Mohammad},
booktitle = {Annual Conference on Neural Information Processing Systems (NIPS)},
pages = {1--10},
title = {{A Lyapunov-based Approach to Safe Reinforcement Learning}},
year = {2018}
}


@INPROCEEDINGS{Kakade02,
    author = {Sham Kakade and John Langford},
    title = {Approximately Optimal Approximate Reinforcement Learning},
    booktitle = {19th International Conference on Machine Learning (ICML)},
    year = {2002},
    pages = {267--274},

}

@article{Rockafellar02,
author = {Rockafellar, R and Uryasev, Stan},
year = {2002},
pages = {1443-1471},
title = {Conditional Value-At-Risk for General Loss Distributions},
volume = {26},
journal = {Journal of Banking and Finance}
}

@InProceedings{mnih2016asynchronous,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1928--1937},
  year = 	 {2016},
  volume = 	 {48},
}

@techreport{Engineering,
pages = {182},
title = {{Workshop No.31 (Process Control Technology) of the Japan Society for the Promotion of Sciences(JSPS)}}
}
@article{grancharova2008explicit,
author = {Grancharova, Alexandra and Kocijan, Ju{\v{s}} and Johansen, Tor A},
journal = {Automatica},
number = {6},
pages = {1621--1631},
publisher = {Elsevier},
title = {{Explicit stochastic predictive control of combustion plants based on Gaussian process models}},
volume = {44},
year = {2008}
}
@book{goodfellow2016deep,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep learning}},
year = {2016}
}
@article{olsen2005plantwide,
author = {Olsen, Donald G and Svrcek, William Y and Young, Brent R},
journal = {Chemical Engineering Communications},
number = {10},
pages = {1243--1257},
publisher = {Taylor {\&} Francis},
title = {{Plantwide control study of a vinyl acetate monomer process design}},
volume = {192},
year = {2005}
}

@article{Arnold2015,
archivePrefix = {arXiv},
arxivId = {1512.07679},
author = {Dulac-Arnold, Gabriel and Evans, Richard and Sunehag, Peter and Coppin, Ben},
eprint = {1512.07679},
journal = {CoRR},
title = {{Reinforcement Learning in Large Discrete Action Spaces}},
volume = {abs/1512.0},
year = {2015}
}

@misc{dulacarnold2015deep,
    title={Deep Reinforcement Learning in Large Discrete Action Spaces},
    author={Gabriel Dulac-Arnold and Richard Evans and Hado van Hasselt and Peter Sunehag and Timothy Lillicrap and Jonathan Hunt and Timothy Mann and Theophane Weber and Thomas Degris and Ben Coppin},
    year={2015},
    eprint={1512.07679},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{kogiso2012identification,
author = {Kogiso, Kiminao and Sawano, Kenta and Itto, Takashi and Sugimoto, Kenji},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {3714--3721},
title = {{Identification procedure for McKibben pneumatic artificial muscle systems}},
year = {2012}
}
@book{rasmussen2006gaussian,
author = {Rasmussen, Carl Edward and Williams, Christopher K I},
publisher = {MIT press Cambridge},
title = {{Gaussian processes for machine learning}},
volume = {1},
year = {2006}
}
@article{daerden2002pneumatic,
author = {Daerden, Frank and Lefeber, Dirk},
journal = {European journal of mechanical and environmental engineering},
number = {1},
pages = {11--21},
title = {{Pneumatic artificial muscles: actuators for robotics and automation}},
volume = {47},
year = {2002}
}
@article{cui2017kernel,
author = {Cui, Yunduan and Matsubara, Takamitsu and Sugimoto, Kenji},
journal = {Neural networks},
pages = {13--23},
title = {{Kernel Dynamic Policy Programming: Applicable Reinforcement Learning to Robot Systems with High Dimensional States}},
volume = {94},
year = {2017}
}
@techreport{ATSE2010,
author = {Tegart},
institution = {ATSE},
title = {{Smart Technology for Healthy Ageing}},
year = {2010}
}
@article{HartSSC1968,
author = {Hart, P E and Nilsson, N J and Raphael, B},
journal = {Systems Science and Cybernetics, IEEE Transactions on},
number = {2},
pages = {100--107},
title = {{A Formal Basis for the Heuristic Determination of Minimum Cost Paths}},
volume = {4},
year = {1968}
}
@article{lagoudakis2003least,
author = {Lagoudakis, Michail G and Parr, Ronald},
journal = {The Journal of Machine Learning Research},
number = {44},
pages = {1107--1149},
publisher = {JMLR. org},
title = {{Least-squares policy iteration}},
volume = {4},
year = {2003}
}
@article{Taylor2009,
author = {Taylor, Gavin and Parr, Ronald},
file = {:C$\backslash$:/Users/Lingwei Zhu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor, Parr - 2009 - Kernelized value function approximation for reinforcement learning.pdf:pdf},
isbn = {978-1-60558-516-1},
journal = {International Conference on Machine Learning (ICML)},
keywords = {cs,duke,edu,gvtaylor,parr,reinforcement learning},
pages = {1017--1024},
title = {{Kernelized value function approximation for reinforcement learning}},
year = {2009}
}
@inproceedings{kormushev2010robot,
author = {Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G},
booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {3232--3237},
title = {{Robot motor skill coordination with {\{}EM{\}}-based reinforcement learning}},
year = {2010}
}
@article{mackay1998introduction,
author = {MacKay, David J C},
journal = {NATO ASI Series F Computer and Systems Sciences},
pages = {133--166},
publisher = {Springer Verlag},
title = {{Introduction to Gaussian processes}},
volume = {168},
year = {1998}
}
@inproceedings{liu2010enhanced,
author = {Liu, Wei and Tan, Ying and Qiu, Qinru},
booktitle = {the Conference on Design, Automation and Test in Europe},
pages = {602--605},
title = {{Enhanced Q-learning algorithm for dynamic power management with performance constraint}},
year = {2010}
}

@article{BHATNAGAR2009,
title = "Natural actor–critic algorithms",
journal = "Automatica",
volume = "45",
number = "11",
pages = "2471 - 2482",
year = "2009",
issn = "0005-1098",
author = "Shalabh Bhatnagar and Richard S. Sutton and Mohammad Ghavamzadeh and Mark Lee",
keywords = "Actor–critic reinforcement learning algorithms, Policy-gradient methods, Approximate dynamic programming, Function approximation, Two-timescale stochastic approximation, Temporal difference learning, Natural gradient",
abstract = "We present four new reinforcement learning algorithms based on actor–critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor–critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor–critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor–critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms."
}

@article{Frieze2004,
 author = {Frieze, Alan and Kannan, Ravi and Vempala, Santosh},
 title = {Fast Monte-carlo Algorithms for Finding Low-rank Approximations},
 journal = {J. ACM},
 issue_date = {November 2004},
 volume = {51},
 number = {6},
 month = nov,
 year = {2004},
 issn = {0004-5411},
 pages = {1025--1041},
 numpages = {17},
 keywords = {Matrix algorithms, low-rank approximation, sampling},
}

@inproceedings{Blum2006,
author="Blum, Avrim",
editor="Saunders, Craig
and Grobelnik, Marko
and Gunn, Steve
and Shawe-Taylor, John",
title="Random Projection, Margins, Kernels, and Feature-Selection",
booktitle="Subspace, Latent Structure and Feature Selection",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="52--68",
abstract="Random projection is a simple technique that has had a number of applications in algorithm design. In the context of machine learning, it can provide insight into questions such as ``why is a learning problem easier if data is separable by a large margin?'' and ``in what sense is choosing a kernel much like choosing a set of features?'' This talk is intended to provide an introduction to random projection and to survey some simple learning algorithms and other applications to learning based on it. I will also discuss how, given a kernel as a black-box function, we can use various forms of random projection to extract an explicit small feature space that captures much of what the kernel is doing. This talk is based in large part on work in [BB05, BBV04] joint with Nina Balcan and Santosh Vempala.",
isbn="978-3-540-34138-3"
}


@inproceedings{Achlioptas2001,
  title={Sampling Techniques for Kernel Methods},
  author={Dimitris Achlioptas and Frank McSherry and Bernhard Sch{\"o}lkopf},
  booktitle={NIPS},
  year={2001}
}

@inproceedings{Konda00,
    author = {Vijay Konda and John Tsitsiklis},
    title = {Actor-Critic Algorithms},
    booktitle = {SIAM Journal on Control and Optimization},
    year = {2000},
    pages = {1008--1014},
    publisher = {MIT Press}
}

@article{Bengio2009,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971v5},
author = {Bengio, Yoshua},
eprint = {1509.02971v5},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
pmid = {24966830},
title = {{CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING}},
year = {2009}
}
@inproceedings{graves2013speech,
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
booktitle = {{\{}IEEE{\}} international conference on acoustics, speech and signal processing ({\{}ICASSP{\}})},
pages = {6645--6649},
title = {{Speech recognition with deep recurrent neural networks}},
year = {2013}
}
@inproceedings{girard2003gaussian,
author = {Girard, Agathe and Rasmussen, Carl Edward and Candela, Joaquin Quinonero and Murray-Smith, Roderick},
booktitle = {Advances in neural information processing systems},
pages = {545--552},
title = {{Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting}},
year = {2003}
}
@article{hoskins1992process,
author = {Hoskins, J C and Himmelblau, D M},
journal = {Computers and Chemical Engineering},
number = {4},
pages = {241--251},
publisher = {Elsevier},
title = {{Process control via artificial neural networks and reinforcement learning}},
volume = {16},
year = {1992}
}

@article{Luyben2011,
author = {Luyben, William L.},
title = {Design and Control of a Modified Vinyl Acetate Monomer Process},
journal = {Industrial \& Engineering Chemistry Research},
volume = {50},
number = {17},
pages = {10136-10147},
year = {2011},
}


@article{luyben2011design,
  title={Design and control of a modified vinyl acetate monomer process},
  author={Luyben, William L},
  journal={Industrial \& Engineering Chemistry Research},
  volume={50},
  number={17},
  pages={10136--10147},
  year={2011},
  publisher={ACS Publications}
}

@inproceedings{Le2013,
abstract = {Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that computing the de- cision function is typically expensive, espe- cially at prediction time. In this paper, we overcome this difficulty by proposing Fast- food, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matri- ces when combined with diagonal Gaussian matrices exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks (Rahimi {\&} Recht, 2007) and thereby speeding up the computation for a large range of ker- nel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) com- putation and storage, without sacrificing ac- curacy. We prove that the approximation is unbiased and has low variance. Extensive ex- periments show that we achieve similar accu- racy to full kernel expansions and Random Kitchen Sinks while being 100x faster and us- ing 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applica- tions that have large training sets and/or re- quire real-time prediction.},
author = {Le, Quoc V. and Sarlos, Tamas and Smola, Alex},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
isbn = {0086538217},
issn = {1938-7228},
title = {{Fastfood-computing hilbert space expansions in loglinear time}},
year = {2013}
}

@inproceedings{le2013fastfood,
  title={Fastfood-computing hilbert space expansions in loglinear time},
  author={Le, Quoc and Sarl{\'o}s, Tam{\'a}s and Smola, Alexander},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={244--252},
  year={2013}
}

@article{Eysenback2017,
archivePrefix = {arXiv},
arxivId = {1711.06782},
author = {Eysenbach, Benjamin and Gu, Shixiang and Ibarz, Julian and Levine, Sergey},
eprint = {1711.06782},
file = {:C$\backslash$:/Users/Lingwei Zhu/Downloads/1711.06782.pdf:pdf},
title = {{Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning}},
volume = {abs/1711.0},
year = {2017}
}

@inproceedings{eysenbach2018leave,
title={Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},
author={Benjamin Eysenbach and Shixiang Gu and Julian Ibarz and Sergey Levine},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
pages = {1--18},
}

@inproceedings{cui2016kernel,
author = {Cui, Yunduan and Matsubara, Takamitsu and Sugimoto, Kenji},
booktitle = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},
pages = {662--667},
title = {{Kernel dynamic policy programming: Practical reinforcement learning for high-dimensional robots}},
year = {2016}
}
@article{endo2008learning,
author = {Endo, Gen and Morimoto, Jun and Matsubara, Takamitsu and Nakanishi, Jun and Cheng, Gordon},
journal = {The International Journal of Robotics Research},
number = {2},
pages = {213--228},
publisher = {SAGE Publications},
title = {{Learning {\{}CPG{\}}-based biped locomotion with a policy gradient method: Application to a humanoid robot}},
volume = {27},
year = {2008}
}
@misc{Website,
  author = "{Omega Simulation Corp.}",
  title = {Visual Modeler},
  howpublished = {\url{https://www.omegasim.co.jp/contents_e/product/vm/}}
}
@book{mackay2003information,
author = {MacKay, David J C},
publisher = {Cambridge university press},
title = {{Information theory, inference and learning algorithms}},
year = {2003}
}
@inproceedings{Shumpei2018,
author = {Kubosawa, Shumpei and Onishi, Takashi and Tsuruoka, Yoshimasa},
booktitle = {SICE Annual Conference},
pages = {1376--1379},
title = {{Synthesizing chemical plant operation procedures using knowledge, dynamic simulation and deep reinforcement learning}},
year = {2018}
}
@inproceedings{uchibe2014inverse,
author = {Uchibe, Eiji and Doya, Kenji},
booktitle = {International Conference on Development and Learning and on Epigenetic Robotics},
organization = {IEEE},
pages = {222--228},
title = {{Inverse reinforcement learning using dynamic policy programming}},
year = {2014}
}
@techreport{OrrRBFN1996,
author = {Orr, M J L},
institution = {University of Edinburgh, Scotland},
title = {{Introduction to Radial Basis Function Networks}},
year = {1996}
}

@article{dotoli2017advanced,
  title={Advanced control in factory automation: a survey},
  author={Dotoli, Mariagrazia and Fay, Alexander and Mi{\'s}kowicz, Marek and Seatzu, Carla},
  journal={International Journal of Production Research},
  volume={55},
  number={5},
  pages={1243--1259},
  year={2017},
  publisher={Taylor \& Francis}
}
