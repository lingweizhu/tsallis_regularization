\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{geist19-regularized}
\citation{vieillard2020leverage}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}{section.2}\protected@file@percent }
\newlabel{eq:PI}{{1}{1}{Background}{equation.2.1}{}}
\citation{azar2012dynamic,vieillard2020leverage,Kozuno-2022-KLGenerativeMinMaxOptimal,chan2021-greedification}
\citation{hiriart2004-convexanalysis,Nachum2020-RLFenchelRockafellar}
\citation{kozunoCVI,geist19-regularized,vieillard2020leverage}
\citation{TsallisEntropy}
\citation{tsallis2009introduction}
\citation{Martins16-sparsemax}
\citation{Blondel-2020LearningFenchelYoundLoss}
\citation{Lee2018-TsallisRAL,Lee2020-generalTsallisRSS}
\citation{zhu2023generalized}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (Left) Behavior of $q$-logarithm and $q$-exponential functions. When $q=1$ they respectively recover the standard logarithm and exponential. (Mid) Tsallis entropy of the Gaussian policy $\mathcal  {N}(2, 1)$. (Right) Tsallis KL divergence between two Gaussian policies $\mathcal  {N}(2.75, 1)$ and $\mathcal  {N}(3.25, 1)$. \relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:q_stats}{{1}{2}{(Left) Behavior of \qlog and $q$-exponential functions. When $q=1$ they respectively recover the standard logarithm and exponential. (Mid) Tsallis entropy of the Gaussian policy $\mathcal {N}(2, 1)$. (Right) Tsallis KL divergence between two Gaussian policies $\mathcal {N}(2.75, 1)$ and $\mathcal {N}(3.25, 1)$. \relax }{figure.caption.2}{}}
\newlabel{eq:average}{{2}{2}{Background}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}$q$-statistics and Tsallis regularization}{2}{section.3}\protected@file@percent }
\citation{Fujimoto2019-InSampleMax}
\citation{Xiao2023-InSampleSoftmax}
\citation{Xiao2023-InSampleSoftmax}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The sparsemax operator acting upon Gaussian and Boltzmann policies for $q=2$ and $q=50$ by truncating actions with values lower than $\psi $. As $q\rightarrow \infty $ the policy tends toward uniform distribution. We assume the The dataset is generated by Tsallis policies such that the actions absent from the offline dataset are those being truncated. \relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{eq:tkl_policy}{{4}{3}{$q$-statistics and Tsallis regularization}{equation.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}In-Sample Softmax for Offline RL}{3}{section.4}\protected@file@percent }
\newlabel{eq:hardmax_offline}{{5}{3}{In-Sample Softmax for Offline RL}{equation.4.5}{}}
\newlabel{eq:insample_softmax}{{6}{3}{In-Sample Softmax for Offline RL}{equation.4.6}{}}
\newlabel{eq:inac_policy}{{9}{3}{In-Sample Softmax for Offline RL}{equation.4.9}{}}
\citation{Fujimoto2019-InSampleMax}
\citation{Xiao2023-InSampleSoftmax}
\citation{Lee2018-TsallisRAL,Martins16-sparsemax}
\citation{Tsai2021-selfsupervisedRelativePredictiveCoding}
\@writefile{toc}{\contentsline {section}{\numberline {5}Forward-Backward Tsallis Learning for Offline RL}{4}{section.5}\protected@file@percent }
\newlabel{eq:insample_sparsemax}{{11}{4}{Forward-Backward Tsallis Learning for Offline RL}{equation.5.11}{}}
\newlabel{eq:proposal_policy}{{12}{4}{Forward-Backward Tsallis Learning for Offline RL}{equation.5.12}{}}
\newlabel{lemma:log_qlog_diff}{{1}{5}{}{lemma.1}{}}
\citation{Xiao2023-InSampleSoftmax}
\citation{Lee2018-TsallisRAL}
\@writefile{toc}{\contentsline {section}{\numberline {6}Implementation}{6}{section.6}\protected@file@percent }
\newlabel{eq:insample_policy}{{17}{6}{Implementation}{equation.6.17}{}}
\citation{Yamano2004-properties-qlogexp}
\newlabel{eq:tsallis_inac_policy}{{18}{7}{Implementation}{equation.6.18}{}}
\bibstyle{abbrvnat}
\bibdata{library}
\bibcite{azar2012dynamic}{{1}{2012}{{Azar et~al.}}{{Azar, G{\'{o}}mez, and Kappen}}}
\bibcite{Blondel-2020LearningFenchelYoundLoss}{{2}{2020}{{Blondel et~al.}}{{Blondel, Martins, and Niculae}}}
\bibcite{chan2021-greedification}{{3}{2022}{{Chan et~al.}}{{Chan, Silva, Lim, Kozuno, Mahmood, and White}}}
\bibcite{Fujimoto2019-InSampleMax}{{4}{2019}{{Fujimoto et~al.}}{{Fujimoto, Meger, and Precup}}}
\bibcite{geist19-regularized}{{5}{2019}{{Geist et~al.}}{{Geist, Scherrer, and Pietquin}}}
\bibcite{hiriart2004-convexanalysis}{{6}{2004}{{Hiriart-Urruty and Lemar{\'e}chal}}{{}}}
\bibcite{kozunoCVI}{{7}{2019}{{Kozuno et~al.}}{{Kozuno, Uchibe, and Doya}}}
\bibcite{Kozuno-2022-KLGenerativeMinMaxOptimal}{{8}{2022}{{Kozuno et~al.}}{{Kozuno, Yang, Vieillard, Kitamura, Tang, Mei, Ménard, Azar, Valko, Munos, Pietquin, Geist, and Szepesvári}}}
\bibcite{Lee2018-TsallisRAL}{{9}{2018}{{Lee et~al.}}{{Lee, Choi, and Oh}}}
\bibcite{Lee2020-generalTsallisRSS}{{10}{2020}{{Lee et~al.}}{{Lee, Kim, Lim, Choi, Hong, Kim, Park, and Oh}}}
\bibcite{Martins16-sparsemax}{{11}{2016}{{Martins and Astudillo}}{{}}}
\bibcite{Nachum2020-RLFenchelRockafellar}{{12}{2020}{{Nachum and Dai}}{{}}}
\bibcite{Tsai2021-selfsupervisedRelativePredictiveCoding}{{13}{2021}{{Tsai et~al.}}{{Tsai, Ma, Yang, Zhao, Morency, and Salakhutdinov}}}
\bibcite{TsallisEntropy}{{14}{1988}{{Tsallis}}{{}}}
\bibcite{tsallis2009introduction}{{15}{2009}{{Tsallis}}{{}}}
\bibcite{vieillard2020leverage}{{16}{2020}{{Vieillard et~al.}}{{Vieillard, Kozuno, Scherrer, Pietquin, Munos, and Geist}}}
\bibcite{Xiao2023-InSampleSoftmax}{{17}{2023}{{Xiao et~al.}}{{Xiao, Wang, Pan, White, and White}}}
\bibcite{Yamano2004-properties-qlogexp}{{18}{2002}{{Yamano}}{{}}}
\bibcite{zhu2023generalized}{{19}{2023}{{Zhu et~al.}}{{Zhu, Chen, Matsubara, and White}}}
\gdef \@abspage@last{9}
