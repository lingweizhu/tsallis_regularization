\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Puterman1994}
\citation{azar2012dynamic,Rawlik2013-SOCRL,vieillard2020leverage}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{1}{subsection.2.1}\protected@file@percent }
\citation{TsallisEntropy,tsallis2009introduction}
\citation{tsallis2009introduction}
\citation{Nachum18a-tsallis,Lee2018-TsallisRAL}
\citation{Blondel-2020LearningFenchelYoundLoss,Martins16-sparsemax}
\citation{Lee2018-TsallisRAL,Nachum18a-tsallis}
\citation{zhu2023generalized}
\citation{Furuichi2004-fundamentals-qKL}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}$q$-statistics and Tsallis regularization}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:tsallis_policy}{{2}{2}{$q$-statistics and Tsallis regularization}{equation.2.2}{}}
\newlabel{eq:tkl_policy}{{3}{2}{$q$-statistics and Tsallis regularization}{equation.2.3}{}}
\citation{Gulcehre2021-regularizedBehavior}
\citation{Dadashi2021-pseudoMetricOffline,Fujimoto2019-InSampleMax,Fujimoto2021-minimalist,Nair2021-awac}
\citation{Ghasemipour2021-EMaQ,Kumar2019-BootstrapErrorQlearningMMD,Jaques2020-OfflineDialog,Wu2020-BehaviorRegularizedAC}
\citation{Fujimoto2019-InSampleMax,Kostrikov2022-implicitQlearning,Xiao2023-InSampleSoftmax}
\citation{Fujimoto2019-InSampleMax}
\citation{Xiao2023-InSampleSoftmax}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  (Left) Comparison between argmax, softmax and sparsemax on the probability simplex. Argmax produces a deterministic policy residing on the vertices, while a softmax policy lies inside the simplex. By contrast, a sparsemax policy lives on the border. (Right) Sparsemax operator acting on a Gaussian policy by truncating actions with value below the threshold $\psi $. Actions with value larger than $\psi $ are collected in the set $K(s)$. \relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:simplex_truncate}{{1}{3}{(Left) Comparison between argmax, softmax and sparsemax on the probability simplex. Argmax produces a deterministic policy residing on the vertices, while a softmax policy lies inside the simplex. By contrast, a sparsemax policy lives on the border. (Right) Sparsemax operator acting on a Gaussian policy by truncating actions with value below the threshold $\psi $. Actions with value larger than $\psi $ are collected in the set $K(s)$. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Offline Reinforcement Learning}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}In-Sample Softmax for Offline RL}{3}{subsection.3.1}\protected@file@percent }
\newlabel{eq:hardmax_offline}{{4}{3}{In-Sample Softmax for Offline RL}{equation.3.4}{}}
\citation{Xiao2023-InSampleSoftmax}
\citation{Wu2020-BehaviorRegularizedAC}
\citation{Kostrikov2021-fisherCriticReg}
\newlabel{eq:inac_policy}{{7}{4}{In-Sample Softmax for Offline RL}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}In-Sample Tsallis Regularization}{4}{subsection.3.2}\protected@file@percent }
\citation{Lee2020-generalTsallisRSS}
\citation{asadi17a,azar2012dynamic}
\citation{Lee2018-TsallisRAL,Martins16-sparsemax}
\newlabel{eq:proposal_policy}{{10}{5}{In-Sample Tsallis Regularization}{equation.3.10}{}}
\newlabel{eq:tsallis_backward_q2}{{11}{5}{In-Sample Tsallis Regularization}{equation.3.11}{}}
\newlabel{lemma:log_qlog_diff}{{1}{5}{}{lemma.1}{}}
\citation{Tsai2021-selfsupervisedRelativePredictiveCoding}
\citation{Xiao2023-InSampleSoftmax}
\citation{Yamano2004-properties-qlogexp}
\citation{Lee2018-TsallisRAL}
\newlabel{eq:tsallis_inac_policy_thoery}{{16}{7}{In-Sample Tsallis Regularization}{equation.3.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{7}{section.4}\protected@file@percent }
\newlabel{eq:insample_policy}{{17}{7}{Implementation}{equation.4.17}{}}
\bibstyle{abbrvnat}
\bibdata{library}
\bibcite{asadi17a}{{1}{2017}{{Asadi and Littman}}{{}}}
\bibcite{azar2012dynamic}{{2}{2012}{{Azar et~al.}}{{Azar, G{\'{o}}mez, and Kappen}}}
\bibcite{Blondel-2020LearningFenchelYoundLoss}{{3}{2020}{{Blondel et~al.}}{{Blondel, Martins, and Niculae}}}
\bibcite{chen2018-TsallisApproximate}{{4}{2018}{{Chen et~al.}}{{Chen, Peng, and Zhang}}}
\bibcite{Nachum18a-tsallis}{{5}{2018}{{Chow et~al.}}{{Chow, Nachum, and Ghavamzadeh}}}
\bibcite{Dadashi2021-pseudoMetricOffline}{{6}{2021}{{Dadashi et~al.}}{{Dadashi, Rezaeifar, Vieillard, Hussenot, Pietquin, and Geist}}}
\bibcite{Fujimoto2021-minimalist}{{7}{2021}{{Fujimoto and Gu}}{{}}}
\bibcite{Fujimoto2019-InSampleMax}{{8}{2019}{{Fujimoto et~al.}}{{Fujimoto, Meger, and Precup}}}
\bibcite{Furuichi2004-fundamentals-qKL}{{9}{2004}{{Furuichi et~al.}}{{Furuichi, Yanagi, and Kuriyama}}}
\bibcite{Ghasemipour2021-EMaQ}{{10}{2021}{{Ghasemipour et~al.}}{{Ghasemipour, Schuurmans, and Gu}}}
\bibcite{Gulcehre2021-regularizedBehavior}{{11}{2021}{{Gulcehre et~al.}}{{Gulcehre, Colmenarejo, Wang, Sygnowski, Paine, Zolna, Chen, Hoffman, Pascanu, and de~Freitas}}}
\bibcite{Jaques2020-OfflineDialog}{{12}{2020}{{Jaques et~al.}}{{Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza, Jones, Gu, and Picard}}}
\bibcite{Kostrikov2021-fisherCriticReg}{{13}{2021}{{Kostrikov et~al.}}{{Kostrikov, Fergus, Tompson, and Nachum}}}
\bibcite{Kostrikov2022-implicitQlearning}{{14}{2022}{{Kostrikov et~al.}}{{Kostrikov, Nair, and Levine}}}
\bibcite{Kumar2019-BootstrapErrorQlearningMMD}{{15}{2019}{{Kumar et~al.}}{{Kumar, Fu, Tucker, and Levine}}}
\bibcite{Lee2018-TsallisRAL}{{16}{2018}{{Lee et~al.}}{{Lee, Choi, and Oh}}}
\bibcite{Lee2020-generalTsallisRSS}{{17}{2020}{{Lee et~al.}}{{Lee, Kim, Lim, Choi, Hong, Kim, Park, and Oh}}}
\bibcite{Martins16-sparsemax}{{18}{2016}{{Martins and Astudillo}}{{}}}
\bibcite{Nair2021-awac}{{19}{2021}{{Nair et~al.}}{{Nair, Dalal, Gupta, and Levine}}}
\bibcite{Puterman1994}{{20}{1994}{{Puterman}}{{}}}
\bibcite{Rawlik2013-SOCRL}{{21}{2013}{{Rawlik et~al.}}{{Rawlik, Toussaint, and Vijayakumar}}}
\bibcite{Tsai2021-selfsupervisedRelativePredictiveCoding}{{22}{2021}{{Tsai et~al.}}{{Tsai, Ma, Yang, Zhao, Morency, and Salakhutdinov}}}
\bibcite{TsallisEntropy}{{23}{1988}{{Tsallis}}{{}}}
\bibcite{tsallis2009introduction}{{24}{2009}{{Tsallis}}{{}}}
\bibcite{vieillard2020leverage}{{25}{2020}{{Vieillard et~al.}}{{Vieillard, Kozuno, Scherrer, Pietquin, Munos, and Geist}}}
\bibcite{Wu2020-BehaviorRegularizedAC}{{26}{2020}{{Wu et~al.}}{{Wu, Tucker, and Nachum}}}
\bibcite{Xiao2023-InSampleSoftmax}{{27}{2023}{{Xiao et~al.}}{{Xiao, Wang, Pan, White, and White}}}
\bibcite{Yamano2004-properties-qlogexp}{{28}{2002}{{Yamano}}{{}}}
\bibcite{zhu2023generalized}{{29}{2023}{{Zhu et~al.}}{{Zhu, Chen, Matsubara, and White}}}
\citation{chen2018-TsallisApproximate}
\@writefile{toc}{\contentsline {section}{\numberline {A}Derivation}{10}{appendix.A}\protected@file@percent }
\newlabel{apdx:derivation}{{A}{10}{Derivation}{appendix.A}{}}
\newlabel{eq:approximate_policy}{{21}{10}{Derivation}{equation.A.21}{}}
\newlabel{eq:apdx_approximate_policy}{{22}{10}{Derivation}{equation.A.22}{}}
\gdef \@abspage@last{10}
