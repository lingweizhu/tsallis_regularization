\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Puterman1994}
\citation{azar2012dynamic,Rawlik2013-SOCRL,vieillard2020leverage}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{1}{subsection.2.1}\protected@file@percent }
\citation{TsallisEntropy,tsallis2009introduction}
\citation{tsallis2009introduction}
\citation{Nachum18a-tsallis,Lee2018-TsallisRAL}
\citation{Blondel-2020LearningFenchelYoundLoss,Martins16-sparsemax}
\citation{Lee2018-TsallisRAL,Nachum18a-tsallis}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (Left) Behavior of $q$-logarithm and $q$-exponential functions. When $q=1$ they respectively recover the standard logarithm and exponential. (Mid) Tsallis entropy of the Gaussian policy $\mathcal  {N}(2, 1)$. (Right) Tsallis KL divergence between two Gaussian policies $\mathcal  {N}(2.75, 1)$ and $\mathcal  {N}(3.25, 1)$. \relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:q_stats}{{1}{2}{(Left) Behavior of \qlog and $q$-exponential functions. When $q=1$ they respectively recover the standard logarithm and exponential. (Mid) Tsallis entropy of the Gaussian policy $\mathcal {N}(2, 1)$. (Right) Tsallis KL divergence between two Gaussian policies $\mathcal {N}(2.75, 1)$ and $\mathcal {N}(3.25, 1)$. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}$q$-statistics and Tsallis regularization}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:tsallis_policy}{{2}{2}{$q$-statistics and Tsallis regularization}{equation.2.2}{}}
\citation{zhu2023generalized}
\citation{Gulcehre2021-regularizedBehavior}
\citation{Dadashi2021-pseudoMetricOffline,Fujimoto2019-InSampleMax,Fujimoto2021-minimalist,Nair2021-awac}
\citation{Ghasemipour2021-EMaQ,Kumar2019-BootstrapErrorQlearningMMD,Jaques2020-OfflineDialog,Wu2020-BehaviorRegularizedAC}
\citation{Fujimoto2019-InSampleMax,Kostrikov2022-implicitQlearning,Xiao2023-InSampleSoftmax}
\citation{Fujimoto2019-InSampleMax}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The sparsemax operator acting upon Gaussian and Boltzmann policies for $q=2$ and $q=50$ by truncating actions with values lower than $\psi $. As $q\rightarrow \infty $ the policy tends toward uniform distribution. We assume the The dataset is generated by Tsallis policies such that the actions absent from the offline dataset are those being truncated. \relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:q_truncation}{{2}{3}{The sparsemax operator acting upon Gaussian and Boltzmann policies for $q=2$ and $q=50$ by truncating actions with values lower than $\psi $. As $q\rightarrow \infty $ the policy tends toward uniform distribution. We assume the The dataset is generated by Tsallis policies such that the actions absent from the offline dataset are those being truncated. \relax }{figure.caption.3}{}}
\newlabel{eq:tkl_policy}{{3}{3}{$q$-statistics and Tsallis regularization}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Offline Reinforcement Learning}{3}{subsection.2.3}\protected@file@percent }
\citation{Xiao2023-InSampleSoftmax}
\citation{Xiao2023-InSampleSoftmax}
\citation{Wu2020-BehaviorRegularizedAC}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}In-Sample Softmax for Offline RL}{4}{subsection.3.1}\protected@file@percent }
\newlabel{eq:hardmax_offline}{{4}{4}{In-Sample Softmax for Offline RL}{equation.3.4}{}}
\newlabel{eq:inac_policy}{{7}{4}{In-Sample Softmax for Offline RL}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}In-Sample Tsallis Regularization}{4}{subsection.3.2}\protected@file@percent }
\newlabel{eq:proposal_policy}{{10}{5}{In-Sample Tsallis Regularization}{equation.3.10}{}}
\newlabel{lemma:log_qlog_diff}{{1}{5}{}{lemma.1}{}}
\citation{Tsai2021-selfsupervisedRelativePredictiveCoding}
\citation{Xiao2023-InSampleSoftmax}
\citation{Yamano2004-properties-qlogexp}
\citation{Lee2018-TsallisRAL}
\newlabel{eq:tsallis_inac_policy_thoery}{{15}{7}{In-Sample Tsallis Regularization}{equation.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{7}{section.4}\protected@file@percent }
\newlabel{eq:insample_policy}{{16}{7}{Implementation}{equation.4.16}{}}
\bibstyle{abbrvnat}
\bibdata{library}
\bibcite{azar2012dynamic}{{1}{2012}{{Azar et~al.}}{{Azar, G{\'{o}}mez, and Kappen}}}
\bibcite{Blondel-2020LearningFenchelYoundLoss}{{2}{2020}{{Blondel et~al.}}{{Blondel, Martins, and Niculae}}}
\bibcite{chen2018-TsallisApproximate}{{3}{2018}{{Chen et~al.}}{{Chen, Peng, and Zhang}}}
\bibcite{Nachum18a-tsallis}{{4}{2018}{{Chow et~al.}}{{Chow, Nachum, and Ghavamzadeh}}}
\bibcite{Dadashi2021-pseudoMetricOffline}{{5}{2021}{{Dadashi et~al.}}{{Dadashi, Rezaeifar, Vieillard, Hussenot, Pietquin, and Geist}}}
\bibcite{Fujimoto2021-minimalist}{{6}{2021}{{Fujimoto and Gu}}{{}}}
\bibcite{Fujimoto2019-InSampleMax}{{7}{2019}{{Fujimoto et~al.}}{{Fujimoto, Meger, and Precup}}}
\bibcite{Ghasemipour2021-EMaQ}{{8}{2021}{{Ghasemipour et~al.}}{{Ghasemipour, Schuurmans, and Gu}}}
\bibcite{Gulcehre2021-regularizedBehavior}{{9}{2021}{{Gulcehre et~al.}}{{Gulcehre, Colmenarejo, Wang, Sygnowski, Paine, Zolna, Chen, Hoffman, Pascanu, and de~Freitas}}}
\bibcite{Jaques2020-OfflineDialog}{{10}{2020}{{Jaques et~al.}}{{Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza, Jones, Gu, and Picard}}}
\bibcite{Kostrikov2022-implicitQlearning}{{11}{2022}{{Kostrikov et~al.}}{{Kostrikov, Nair, and Levine}}}
\bibcite{Kumar2019-BootstrapErrorQlearningMMD}{{12}{2019}{{Kumar et~al.}}{{Kumar, Fu, Tucker, and Levine}}}
\bibcite{Lee2018-TsallisRAL}{{13}{2018}{{Lee et~al.}}{{Lee, Choi, and Oh}}}
\bibcite{Martins16-sparsemax}{{14}{2016}{{Martins and Astudillo}}{{}}}
\bibcite{Nair2021-awac}{{15}{2021}{{Nair et~al.}}{{Nair, Dalal, Gupta, and Levine}}}
\bibcite{Puterman1994}{{16}{1994}{{Puterman}}{{}}}
\bibcite{Rawlik2013-SOCRL}{{17}{2013}{{Rawlik et~al.}}{{Rawlik, Toussaint, and Vijayakumar}}}
\bibcite{Tsai2021-selfsupervisedRelativePredictiveCoding}{{18}{2021}{{Tsai et~al.}}{{Tsai, Ma, Yang, Zhao, Morency, and Salakhutdinov}}}
\bibcite{TsallisEntropy}{{19}{1988}{{Tsallis}}{{}}}
\bibcite{tsallis2009introduction}{{20}{2009}{{Tsallis}}{{}}}
\bibcite{vieillard2020leverage}{{21}{2020}{{Vieillard et~al.}}{{Vieillard, Kozuno, Scherrer, Pietquin, Munos, and Geist}}}
\bibcite{Wu2020-BehaviorRegularizedAC}{{22}{2020}{{Wu et~al.}}{{Wu, Tucker, and Nachum}}}
\bibcite{Xiao2023-InSampleSoftmax}{{23}{2023}{{Xiao et~al.}}{{Xiao, Wang, Pan, White, and White}}}
\bibcite{Yamano2004-properties-qlogexp}{{24}{2002}{{Yamano}}{{}}}
\bibcite{zhu2023generalized}{{25}{2023}{{Zhu et~al.}}{{Zhu, Chen, Matsubara, and White}}}
\citation{chen2018-TsallisApproximate}
\@writefile{toc}{\contentsline {section}{\numberline {A}Derivation}{10}{appendix.A}\protected@file@percent }
\newlabel{apdx:derivation}{{A}{10}{Derivation}{appendix.A}{}}
\newlabel{eq:approximate_policy}{{20}{10}{Derivation}{equation.A.20}{}}
\newlabel{eq:apdx_approximate_policy}{{21}{10}{Derivation}{equation.A.21}{}}
\gdef \@abspage@last{10}
