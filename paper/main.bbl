\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asadi and Littman(2017)]{asadi17a}
K.~Asadi and M.~L. Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, volume~70 of \emph{Proceedings of Machine Learning
  Research}, pages 243--252, International Convention Centre, Sydney,
  Australia, 06--11 Aug 2017. PMLR.

\bibitem[Azar et~al.(2012)Azar, G{\'{o}}mez, and Kappen]{azar2012dynamic}
M.~G. Azar, V.~G{\'{o}}mez, and H.~J. Kappen.
\newblock {Dynamic policy programming}.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 3207--3245, 2012.

\bibitem[Blondel et~al.(2020)Blondel, Martins, and
  Niculae]{Blondel-2020LearningFenchelYoundLoss}
M.~Blondel, A.~F. Martins, and V.~Niculae.
\newblock Learning with fenchel-young losses.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (35):\penalty0 1--69, 2020.

\bibitem[Chen et~al.(2018)Chen, Peng, and Zhang]{chen2018-TsallisApproximate}
G.~Chen, Y.~Peng, and M.~Zhang.
\newblock Effective exploration for deep reinforcement learning via
  bootstrapped q-ensembles under tsallis entropy regularization.
\newblock \emph{arXiv:abs/1809.00403}, 2018.
\newblock URL \url{http://arxiv.org/abs/1809.00403}.

\bibitem[Chow et~al.(2018)Chow, Nachum, and Ghavamzadeh]{Nachum18a-tsallis}
Y.~Chow, O.~Nachum, and M.~Ghavamzadeh.
\newblock Path consistency learning in {T}sallis entropy regularized {MDP}s.
\newblock In \emph{International Conference on Machine Learning}, pages
  979--988, 2018.

\bibitem[Dadashi et~al.(2021)Dadashi, Rezaeifar, Vieillard, Hussenot, Pietquin,
  and Geist]{Dadashi2021-pseudoMetricOffline}
R.~Dadashi, S.~Rezaeifar, N.~Vieillard, L.~Hussenot, O.~Pietquin, and M.~Geist.
\newblock Offline reinforcement learning with pseudometric learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pages 2307--2318, 2021.

\bibitem[Fujimoto and Gu(2021)]{Fujimoto2021-minimalist}
S.~Fujimoto and S.~S. Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and
  Precup]{Fujimoto2019-InSampleMax}
S.~Fujimoto, D.~Meger, and D.~Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 2052--2062, 2019.

\bibitem[Furuichi et~al.(2004)Furuichi, Yanagi, and
  Kuriyama]{Furuichi2004-fundamentals-qKL}
S.~Furuichi, K.~Yanagi, and K.~Kuriyama.
\newblock Fundamental properties of tsallis relative entropy.
\newblock \emph{Journal of Mathematical Physics}, 45\penalty0 (12):\penalty0
  4868--4877, 2004.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{Ghasemipour2021-EMaQ}
S.~K.~S. Ghasemipour, D.~Schuurmans, and S.~S. Gu.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139, pages 3682--3691, 2021.

\bibitem[Gulcehre et~al.(2021)Gulcehre, Colmenarejo, Wang, Sygnowski, Paine,
  Zolna, Chen, Hoffman, Pascanu, and
  de~Freitas]{Gulcehre2021-regularizedBehavior}
C.~Gulcehre, S.~G. Colmenarejo, Z.~Wang, J.~Sygnowski, T.~Paine, K.~Zolna,
  Y.~Chen, M.~Hoffman, R.~Pascanu, and N.~de~Freitas.
\newblock Regularized behavior value estimation, 2021.

\bibitem[Jaques et~al.(2020)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{Jaques2020-OfflineDialog}
N.~Jaques, A.~Ghandeharioun, J.~H. Shen, C.~Ferguson, A.~Lapedriza, N.~Jones,
  S.~Gu, and R.~Picard.
\newblock Way off-policy batch deep reinforcement learning of human preferences
  in dialog, 2020.
\newblock URL \url{https://openreview.net/forum?id=rJl5rRVFvH}.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Fergus, Tompson, and
  Nachum]{Kostrikov2021-fisherCriticReg}
I.~Kostrikov, R.~Fergus, J.~Tompson, and O.~Nachum.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pages 5774--5783, 2021.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and
  Levine]{Kostrikov2022-implicitQlearning}
I.~Kostrikov, A.~Nair, and S.~Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=68n2s9ZJWF8}.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and
  Levine]{Kumar2019-BootstrapErrorQlearningMMD}
A.~Kumar, J.~Fu, G.~Tucker, and S.~Levine.
\newblock \emph{Stabilizing Off-Policy Q-Learning via Bootstrapping Error
  Reduction}.
\newblock 2019.

\bibitem[Lee et~al.(2018)Lee, Choi, and Oh]{Lee2018-TsallisRAL}
K.~Lee, S.~Choi, and S.~Oh.
\newblock Sparse markov decision processes with causal sparse tsallis entropy
  regularization for reinforcement learning.
\newblock \emph{IEEE Robotics and Automation Letters}, 3:\penalty0 1466--1473,
  2018.

\bibitem[Lee et~al.(2020)Lee, Kim, Lim, Choi, Hong, Kim, Park, and
  Oh]{Lee2020-generalTsallisRSS}
K.~Lee, S.~Kim, S.~Lim, S.~Choi, M.~Hong, J.~I. Kim, Y.~Park, and S.~Oh.
\newblock Generalized tsallis entropy reinforcement learning and its
  application to soft mobile robots.
\newblock In \emph{Robotics: Science and Systems XVI}, pages 1--10, 2020.

\bibitem[Martins and Astudillo(2016)]{Martins16-sparsemax}
A.~F.~T. Martins and R.~F. Astudillo.
\newblock From softmax to sparsemax: A sparse model of attention and
  multi-label classification.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, page 1614–1623, 2016.

\bibitem[Nair et~al.(2021)Nair, Dalal, Gupta, and Levine]{Nair2021-awac}
A.~Nair, M.~Dalal, A.~Gupta, and S.~Levine.
\newblock {\{}AWAC{\}}: Accelerating online reinforcement learning with offline
  datasets, 2021.
\newblock URL \url{https://openreview.net/forum?id=OJiM1R3jAtZ}.

\bibitem[Puterman(1994)]{Puterman1994}
M.~L. Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1st edition, 1994.

\bibitem[Rawlik et~al.(2013)Rawlik, Toussaint, and
  Vijayakumar]{Rawlik2013-SOCRL}
K.~Rawlik, M.~Toussaint, and S.~Vijayakumar.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference (extended abstract).
\newblock In \emph{Proceedings of the Twenty-Third International Joint
  Conference on Artificial Intelligence}, IJCAI ’13, page 3052–3056, 2013.

\bibitem[Tsai et~al.(2021)Tsai, Ma, Yang, Zhao, Morency, and
  Salakhutdinov]{Tsai2021-selfsupervisedRelativePredictiveCoding}
Y.-H.~H. Tsai, M.~Q. Ma, M.~Yang, H.~Zhao, L.-P. Morency, and R.~Salakhutdinov.
\newblock Self-supervised representation learning with relative predictive
  coding.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=068E_JSq9O}.

\bibitem[Tsallis(1988)]{TsallisEntropy}
C.~Tsallis.
\newblock Possible generalization of boltzmann-gibbs statistics.
\newblock \emph{Journal of Statistical Physics}, 52:\penalty0 479--487, 1988.

\bibitem[Tsallis(2009)]{tsallis2009introduction}
C.~Tsallis.
\newblock \emph{Introduction to Nonextensive Statistical Mechanics: Approaching
  a Complex World}.
\newblock Springer New York, 2009.
\newblock ISBN 9780387853581.

\bibitem[Vieillard et~al.(2020)Vieillard, Kozuno, Scherrer, Pietquin, Munos,
  and Geist]{vieillard2020leverage}
N.~Vieillard, T.~Kozuno, B.~Scherrer, O.~Pietquin, R.~Munos, and M.~Geist.
\newblock Leverage the average: an analysis of regularization in rl.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pages
  1--12, 2020.

\bibitem[Wu et~al.(2020)Wu, Tucker, and Nachum]{Wu2020-BehaviorRegularizedAC}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock Behavior regularized offline reinforcement learning, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJg9hTNKPH}.

\bibitem[Xiao et~al.(2023)Xiao, Wang, Pan, White, and
  White]{Xiao2023-InSampleSoftmax}
C.~Xiao, H.~Wang, Y.~Pan, A.~White, and M.~White.
\newblock The in-sample softmax for offline reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=u-RuvyDYqCM}.

\bibitem[Yamano(2002)]{Yamano2004-properties-qlogexp}
T.~Yamano.
\newblock Some properties of q-logarithm and q-exponential functions in tsallis
  statistics.
\newblock \emph{Physica A: Statistical Mechanics and its Applications},
  305\penalty0 (3):\penalty0 486--496, 2002.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Matsubara, and White]{zhu2023generalized}
L.~Zhu, Z.~Chen, T.~Matsubara, and M.~White.
\newblock Generalized munchausen reinforcement learning using tsallis kl
  divergence, 2023.

\end{thebibliography}
